# üèóÔ∏è Builder Pattern Guide

Le Builder Pattern est l'API recommand√©e pour construire et entra√Æner des r√©seaux de neurones avec `test-neural`. Il offre une interface fluide et intuitive qui remplace les multiples m√©thodes de construction traditionnelles.

## Table des mati√®res

- [Pourquoi le Builder Pattern ?](#pourquoi-le-builder-pattern-)
- [NetworkBuilder](#networkbuilder)
- [TrainingBuilder](#trainingbuilder)
- [Exemples complets](#exemples-complets)
- [Comparaison avec l'API traditionnelle](#comparaison-avec-lapi-traditionnelle)

---

## Pourquoi le Builder Pattern ?

### Probl√®mes r√©solus

‚ùå **Avant** - Prolif√©ration de m√©thodes:
- `Network::new()` - r√©seau simple (1 couche cach√©e)
- `Network::new_deep()` - r√©seau profond (init auto)
- `Network::new_deep_with_init()` - r√©seau profond (init manuelle)
- `Network::fit()` - entra√Ænement avec callbacks
- `Network::fit_with_scheduler()` - entra√Ænement avec scheduler
- Gestion manuelle de `Vec<Box<dyn Callback>>`
- Confusion sur quelle m√©thode utiliser

‚úÖ **Apr√®s** - Une seule mani√®re:
- `NetworkBuilder` - construction intuitive par cha√Ænage
- `.trainer()` - entra√Ænement unifi√©
- Plus besoin de g√©rer les Vec manuellement
- API auto-document√©e

---

## NetworkBuilder

### Construction simple

```rust
use test_neural::builder::NetworkBuilder;
use test_neural::network::{Activation, LossFunction};
use test_neural::optimizer::OptimizerType;

let network = NetworkBuilder::new(2, 1)  // input_size, output_size
    .hidden_layer(8, Activation::Tanh)   // 1 couche cach√©e
    .build();
```

### R√©seau profond

```rust
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(16, Activation::ReLU)
    .hidden_layer(8, Activation::ReLU)
    .hidden_layer(4, Activation::Tanh)
    .build();
```

### Configuration compl√®te

```rust
let network = NetworkBuilder::new(input_size, output_size)
    // Couches cach√©es
    .hidden_layer(64, Activation::ReLU)
    .hidden_layer(32, Activation::ReLU)
    .hidden_layer(16, Activation::Tanh)
    
    // Sortie
    .output_activation(Activation::Sigmoid)  // d√©faut: Sigmoid
    .loss(LossFunction::BinaryCrossEntropy)  // d√©faut: BCE
    
    // Optimizer
    .optimizer(OptimizerType::adam(0.001))   // d√©faut: Adam(0.001)
    
    // R√©gularisation
    .dropout(0.3)           // appliqu√© aux couches cach√©es
    .l2(0.01)               // L2 regularization
    
    // Initialisation optionnelle
    .weight_init(WeightInit::He)  // sinon: auto selon activation
    
    .build();
```

### Options de r√©gularisation

```rust
// L1 (Lasso) - encourage la sparsit√©
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(8, Activation::Tanh)
    .l1(0.001)
    .build();

// L2 (Ridge) - p√©nalise les grands poids
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(8, Activation::Tanh)
    .l2(0.01)
    .build();

// Elastic Net - combine L1 et L2
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(8, Activation::Tanh)
    .elastic_net(0.5, 0.01)  // l1_ratio=0.5, lambda=0.01
    .build();

// Dropout + L2 (recommand√©)
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(16, Activation::ReLU)
    .hidden_layer(8, Activation::ReLU)
    .dropout(0.3)
    .l2(0.001)
    .build();
```

### Valeurs par d√©faut

Si vous n'sp√©cifiez pas certaines options, les valeurs par d√©faut sont:
- `output_activation`: `Activation::Sigmoid`
- `loss`: `LossFunction::BinaryCrossEntropy`
- `optimizer`: `OptimizerType::adam(0.001)`
- `weight_init`: Auto-d√©tection selon l'activation
- `dropout`: Aucun
- `regularization`: Aucune

---

## TrainingBuilder

### Entra√Ænement simple

```rust
use test_neural::builder::NetworkTrainer;  // Trait pour .trainer()

let history = network.trainer()
    .train_data(&train_dataset)
    .epochs(100)
    .fit();
```

### Avec validation

```rust
let history = network.trainer()
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    .epochs(100)
    .batch_size(32)
    .fit();
```

### Avec callbacks

```rust
use test_neural::callbacks::{EarlyStopping, ModelCheckpoint, ProgressBar};

let history = network.trainer()
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    .epochs(200)
    .batch_size(32)
    .callback(Box::new(EarlyStopping::new(10, 0.0001)))
    .callback(Box::new(ModelCheckpoint::new("best_model.json", true)))
    .callback(Box::new(ProgressBar::new(200)))
    .fit();
```

### Avec Learning Rate Scheduler

```rust
use test_neural::callbacks::{LearningRateScheduler, LRSchedule};

// StepLR: r√©duit le LR tous les N epochs
let history = network.trainer()
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    .epochs(100)
    .batch_size(32)
    .scheduler(LearningRateScheduler::new(
        LRSchedule::StepLR { 
            step_size: 30,  // tous les 30 epochs
            gamma: 0.1      // multiplier par 0.1
        }
    ))
    .fit();

// ReduceOnPlateau: r√©duit le LR quand loss stagne (recommand√©!)
let history = network.trainer()
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    .epochs(100)
    .batch_size(32)
    .scheduler(LearningRateScheduler::new(
        LRSchedule::ReduceOnPlateau { 
            patience: 10,          // attendre 10 epochs
            factor: 0.5,           // diviser par 2
            min_delta: 0.0001     // am√©lioration minimale
        }
    ))
    .fit();

// ExponentialLR: d√©croissance exponentielle
let history = network.trainer()
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    .epochs(100)
    .batch_size(32)
    .scheduler(LearningRateScheduler::new(
        LRSchedule::ExponentialLR { gamma: 0.95 }
    ))
    .fit();
```

### Configuration compl√®te (tout combin√©)

```rust
let history = network.trainer()
    // Donn√©es
    .train_data(&train_dataset)
    .validation_data(&val_dataset)
    
    // Hyperparam√®tres
    .epochs(200)
    .batch_size(32)
    
    // Learning rate scheduling
    .scheduler(LearningRateScheduler::new(
        LRSchedule::ReduceOnPlateau { 
            patience: 10, 
            factor: 0.5, 
            min_delta: 0.0001 
        }
    ))
    
    // Callbacks (dans l'ordre d'ex√©cution)
    .callback(Box::new(ProgressBar::new(200)))
    .callback(Box::new(ModelCheckpoint::new("best_model.json", true)))
    .callback(Box::new(EarlyStopping::new(20, 0.00001)))
    
    .fit();

// history contient (train_loss, val_loss) pour chaque epoch
println!("Loss finale: {:.6}", history.last().unwrap().1.unwrap());
```

---

## Exemples complets

### Exemple 1: Classification binaire (XOR)

```rust
use test_neural::builder::{NetworkBuilder, NetworkTrainer};
use test_neural::network::{Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::dataset::Dataset;
use test_neural::callbacks::{EarlyStopping, ModelCheckpoint};
use ndarray::array;

fn main() {
    // Donn√©es XOR
    let inputs = vec![
        array![0.0, 0.0],
        array![0.0, 1.0],
        array![1.0, 0.0],
        array![1.0, 1.0],
    ];
    
    let targets = vec![
        array![0.0],
        array![1.0],
        array![1.0],
        array![0.0],
    ];
    
    let dataset = Dataset::new(inputs.clone(), targets.clone());
    let (train, val) = dataset.split(0.75);
    
    // Construction du r√©seau
    let mut network = NetworkBuilder::new(2, 1)
        .hidden_layer(8, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .build();
    
    // Entra√Ænement
    let history = network.trainer()
        .train_data(&train)
        .validation_data(&val)
        .epochs(1000)
        .batch_size(2)
        .callback(Box::new(EarlyStopping::new(50, 0.0001)))
        .callback(Box::new(ModelCheckpoint::new("best_xor.json", true)))
        .fit();
    
    // Pr√©dictions
    for (input, target) in inputs.iter().zip(targets.iter()) {
        let prediction = network.predict(input);
        println!("[{:.0}, {:.0}] ‚Üí {:.3} (attendu {:.0})", 
            input[0], input[1], prediction[0], target[0]);
    }
}
```

### Exemple 2: R√©seau profond avec r√©gularisation

```rust
use test_neural::builder::{NetworkBuilder, NetworkTrainer};
use test_neural::network::{Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::callbacks::{LearningRateScheduler, LRSchedule, ProgressBar};

fn main() {
    let mut network = NetworkBuilder::new(784, 10)  // MNIST-like
        .hidden_layer(128, Activation::ReLU)
        .hidden_layer(64, Activation::ReLU)
        .hidden_layer(32, Activation::ReLU)
        .output_activation(Activation::Softmax)
        .loss(LossFunction::CategoricalCrossEntropy)
        .optimizer(OptimizerType::adam(0.001))
        .dropout(0.3)
        .l2(0.0001)
        .build();
    
    let history = network.trainer()
        .train_data(&train_dataset)
        .validation_data(&val_dataset)
        .epochs(100)
        .batch_size(64)
        .scheduler(LearningRateScheduler::new(
            LRSchedule::ReduceOnPlateau { 
                patience: 5, 
                factor: 0.5, 
                min_delta: 0.001 
            }
        ))
        .callback(Box::new(ProgressBar::new(100)))
        .fit();
    
    println!("Entra√Ænement termin√©: {} epochs", history.len());
}
```

---

## Comparaison avec l'API traditionnelle

### Construction

**Traditionnelle**:
```rust
// Simple
let network = Network::new(
    2, 8, 1,
    Activation::Tanh,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.01)
);

// Profond
let network = Network::new_deep(
    2,
    vec![16, 8, 4],           // Vec<usize>
    1,
    vec![Activation::ReLU, Activation::ReLU, Activation::Tanh],  // Vec<Activation>
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
);
```

**Builder**:
```rust
// Simple
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(8, Activation::Tanh)
    .build();

// Profond
let network = NetworkBuilder::new(2, 1)
    .hidden_layer(16, Activation::ReLU)
    .hidden_layer(8, Activation::ReLU)
    .hidden_layer(4, Activation::Tanh)
    .build();
```

### Entra√Ænement

**Traditionnelle**:
```rust
// Sans scheduler
let mut callbacks: Vec<Box<dyn Callback>> = vec![
    Box::new(EarlyStopping::new(10, 0.0001)),
];
let history = network.fit(&train, Some(&val), 100, 32, &mut callbacks);

// Avec scheduler
let mut scheduler = LearningRateScheduler::new(...);
let mut callbacks: Vec<Box<dyn Callback>> = vec![...];
let history = network.fit_with_scheduler(
    &train, Some(&val), 100, 32, &mut scheduler, &mut callbacks
);
```

**Builder**:
```rust
// Tout unifi√©
let history = network.trainer()
    .train_data(&train)
    .validation_data(&val)
    .epochs(100)
    .batch_size(32)
    .callback(Box::new(EarlyStopping::new(10, 0.0001)))
    .scheduler(LearningRateScheduler::new(...))
    .fit();
```

---

## Conclusion

Le Builder Pattern offre:

‚úÖ **API intuitive** - Code auto-document√©  
‚úÖ **Moins d'erreurs** - Plus de Vec √† g√©rer  
‚úÖ **Flexibilit√©** - Combinez n'importe quelles options  
‚úÖ **Unification** - Une seule mani√®re de faire  
‚úÖ **√âvolutivit√©** - Facile d'ajouter de nouvelles options  
‚úÖ **Backward compatible** - L'ancienne API reste disponible

üöÄ **Commencez ici**: `cargo run --example builder_showcase`
