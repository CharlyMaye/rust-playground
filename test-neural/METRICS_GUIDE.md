# Guide des M√©triques d'√âvaluation - Approches et Approfondissements

Ce document d√©taille les diff√©rentes approches pour √©valuer un r√©seau de neurones et comment approfondir chaque m√©trique.

## üìä Vue d'Ensemble

### M√©triques Impl√©ment√©es

| M√©trique | Usage | Binaire | Multi-Classe | Complexit√© |
|----------|-------|---------|--------------|------------|
| **Accuracy** | Pourcentage correct | ‚úÖ | ‚úÖ | Simple |
| **Precision** | Vrais positifs / Pr√©dits positifs | ‚úÖ | ‚úÖ | Moyen |
| **Recall** | Vrais positifs / R√©els positifs | ‚úÖ | ‚úÖ | Moyen |
| **F1-Score** | Moyenne harmonique P/R | ‚úÖ | ‚úÖ | Moyen |
| **Confusion Matrix** | Vue d√©taill√©e erreurs | ‚úÖ | ‚úÖ | Simple |
| **ROC Curve** | Performance √† tous seuils | ‚úÖ | üî∂ | Avanc√© |
| **AUC** | Aire sous courbe ROC | ‚úÖ | üî∂ | Avanc√© |

---

## 1. Accuracy (Exactitude)

### D√©finition
```
Accuracy = (Pr√©dictions Correctes) / (Total Pr√©dictions)
         = (TP + TN) / (TP + TN + FP + FN)
```

### Quand l'utiliser
- ‚úÖ **Dataset √©quilibr√©** (50% classe A, 50% classe B)
- ‚úÖ **Premi√®re m√©trique** √† regarder (simple et intuitive)
- ‚úÖ **Validation rapide** pendant l'entra√Ænement

### Quand NE PAS l'utiliser
- ‚ùå **Dataset d√©s√©quilibr√©** (99% classe A, 1% classe B)
  - Exemple: D√©tection de fraude (fraudes rares)
  - Un mod√®le qui pr√©dit toujours "pas de fraude" aura 99% accuracy mais est inutile
- ‚ùå **Co√ªts asym√©triques** (faux n√©gatif ‚â† faux positif)
  - Exemple: Diagnostic m√©dical (manquer un cancer est pire qu'un faux positif)

### Impl√©mentation Actuelle
```rust
pub fn accuracy(predictions: &[Array1<f64>], targets: &[Array1<f64>], threshold: f64) -> f64
```
- Supporte binaire (seuil) et multi-classes (argmax)
- Simple et rapide
- Pas de d√©pendances externes

### Approfondissements Possibles

#### 1.1 Balanced Accuracy
Pour datasets d√©s√©quilibr√©s :
```rust
pub fn balanced_accuracy(predictions, targets, threshold) -> f64 {
    // Moyenne du recall par classe
    // = (Sensitivity + Specificity) / 2
    let metrics = binary_metrics(predictions, targets, threshold);
    let sensitivity = metrics.recall;
    let specificity = metrics.true_negatives as f64 / 
                      (metrics.true_negatives + metrics.false_positives) as f64;
    (sensitivity + specificity) / 2.0
}
```
**Usage:** D√©tection d'anomalies, datasets m√©dicaux

#### 1.2 Top-K Accuracy
Pour classification multi-classes :
```rust
pub fn top_k_accuracy(predictions: &[Array1<f64>], targets: &[Array1<f64>], k: usize) -> f64 {
    // Correct si la vraie classe est dans les k pr√©dictions les plus probables
    // Utilis√© dans ImageNet (top-5 accuracy)
}
```
**Usage:** ImageNet, classification sur beaucoup de classes (1000+)

#### 1.3 Per-Class Accuracy
```rust
pub fn per_class_accuracy(predictions, targets) -> Vec<f64> {
    // Accuracy s√©par√©e pour chaque classe
    // Identifie les classes probl√©matiques
}
```
**Usage:** D√©bogage, analyse de performance par classe

---

## 2. Precision, Recall, F1-Score

### D√©finitions

**Precision (Pr√©cision):**
```
Precision = TP / (TP + FP)
= "Quand je pr√©dis positif, √† quelle fr√©quence ai-je raison?"
```

**Recall (Rappel / Sensibilit√©):**
```
Recall = TP / (TP + FN)
= "Je capture quel % de tous les positifs r√©els?"
```

**F1-Score:**
```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
= Moyenne harmonique de Precision et Recall
```

### Trade-off Precision vs Recall

| Seuil | Precision | Recall | Usage |
|-------|-----------|--------|-------|
| **√âlev√© (0.9)** | ‚¨ÜÔ∏è Haute | ‚¨áÔ∏è Basse | √âviter faux positifs (spam filter) |
| **Moyen (0.5)** | ‚û°Ô∏è √âquilibr√© | ‚û°Ô∏è √âquilibr√© | G√©n√©ral |
| **Bas (0.1)** | ‚¨áÔ∏è Basse | ‚¨ÜÔ∏è Haute | Capturer tous les positifs (diagnostic m√©dical) |

### Cas d'Usage

| Contexte | Priorit√© | Raison |
|----------|----------|--------|
| **Spam Filter** | üî¥ Precision | Ne pas bloquer vrais emails |
| **D√©tection Cancer** | üî¥ Recall | Ne pas manquer de malades |
| **Recommandations** | üî¥ Precision | Montrer contenu pertinent |
| **Moteur Recherche** | üü° F1 (√©quilibr√©) | Pertinence et couverture |

### Impl√©mentation Actuelle
```rust
pub fn binary_metrics(predictions, targets, threshold) -> BinaryMetrics {
    // Retourne struct avec accuracy, precision, recall, f1_score, TP/FP/TN/FN
}
```

### Approfondissements Possibles

#### 2.1 Precision-Recall Curve
```rust
pub fn precision_recall_curve(predictions, targets, num_thresholds) 
    -> (Vec<f64>, Vec<f64>, Vec<f64>) {
    // (precision_values, recall_values, thresholds)
    // Visualise trade-off precision/recall
    // Utile pour choisir le bon seuil
}
```

#### 2.2 Average Precision (AP)
```rust
pub fn average_precision(predictions, targets) -> f64 {
    // Aire sous la courbe Precision-Recall
    // M√©trique standard pour Object Detection
    // Utilis√© dans COCO dataset, PASCAL VOC
}
```

#### 2.3 F-Beta Score
```rust
pub fn f_beta_score(precision: f64, recall: f64, beta: f64) -> f64 {
    // F_Œ≤ = (1 + Œ≤¬≤) √ó (P √ó R) / (Œ≤¬≤ √ó P + R)
    // Œ≤ = 0.5: Favorise Precision
    // Œ≤ = 1.0: F1 (√©quilibr√©)
    // Œ≤ = 2.0: Favorise Recall
}
```
**Usage:** Ajuster l'importance de P vs R selon le contexte

#### 2.4 Macro/Micro/Weighted Averages (Multi-Classe)
```rust
pub enum AverageMethod {
    Macro,    // Moyenne simple des m√©triques par classe
    Micro,    // Calculer sur TP/FP/FN globaux
    Weighted, // Moyenne pond√©r√©e par nombre d'exemples
}

pub fn precision_multiclass(predictions, targets, method: AverageMethod) -> f64
```

**Exemple:**
```
Classes: A (100 ex), B (10 ex)
Precision_A = 0.9, Precision_B = 0.5

Macro:    (0.9 + 0.5) / 2 = 0.70  // Traite classes √©galement
Weighted: (0.9√ó100 + 0.5√ó10) / 110 = 0.86  // Pond√®re par fr√©quence
```

---

## 3. Confusion Matrix

### D√©finition

**Binaire (2x2):**
```
                Pr√©dit
             Neg    Pos
R√©el  Neg [  TN  |  FP  ]
      Pos [  FN  |  TP  ]
```

**Multi-Classe (NxN):**
```
matrix[i][j] = nombre d'exemples de classe i pr√©dits comme classe j
```

### Interpr√©tation

| M√©trique | Formule | Signification |
|----------|---------|---------------|
| **True Positive Rate** | TP / (TP + FN) | = Recall = Sensitivity |
| **False Positive Rate** | FP / (FP + TN) | Taux fausses alarmes |
| **True Negative Rate** | TN / (TN + FP) | = Specificity |
| **False Negative Rate** | FN / (FN + TP) | Taux manqu√©s |

### Impl√©mentation Actuelle
```rust
pub fn confusion_matrix_binary(predictions, targets, threshold) -> Array2<usize>
pub fn confusion_matrix_multiclass(predictions, targets, num_classes) -> Array2<usize>
pub fn format_confusion_matrix(matrix, class_names) -> String
```

### Approfondissements Possibles

#### 3.1 Normalized Confusion Matrix
```rust
pub fn confusion_matrix_normalized(predictions, targets, num_classes, 
                                   normalize: NormalizeMethod) -> Array2<f64> {
    enum NormalizeMethod {
        True,   // Normaliser par ligne (somme = 1 par vraie classe)
        Pred,   // Normaliser par colonne (somme = 1 par pr√©diction)
        All,    // Normaliser par total (toute matrice somme = 1)
    }
}
```
**Usage:** Visualisation, comparaison entre datasets de tailles diff√©rentes

#### 3.2 M√©triques D√©riv√©es de la Matrice
```rust
pub struct ConfusionMetrics {
    pub sensitivity: f64,     // = Recall = TPR
    pub specificity: f64,     // = TNR
    pub positive_likelihood_ratio: f64,  // TPR / FPR
    pub negative_likelihood_ratio: f64,  // FNR / TNR
    pub diagnostic_odds_ratio: f64,      // PLR / NLR
}
```

#### 3.3 Cohen's Kappa
```rust
pub fn cohens_kappa(confusion_matrix: &Array2<usize>) -> f64 {
    // Mesure accord au-del√† du hasard
    // Œ∫ = (p_o - p_e) / (1 - p_e)
    // 1.0 = accord parfait, 0 = accord al√©atoire
}
```
**Usage:** Inter-rater reliability, annoter qualit√©

---

## 4. ROC Curve & AUC

### ROC Curve (Receiver Operating Characteristic)

**D√©finition:**
- Graphique: FPR (x-axis) vs TPR (y-axis) √† diff√©rents seuils
- Montre trade-off entre sensibilit√© et sp√©cificit√©

**Impl√©mentation Actuelle:**
```rust
pub fn roc_curve(predictions, targets, num_thresholds) 
    -> (Vec<f64>, Vec<f64>, Vec<f64>)  // (FPR, TPR, thresholds)
```

### AUC (Area Under Curve)

**D√©finition:**
- Aire sous la courbe ROC
- **1.0** = Pr√©dictions parfaites (tous les positifs avant tous les n√©gatifs)
- **0.5** = Performance al√©atoire (ligne diagonale)
- **< 0.5** = Pire que random (mod√®le invers√©!)

**Interpr√©tation:**
```
AUC = Probabilit√© qu'un exemple positif al√©atoire 
      ait un score plus √©lev√© qu'un exemple n√©gatif al√©atoire
```

**Impl√©mentation Actuelle:**
```rust
pub fn auc_roc(predictions, targets) -> f64
```

### Avantages ROC/AUC

‚úÖ **Ind√©pendant du seuil** - √âvalue performance globale
‚úÖ **R√©sistant d√©s√©quilibre** - Contrairement √† accuracy
‚úÖ **Standard industrie** - Benchmarking, publications

### Limites

‚ùå **Datasets tr√®s d√©s√©quilibr√©s** - Pr√©f√©rer Precision-Recall
‚ùå **Multi-classes** - N√©cessite One-vs-Rest ou One-vs-One
‚ùå **Petit dataset** - Courbe instable (peu de points)

### Approfondissements Possibles

#### 4.1 Partial AUC
```rust
pub fn partial_auc(predictions, targets, fpr_range: (f64, f64)) -> f64 {
    // AUC dans une r√©gion sp√©cifique de FPR
    // Utile si on s'int√©resse √† un taux FPR sp√©cifique
    // Ex: Partial AUC entre FPR 0.0-0.1 pour high-precision tasks
}
```

#### 4.2 Multi-Class ROC
```rust
pub fn roc_auc_multiclass(predictions, targets, num_classes, 
                          method: MultiClassMethod) -> f64 {
    enum MultiClassMethod {
        OneVsRest,  // N courbes ROC (classe i vs reste)
        OneVsOne,   // N√ó(N-1)/2 courbes (toutes paires)
    }
}
```

#### 4.3 Bootstrap Confidence Intervals
```rust
pub fn auc_confidence_interval(predictions, targets, 
                                num_bootstraps: usize, 
                                confidence: f64) -> (f64, f64, f64) {
    // (lower_bound, auc, upper_bound)
    // Donne incertitude sur l'AUC
    // Utile pour petits datasets
}
```

---

## 5. M√©triques Avanc√©es (Non Impl√©ment√©es)

### 5.1 Log Loss (Cross-Entropy Loss)
```rust
pub fn log_loss(predictions: &[Array1<f64>], targets: &[Array1<f64>]) -> f64 {
    // P√©nalise fortement pr√©dictions confiantes mais fausses
    // Standard pour comp√©titions (Kaggle)
    // Meilleur que accuracy car prend en compte probabilit√©s
}
```

### 5.2 Brier Score
```rust
pub fn brier_score(predictions: &[Array1<f64>], targets: &[Array1<f64>]) -> f64 {
    // = Mean Squared Error entre probabilit√©s pr√©dites et r√©elles
    // Plus sensible aux pr√©dictions extr√™mes que log loss
}
```

### 5.3 Calibration Curve
```rust
pub fn calibration_curve(predictions, targets, num_bins: usize) 
    -> (Vec<f64>, Vec<f64>) {
    // (mean_predicted_probability, fraction_of_positives)
    // V√©rifie si probabilit√©s pr√©dites sont calibr√©es
    // Ex: Parmi toutes les pr√©dictions √† 70%, 70% doivent √™tre positives
}
```

### 5.4 Matthews Correlation Coefficient (MCC)
```rust
pub fn matthews_correlation_coefficient(confusion_matrix: &Array2<usize>) -> f64 {
    // MCC = (TP√óTN - FP√óFN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))
    // Range: [-1, 1]
    // Prend en compte toutes les 4 valeurs (TP/TN/FP/FN)
    // Meilleur que accuracy pour datasets d√©s√©quilibr√©s
}
```

---

## 6. M√©triques Sp√©cialis√©es

### 6.1 Object Detection
```rust
pub struct ObjectDetectionMetrics {
    pub map_50: f64,        // mAP @ IoU=0.5
    pub map_50_95: f64,     // mAP @ IoU=0.5:0.05:0.95
    pub ar_1: f64,          // Average Recall @ 1 detection
    pub ar_10: f64,         // Average Recall @ 10 detections
}
```

### 6.2 Segmentation
```rust
pub fn iou(pred_mask: &Array2<bool>, true_mask: &Array2<bool>) -> f64 {
    // Intersection over Union
    // = Area(intersection) / Area(union)
}

pub fn dice_coefficient(pred_mask: &Array2<bool>, true_mask: &Array2<bool>) -> f64 {
    // = 2 √ó |A ‚à© B| / (|A| + |B|)
    // √âquivalent √† F1 pour pixels
}
```

### 6.3 Ranking Metrics
```rust
pub fn mean_average_precision(predictions: &[Vec<f64>], targets: &[Vec<usize>]) -> f64 {
    // Pour syst√®mes de recommandation
}

pub fn ndcg(predictions: &[f64], relevances: &[f64], k: usize) -> f64 {
    // Normalized Discounted Cumulative Gain
    // Pour ranking et moteurs de recherche
}
```

---

## 7. Choix de M√©trique par Domaine

### Machine Learning G√©n√©ral

| Probl√®me | M√©trique Principale | M√©triques Secondaires |
|----------|---------------------|----------------------|
| **Classification Binaire √âquilibr√©e** | Accuracy | Precision, Recall, F1 |
| **Classification Binaire D√©s√©quilibr√©e** | F1, AUPRC | Recall, MCC |
| **Classification Multi-Classe** | Accuracy (macro) | Confusion Matrix, per-class F1 |
| **Ranking / Recommandation** | MAP, NDCG | Precision@K, Recall@K |
| **Probabilit√©s Calibr√©es** | Log Loss, Brier Score | Calibration Curve |

### Domaines Sp√©cifiques

| Domaine | M√©trique | Pourquoi |
|---------|----------|----------|
| **D√©tection Fraude** | Recall, AUPRC | Ne pas manquer fraudes (co√ªteux) |
| **Spam Filter** | Precision | Ne pas bloquer vrais emails |
| **Diagnostic M√©dical** | Sensitivity (Recall) | Ne pas manquer malades |
| **Vision par Ordinateur** | mAP, IoU | Standard pour benchmarks |
| **NLP (Sentiment Analysis)** | Accuracy, F1 (macro) | Classes peuvent √™tre d√©s√©quilibr√©es |
| **Search Engine** | NDCG, MAP | Ordre des r√©sultats important |

---

## 8. Bonnes Pratiques

### 8.1 Toujours Rapporter Plusieurs M√©triques
```rust
pub struct EvaluationReport {
    pub accuracy: f64,
    pub precision: f64,
    pub recall: f64,
    pub f1: f64,
    pub auc: f64,
    pub confusion_matrix: Array2<usize>,
}
```
**Pourquoi:** Une seule m√©trique peut √™tre trompeuse

### 8.2 Utiliser Cross-Validation
```rust
pub fn cross_validate_metrics(
    model_builder: impl Fn() -> Network,
    dataset: &Dataset,
    k_folds: usize
) -> Vec<EvaluationReport> {
    // Retourne m√©triques pour chaque fold
    // Donne incertitude sur performance
}
```

### 8.3 Stratification pour Datasets D√©s√©quilibr√©s
```rust
pub fn stratified_split(dataset: &Dataset, test_ratio: f64) 
    -> (Dataset, Dataset) {
    // Maintient proportions de classes dans train/test
    // Critique pour datasets d√©s√©quilibr√©s
}
```

### 8.4 Threshold Tuning
```rust
pub fn find_optimal_threshold(
    predictions: &[Array1<f64>],
    targets: &[Array1<f64>],
    metric: MetricType
) -> f64 {
    enum MetricType {
        MaxF1,          // Maximiser F1
        MaxAccuracy,    // Maximiser Accuracy
        TargetRecall(f64),  // Atteindre recall minimum
        TargetPrecision(f64), // Atteindre precision minimum
    }
}
```

---

## 9. Visualisation des M√©triques

### 9.1 Plots √† Impl√©menter
```rust
// N√©cessite int√©gration avec plotters ou similar
pub fn plot_roc_curve(fpr: &[f64], tpr: &[f64]) -> Result<(), Error>
pub fn plot_precision_recall_curve(precision: &[f64], recall: &[f64]) -> Result<(), Error>
pub fn plot_confusion_matrix_heatmap(matrix: &Array2<usize>) -> Result<(), Error>
pub fn plot_learning_curve(train_metrics: &[f64], val_metrics: &[f64]) -> Result<(), Error>
```

### 9.2 Export pour Outils Externes
```rust
pub fn export_metrics_csv(metrics: &EvaluationReport, path: &str) -> Result<(), Error>
pub fn export_metrics_json(metrics: &EvaluationReport, path: &str) -> Result<(), Error>
```

---

## 10. Roadmap M√©triques

### Phase 1: ‚úÖ Compl√©t√©e
- [x] Accuracy (binaire + multi-classes)
- [x] Precision, Recall, F1
- [x] Confusion Matrix
- [x] ROC Curve & AUC

### Phase 2: Recommand√© Prochainement
- [ ] Log Loss / Cross-Entropy
- [ ] Precision-Recall Curve & Average Precision
- [ ] Matthews Correlation Coefficient (MCC)
- [ ] Per-Class Metrics (multi-classe)

### Phase 3: Avanc√©
- [ ] Calibration Curve & Brier Score
- [ ] Bootstrap Confidence Intervals
- [ ] Multi-Class ROC (One-vs-Rest, One-vs-One)
- [ ] Threshold Optimization

### Phase 4: Sp√©cialis√©
- [ ] Object Detection (mAP, IoU)
- [ ] Ranking (NDCG, MAP)
- [ ] Regression (MAE, MSE, R¬≤)
- [ ] Time Series (MAPE, SMAPE)

---

## R√©f√©rences

1. **Scikit-learn Metrics** - https://scikit-learn.org/stable/modules/model_evaluation.html
2. **ROC Analysis** - Fawcett, T. (2006). "An introduction to ROC analysis"
3. **Precision-Recall vs ROC** - Davis & Goadrich (2006)
4. **Calibration** - Guo et al. (2017). "On Calibration of Modern Neural Networks"
5. **Multi-Class Metrics** - Sokolova & Lapalme (2009). "A systematic analysis of performance measures"
