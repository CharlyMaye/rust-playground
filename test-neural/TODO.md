# TODO - Am√©liorations du R√©seau de Neurones

## ‚úÖ Compl√©t√©

- [x] Impl√©mentation des fonctions d'activation configurables (15 fonctions)
- [x] Documentation d√©taill√©e de toutes les activations
- [x] Impl√©mentation des fonctions de perte (5 loss functions)
- [x] M√©thode `predict()` pour l'inf√©rence
- [x] M√©thode `predict_with_confidence()` pour estimation d'incertitude
- [x] M√©thode `evaluate()` pour calculer la loss sans update
- [x] Documentation compl√®te dans readme.md
- [x] **Architecture multi-couches** avec `Network::new_deep()`
- [x] Backpropagation g√©n√©ralis√©e pour N couches
- [x] Tests sur XOR avec r√©seaux profonds (2 et 3 couches)
- [x] **Initialisation des poids** (Xavier, He, LeCun) avec s√©lection automatique
- [x] **S√©rialisation** (save/load) avec module I/O externalis√©
- [x] **M√©triques d'√©valuation** (accuracy, precision, recall, F1, confusion matrix, ROC/AUC)

### R√©sultats Architecture Multi-Couches

‚úÖ **Fonctionne parfaitement :**
- R√©seau simple : 2 ‚Üí [5] ‚Üí 1 (1 couche cach√©e)
- R√©seau profond : 2 ‚Üí [5, 3] ‚Üí 1 (2 couches cach√©es)

‚ö†Ô∏è **Probl√®me identifi√© :**
- R√©seau tr√®s profond : 2 ‚Üí [8, 5, 3] ‚Üí 1 (3 couches) ‚Üí Ne converge pas (vanishing gradients)
- Reste bloqu√© √† 0.496 (pr√©diction = 0.5 partout)
- **Solution n√©cessaire :** Meilleure initialisation des poids + activation ReLU

---

## ‚úÖ S√©rialisation (Compl√©t√©e)

### Module I/O Externalis√©

La gestion de fichiers est compl√®tement **externe au r√©seau de neurones**, comme demand√©.

#### Structure

```rust
// src/io.rs - Module s√©par√© pour la persistance
pub fn save_json(network: &Network, path: &str) -> Result<(), IoError>
pub fn load_json(path: &str) -> Result<Network, IoError>
pub fn save_binary(network: &Network, path: &str) -> Result<(), IoError>
pub fn load_binary(path: &str) -> Result<Network, IoError>
pub fn get_serialized_size(network: &Network) -> (usize, usize)  // (json, bincode)
```

#### Formats Support√©s

1. **JSON** (`save_json`, `load_json`)
   - ‚úÖ Human-readable
   - ‚úÖ Editable manuellement
   - ‚úÖ Compatible multi-plateformes
   - ‚ö†Ô∏è Plus volumineux (~660 bytes pour XOR)

2. **Binary** (`save_binary`, `load_binary`)
   - ‚úÖ Compact (280 bytes pour XOR)
   - ‚úÖ Compression ~2.35x vs JSON
   - ‚úÖ Performant
   - ‚ö†Ô∏è Non-lisible

#### R√©sultats Tests XOR

```
Training: loss 0.0001 en 10000 epochs
JSON: 659 bytes
Binary: 280 bytes
Ratio: 2.35x compression

Loaded predictions:
[0,0] -> 0.000 ‚úì
[0,1] -> 1.000 ‚úì
[1,0] -> 1.000 ‚úì
[1,1] -> 0.000 ‚úì
```

#### Avantages Architecture

‚úÖ **S√©paration des responsabilit√©s :**
- `Network` : logique d'apprentissage
- `io` module : persistance et I/O
- Pas de m√©thodes `save()`/`load()` dans `Network`

‚úÖ **Flexibilit√© :**
- Plusieurs formats disponibles (JSON, bincode)
- Facile d'ajouter d'autres formats (YAML, MessagePack...)
- Pas de couplage fort

‚úÖ **Testable :**
- Tests unitaires dans le module `io`
- Mock du syst√®me de fichiers possible
- Validation ind√©pendante

---

## ‚úÖ M√©triques d'√âvaluation (Compl√©t√©es)

### Module `metrics.rs` - √âvaluation Externalis√©e

- [x] **Accuracy** - Pourcentage de pr√©dictions correctes (binaire + multi-classes)
- [x] **Binary Metrics** - Precision, Recall, F1-Score, TP/FP/TN/FN
- [x] **Confusion Matrix** - 2x2 (binaire) et NxN (multi-classes)
- [x] **ROC Curve & AUC** - Courbe ROC et aire sous la courbe

**R√©sultats Tests (XOR):**
```
Perfect: Accuracy=100%, Precision=1.0, Recall=1.0, F1=1.0
Imperfect: Accuracy=75%, Precision=1.0, Recall=0.5, F1=0.667
```

**Architecture:**
- Module s√©par√© `metrics.rs` (ind√©pendant de Network)
- Tests unitaires complets  
- Support binaire et multi-classes
- Exemple: `cargo run --example metrics_demo`

---

## Prochaines Priorit√©s


### 1. **Optimiseurs Avanc√©s (Adam, RMSprop)** üöÄ
Convergence plus rapide et stable

- [ ] **Enum `Optimizer`**
  ```rust
  pub enum Optimizer {
      SGD { learning_rate: f64 },
      Momentum { learning_rate: f64, beta: f64 },
      RMSprop { learning_rate: f64, beta: f64, epsilon: f64 },
      Adam { learning_rate: f64, beta1: f64, beta2: f64, epsilon: f64 },
      AdamW { learning_rate: f64, beta1: f64, beta2: f64, epsilon: f64, weight_decay: f64 },
  }
  ```

- [ ] **Adam Optimizer** (Priorit√© #1)
  - Adapte le learning rate par param√®tre
  - Converge 2-10x plus vite que SGD
  - √âtat : `m` (momentum) et `v` (variance) par poids
  - Standard moderne pour deep learning

- [ ] **RMSprop**
  - Adaptatif comme Adam mais plus simple
  - Bon pour RNN et probl√®mes non-stationnaires

- [ ] **Momentum**
  - Acc√©l√®re SGD dans les bonnes directions
  - R√©duit les oscillations

- [ ] **Learning Rate Scheduling**
  ```rust
  pub enum LRSchedule {
      Constant(f64),
      StepDecay { initial: f64, drop: f64, epochs_drop: usize },
      ExponentialDecay { initial: f64, decay_rate: f64 },
      CosineAnnealing { initial: f64, min_lr: f64, period: usize },
  }
  ```

---

### 2. **R√©gularisation** üõ°Ô∏è
√âviter l'overfitting et am√©liorer la g√©n√©ralisation

- [ ] **Dropout**
  ```rust
  pub struct Layer {
      weights: Array2<f64>,
      biases: Array1<f64>,
      activation: Activation,
      dropout_rate: Option<f64>,  // Nouveau
  }
  ```
  - D√©sactive al√©atoirement p% des neurones
  - Mode training vs inference
  - Recommand√© : 0.2-0.5 pour couches cach√©es

- [ ] **L2 Regularization (Weight Decay)**
  ```rust
  loss = loss + lambda * weights.mapv(|w| w * w).sum()
  gradient = gradient + lambda * weights
  ```
  - P√©nalise les poids trop grands
  - Typique : lambda = 0.0001 - 0.01

- [ ] **L1 Regularization**
  - Encourage la sparsit√© (poids √† z√©ro)
  - S√©lection automatique de features

- [ ] **Early Stopping**
  - Arr√™te l'entra√Ænement si val_loss n'am√©liore plus
  - Param√®tre `patience` (nombre d'epochs sans am√©lioration)

- [ ] **Batch Normalization**
  ```rust
  pub struct BatchNorm {
      gamma: Array1<f64>,      // Scale (learnable)
      beta: Array1<f64>,       // Shift (learnable)
      running_mean: Array1<f64>,
      running_var: Array1<f64>,
      momentum: f64,
      epsilon: f64,
  }
  ```
  - Normalise les activations par batch
  - Acc√©l√®re convergence
  - R√©duit vanishing gradients

---

### 4. **Mini-Batch Training** üì¶
Scalabilit√© sur gros datasets

- [ ] **Dataset Struct**
  ```rust
  pub struct Dataset {
      inputs: Vec<Array1<f64>>,
      targets: Vec<Array1<f64>>,
  }
  
  impl Dataset {
      pub fn shuffle(&mut self);
      pub fn split(&self, ratios: (f64, f64, f64)) 
          -> (Dataset, Dataset, Dataset);  // train, val, test
      pub fn batches(&self, batch_size: usize) -> BatchIterator;
  }
  ```

- [ ] **M√©thode `train_batch()`**
  ```rust
  pub fn train_batch(&mut self, 
                     batch: &[(Array1<f64>, Array1<f64>)], 
                     optimizer: &Optimizer)
  ```
  - Accumule gradients sur le batch
  - Update poids une fois par batch
  - 10-100x plus rapide que SGD pur

- [ ] **Strat√©gies de Batching**
  - Batch size typique : 16, 32, 64, 128
  - Trade-off : vitesse vs qualit√© du gradient
  - Plus petit batch = plus de bruit (peut aider la g√©n√©ralisation)

- [ ] **Shuffling**
  - M√©langer les donn√©es avant chaque epoch
  - √âvite l'apprentissage de l'ordre

---

### 4. **Callbacks et Contr√¥le de l'Entra√Ænement** üéõÔ∏è
Monitoring et automation

- [ ] **Trait `Callback`**
  ```rust
  pub trait Callback {
      fn on_epoch_begin(&mut self, epoch: usize);
      fn on_epoch_end(&mut self, epoch: usize, metrics: &Metrics);
      fn on_train_begin(&mut self);
      fn on_train_end(&mut self);
      fn should_stop(&self) -> bool;
  }
  ```

- [ ] **EarlyStopping Callback**
  ```rust
  pub struct EarlyStopping {
      patience: usize,
      best_loss: f64,
      wait: usize,
      restore_best_weights: bool,
  }
  ```
  - Arr√™te si val_loss ne s'am√©liore pas
  - Restaure les meilleurs poids

- [ ] **ModelCheckpoint Callback**
  ```rust
  pub struct ModelCheckpoint {
      filepath: String,
      save_best_only: bool,
      monitor: String,  // "loss" ou "val_loss"
  }
  ```
  - Sauvegarde automatique du meilleur mod√®le
  - √âvite de perdre le progr√®s

- [ ] **LearningRateScheduler Callback**
  - Ajuste le learning rate pendant l'entra√Ænement
  - Warmup, decay, cyclic LR

- [ ] **ProgressBar et Logging**
  - Affichage temps r√©el : epoch, loss, metrics
  - Estimation du temps restant
  - Logging dans fichier CSV/JSON

---

### 6. **Architecture et Validation** üèóÔ∏è

- [ ] **M√©thode `fit()` Compl√®te**
  ```rust
  pub fn fit(&mut self,
             train_data: &Dataset,
             validation_data: Option<&Dataset>,
             epochs: usize,
             batch_size: usize,
             optimizer: Optimizer,
             callbacks: Vec<Box<dyn Callback>>) -> History
  ```
  - Interface unifi√©e pour l'entra√Ænement
  - Validation automatique √† chaque epoch
  - Retourne historique (loss, metrics par epoch)

- [ ] **Cross-Validation**
  ```rust
  pub fn cross_validate(network_builder: impl Fn() -> Network,
                        dataset: &Dataset,
                        k_folds: usize) -> Vec<f64>
  ```
  - K-fold cross-validation
  - √âvaluation robuste sur petits datasets

- [ ] **Grid Search / Random Search**
  - Recherche automatique d'hyperparam√®tres
  - Learning rate, architecture, dropout rate, etc.

---

### 7. **Datasets et Benchmarks** üìä

- [ ] **Chargeurs de Datasets Standard**
  ```rust
  pub fn load_mnist() -> (Dataset, Dataset)
  pub fn load_iris() -> Dataset
  pub fn load_wine() -> Dataset
  ```
  - MNIST : 28x28 images de chiffres
  - Iris : classification de fleurs (classique)
  - Wine : classification de vins

- [ ] **Data Augmentation**
  - Rotation, flip, noise pour images
  - Augmente artificiellement le dataset
  - Am√©liore g√©n√©ralisation

- [ ] **Normalisation**
  ```rust
  pub fn normalize(&mut self, method: NormMethod)
  
  pub enum NormMethod {
      MinMax,           // [0, 1]
      StandardScore,    // mean=0, std=1
      MaxAbs,           // [-1, 1]
  }
  ```

---

### 8. **Visualisation et Debug** üîç

- [ ] **Visualisation des Poids**
  ```rust
  pub fn visualize_weights(&self, layer: usize) -> Array2<f64>
  ```
  - Comprendre ce que le r√©seau a appris

- [ ] **Activation Maps**
  - Voir quels neurones s'activent pour une entr√©e donn√©e

- [ ] **Gradient Flow Analysis**
  - D√©tecter vanishing/exploding gradients
  - Norms des gradients par couche

- [ ] **Learning Curves**
  - Plot train_loss vs val_loss
  - D√©tecter overfitting/underfitting

---

### 9. **Performance et Optimisation** ‚ö°

- [ ] **Parallelisation**
  - Utiliser `rayon` pour parall√©liser batch processing
  - Multi-threading pour forward/backward pass

- [ ] **SIMD Optimizations**
  - Vectorisation avec instructions CPU modernes
  - ndarray supporte d√©j√† partiellement

- [ ] **GPU Support** (Long terme)
  - Int√©gration avec `wgpu` ou `cudarc`
  - 10-100x speedup sur gros r√©seaux

- [ ] **Quantization**
  - R√©duire pr√©cision (f32 ‚Üí f16, int8)
  - Inference plus rapide, moins de m√©moire

---

### 10. **Architecture Avanc√©es** üß†

#### Convolutional Neural Networks (CNN)
- [ ] **Conv2D Layer**
  ```rust
  pub struct Conv2D {
      filters: Array4<f64>,  // [num_filters, channels, height, width]
      stride: (usize, usize),
      padding: Padding,
  }
  ```
- [ ] **MaxPool2D / AvgPool2D**
- [ ] **Flatten Layer**
- [ ] Example : LeNet-5, ResNet basique

#### Recurrent Neural Networks (RNN)
- [ ] **LSTM Cell**
  ```rust
  pub struct LSTM {
      input_size: usize,
      hidden_size: usize,
      // Gates : forget, input, output
  }
  ```
- [ ] **GRU Cell** (version simplifi√©e de LSTM)
- [ ] **Bidirectional RNN**
- [ ] Example : classification de s√©quences

#### Attention Mechanisms
- [ ] **Multi-Head Attention**
- [ ] **Transformer Block** (tr√®s long terme)

---

## üéØ Roadmap Recommand√©e

### Phase 1 : M√©triques et Optimisation (1-2 semaines)
1. ‚úÖ S√©rialisation (FAIT)
2. **M√©thode `accuracy()`** ‚Üê Commencer ici
3. Adam optimizer
4. Mini-batch training basique

### Phase 2 : R√©gularisation (1 semaine)
1. Dropout
2. L2 regularization
3. Early stopping
4. Dataset struct avec split/shuffle

### Phase 3 : Production Ready (1-2 semaines)
1. Callbacks (EarlyStopping, ModelCheckpoint)
2. M√©thode `fit()` unifi√©e
3. Cross-validation
4. Chargeurs de datasets (MNIST, Iris)

### Phase 4 : Architectures Avanc√©es (Long terme)
1. CNN layers
2. RNN/LSTM
3. GPU support

---



## ‚úÖ Initialisation des Poids (Compl√©t√©e)

**Probl√®me r√©solu !** L'initialisation Xavier/He permet maintenant aux r√©seaux profonds de converger.

#### R√©sultats avec XOR

**Avant (Uniform -1..1) :**
- ‚úÖ R√©seau simple : 2 ‚Üí [5] ‚Üí 1 (converge)
- ‚úÖ R√©seau 2 couches : 2 ‚Üí [5, 3] ‚Üí 1 (converge)  
- ‚ùå R√©seau 3 couches : 2 ‚Üí [8, 5, 3] ‚Üí 1 (ne converge PAS)

**Apr√®s (Xavier/He automatique) :**
- ‚úÖ R√©seau simple : 2 ‚Üí [5] ‚Üí 1 (converge)
- ‚úÖ R√©seau 2 couches : 2 ‚Üí [5, 3] ‚Üí 1 (converge)
- ‚úÖ R√©seau 3 couches : 2 ‚Üí [8, 5, 3] ‚Üí 1 (**converge maintenant !** avec lr=0.3, 100k epochs)

#### Impl√©mentation

- [x] Enum `WeightInit { Uniform, Xavier, He, LeCun }`
- [x] M√©thode automatique `WeightInit::for_activation()` 
- [x] M√©thode `new_deep_with_init()` pour contr√¥le manuel
- [x] Distribution gaussienne via Box-Muller transform
- [x] Biases initialis√©s √† z√©ro (recommand√©)
- [x] Tests sur XOR avec r√©seaux profonds

#### Mapping Impl√©ment√©

| **Activation** | **Initialisation Auto** |
|----------------|-------------------------|
| Sigmoid, Tanh, Softsign, HardSigmoid, HardTanh, Softmax | Xavier |
| ReLU, LeakyReLU, ELU, GELU, Swish, Mish, Softplus | He |
| SELU | LeCun |
| Linear | Xavier |

---

## üîÑ Priorit√©s Suivantes

| XOR, probl√®mes simples | 1-2 couches cach√©es | Suffisant |
| MNIST (chiffres) | 2-3 couches | Patterns simples |
| Images (CIFAR, ImageNet) | 10-50+ couches | Hi√©rarchie complexe |
| Traitement du langage | 12-96+ couches (Transformers) | Relations longue distance |
| Jeux (AlphaGo) | 40+ couches | Strat√©gie complexe |

---

### 2. Initialisation des Poids

**Probl√®me actuel :** Initialisation uniforme `random_range(-1.0..1.0)` ne prend pas en compte :
- La taille de la couche
- Le type d'activation utilis√©
- Risque de gradients qui disparaissent/explosent

#### M√©thodes d'Initialisation

##### Uniform (Actuelle)
```rust
weight = rng.random_range(-1.0..1.0)
```
‚úÖ Simple, fonctionne pour r√©seaux peu profonds  
‚ùå Pas adapt√©e aux r√©seaux profonds

##### Xavier/Glorot Initialization

Pour Tanh et Sigmoid :
```rust
let std = (2.0 / (input_size + output_size) as f64).sqrt();
let weight = rng.sample::<f64, _>(StandardNormal) * std;
```
‚úÖ Maintient la variance constante √† travers les couches  
‚úÖ Id√©al pour activations sym√©triques (Tanh, Softsign)

##### He Initialization

Pour ReLU et variantes :
```rust
let std = (2.0 / input_size as f64).sqrt();
let weight = rng.sample::<f64, _>(StandardNormal) * std;
```
‚úÖ Compense pour les neurones "morts" de ReLU  
‚úÖ Standard moderne pour r√©seaux profonds

##### LeCun Initialization

Pour SELU :
```rust
let std = (1.0 / input_size as f64).sqrt();
let weight = rng.sample::<f64, _>(StandardNormal) * std;
```

#### Impl√©mentation

- [x] Enum `WeightInit { Uniform, Xavier, He, LeCun }`
- [x] Adapter l'initialisation selon l'activation choisie
- [ ] Initialisation automatique bas√©e sur l'activation
- [ ] Benchmark comparatif des m√©thodes

#### Mapping Recommand√©

| **Activation** | **Initialisation Recommand√©e** |
|----------------|-------------------------------|
| Sigmoid, Tanh | Xavier/Glorot |
| ReLU, LeakyReLU, ELU | He |
| SELU | LeCun |
| GELU, Swish, Mish | He (exp√©rimental) |
| Softmax | Xavier |

---

### 3. Optimiseurs Avanc√©s

#### Actuellement : SGD Simple

```rust
weight = weight - learning_rate * gradient
```

#### Adam Optimizer

```rust
m = beta1 * m + (1 - beta1) * gradient          // Momentum
v = beta2 * v + (1 - beta2) * gradient^2        // RMSprop
m_hat = m / (1 - beta1^t)                        // Bias correction
v_hat = v / (1 - beta2^t)
weight = weight - learning_rate * m_hat / (sqrt(v_hat) + epsilon)
```

**Avantages :**
- Adapte le learning rate par param√®tre
- Converge plus vite
- Plus stable

- [ ] Impl√©menter enum `Optimizer { SGD, Momentum, RMSprop, Adam, AdamW }`
- [ ] Ajouter √©tats optimizer dans Network (m, v pour Adam)
- [ ] Modifier train() pour utiliser l'optimizer choisi

---

### 4. R√©gularisation

#### Dropout

D√©sactive al√©atoirement p% des neurones pendant l'entra√Ænement :

```rust
if training {
    let mask: Array1<f64> = Array1::from_shape_fn(size, |_| {
        if rng.gen::<f64>() > dropout_rate { 1.0 / (1.0 - dropout_rate) } else { 0.0 }
    });
    hidden = hidden * mask;
}
```

- [ ] Ajouter param√®tre `dropout_rate` par couche
- [ ] Mode training/inference distinct
- [ ] D√©sactiver dropout en inference

#### L1/L2 Regularization

P√©nalise les poids trop grands :

```rust
// L2 (Weight Decay)
loss = loss + lambda * (weights^2).sum()
gradient = gradient + lambda * weights

// L1 (Lasso)
loss = loss + lambda * |weights|.sum()
gradient = gradient + lambda * sign(weights)
```

- [ ] Ajouter param√®tre `l2_lambda` dans Network
- [ ] Modifier calcul des gradients

---

### 5. Batch Normalization

Normalise les activations de chaque couche :

```rust
// Training
mean = batch.mean()
var = batch.var()
x_normalized = (x - mean) / sqrt(var + epsilon)
output = gamma * x_normalized + beta  // Param√®tres apprenables

// Inference
output = gamma * (x - running_mean) / sqrt(running_var + epsilon) + beta
```

**Avantages :**
- Acc√©l√®re la convergence
- R√©duit le probl√®me des gradients qui disparaissent
- Permet d'utiliser des learning rates plus √©lev√©s

- [ ] Impl√©menter struct `BatchNorm`
- [ ] Maintenir running_mean et running_var
- [ ] Mode training/inference

---

### 6. S√©rialisation (Sauvegarder/Charger le Mod√®le)

```rust
// Sauvegarder
network.save("model.bin")?;
network.save_json("model.json")?;

// Charger
let network = Network::load("model.bin")?;
```

- [ ] Impl√©menter `serde::Serialize` et `Deserialize`
- [ ] M√©thodes `save()` et `load()`
- [ ] Support formats : binaire (bincode), JSON, ONNX

---

### 7. M√©triques d'√âvaluation

```rust
// Accuracy
let accuracy = network.accuracy(&test_inputs, &test_targets);

// Precision, Recall, F1
let (precision, recall, f1) = network.metrics(&test_inputs, &test_targets);

// Confusion Matrix
let confusion = network.confusion_matrix(&test_inputs, &test_targets);
```

- [ ] M√©thode `accuracy()`
- [ ] M√©thode `precision_recall_f1()`
- [ ] M√©thode `confusion_matrix()`
- [ ] Courbes ROC/AUC pour classification

---

### 8. Dataset Helpers

```rust
// Train/validation/test split
let (train, val, test) = dataset.split(0.7, 0.15, 0.15);

// Mini-batches
for batch in train.batches(batch_size=32) {
    network.train_batch(batch.inputs, batch.targets, lr);
}

// Data augmentation (images)
let augmented = dataset.augment(rotation=15, flip=true);
```

- [ ] Struct `Dataset` avec split
- [ ] Iterator pour mini-batches
- [ ] Shuffle avant chaque epoch
- [ ] Data augmentation basique

---

### 9. Callbacks et Logging

```rust
let callbacks = vec![
    EarlyStopping::new(patience=10),
    ModelCheckpoint::new("best_model.bin"),
    LearningRateScheduler::new(|epoch| 0.1 * 0.95_f64.powi(epoch)),
];

network.fit(
    &train_inputs, &train_targets,
    epochs=100,
    validation_data=(&val_inputs, &val_targets),
    callbacks=callbacks
);
```

- [ ] Trait `Callback`
- [ ] `EarlyStopping`
- [ ] `ModelCheckpoint`
- [ ] `LearningRateScheduler`
- [ ] `TensorBoard` logging (optionnel)

---

### 10. Architectures Sp√©cialis√©es

#### CNN (Convolutional Neural Networks)

Pour images :
```rust
let cnn = CNN::new()
    .add(Conv2D::new(32, kernel_size=3))
    .add(MaxPool2D::new(pool_size=2))
    .add(Conv2D::new(64, kernel_size=3))
    .add(Flatten::new())
    .add(Dense::new(128))
    .add(Dense::new(10));
```

- [ ] Couches convolutionnelles (`Conv2D`)
- [ ] Pooling layers (`MaxPool2D`, `AvgPool2D`)
- [ ] Padding et stride

#### RNN (Recurrent Neural Networks)

Pour s√©quences :
```rust
let rnn = RNN::new()
    .add(LSTM::new(128))
    .add(Dense::new(10));
```

- [ ] LSTM cells
- [ ] GRU cells
- [ ] Bidirectional RNN

---

## üìö R√©f√©rences

### Livres
- **"Deep Learning"** by Goodfellow, Bengio, Courville
- **"Neural Networks from Scratch"** by Harrison Kinsley
- **"Hands-On Machine Learning"** by Aur√©lien G√©ron

### Papers
- **Dropout:** Srivastava et al., 2014
- **Batch Normalization:** Ioffe & Szegedy, 2015
- **Adam:** Kingma & Ba, 2015
- **ResNet:** He et al., 2015
- **Transformers:** Vaswani et al., 2017

### Ressources Rust
- [burn](https://github.com/tracel-ai/burn) - Deep learning framework en Rust
- [candle](https://github.com/huggingface/candle) - ML framework Hugging Face
- [tch-rs](https://github.com/LaurentMazare/tch-rs) - Rust bindings pour PyTorch

---

## üí° Notes Techniques

### Design Patterns Rust
- **Builder Pattern** : Pour construction flexible des r√©seaux
- **Type Safety** : Utiliser types phantom pour valider architecture √† compile-time
- **Zero-Cost Abstractions** : Pas de runtime overhead pour les abstractions
- **Error Handling** : `Result<T, E>` partout, jamais de panic en production

### Best Practices
- Tests unitaires pour chaque feature
- Benchmarks avec `criterion`
- Documentation avec exemples ex√©cutables (`cargo test --doc`)
- CI/CD avec GitHub Actions

### Performance Tips
- `ndarray` avec BLAS (OpenBLAS, MKL) pour algebra lin√©aire
- Profile avec `perf`, `flamegraph`
- √âviter allocations inutiles dans boucles d'entra√Ænement
- `cargo build --release` donne 10-100x speedup vs debug

---

## üìö R√©f√©rences Techniques

### Papers Fondamentaux
- **Dropout:** Srivastava et al., 2014 - "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
- **Batch Normalization:** Ioffe & Szegedy, 2015 - "Batch Normalization: Accelerating Deep Network Training"
- **Adam:** Kingma & Ba, 2015 - "Adam: A Method for Stochastic Optimization"
- **Xavier Init:** Glorot & Bengio, 2010 - "Understanding the difficulty of training deep feedforward neural networks"
- **He Init:** He et al., 2015 - "Delving Deep into Rectifiers"

### Frameworks Rust ML/DL
- **burn** - Framework complet, tr√®s prometteur
- **candle** - Par Hugging Face, l√©ger et rapide
- **tch-rs** - Bindings PyTorch pour Rust
- **linfa** - Scikit-learn-like pour Rust

### Datasets
- **MNIST** : 60k images de chiffres manuscrits
- **CIFAR-10** : 60k images 32x32 (10 classes)
- **Iris** : 150 samples, classification florale
- **Boston Housing** : R√©gression de prix
