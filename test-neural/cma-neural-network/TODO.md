# TODO - Am√©liorations du R√©seau de Neurones

## ‚úÖ R√©cemment Accompli

### Corrections Math√©matiques (v0.2)
- [x] **S√©paration pr√©-activation / post-activation**
  - Nouvelle m√©thode `derivative_from_preactivation(z)` pour calculs corrects
  - Les d√©riv√©es GELU, Mish, Swish, SELU, ELU, Softplus sont maintenant math√©matiquement exactes
  - Structure `ForwardResult` stocke z et a s√©par√©ment

- [x] **Dropout complet en backward**
  - Les masques dropout sont stock√©s et r√©appliqu√©s au gradient
  - Inverted dropout correctement impl√©ment√©

- [x] **Reproductibilit√©**
  - M√©thode `set_seed(u64)` pour entra√Ænement d√©terministe
  - RNG stock√© dans Network pour √©viter recr√©ations r√©p√©t√©es

- [x] **S√©curit√© Softmax**
  - `debug_assert!` ajout√© pour pr√©venir usage incorrect de la d√©riv√©e g√©n√©rique

---

## üéØ Prochaines Priorit√©s

### 1. **Datasets et Benchmarks** üìä

- [ ] **Chargeurs de Datasets Standard**
  ```rust
  pub fn load_mnist() -> (Dataset, Dataset)
  pub fn load_iris() -> Dataset
  pub fn load_wine() -> Dataset
  ```
  - MNIST : 28x28 images de chiffres
  - Iris : classification de fleurs (classique)
  - Wine : classification de vins

- [ ] **Data Augmentation**
  - Rotation, flip, noise pour images
  - Augmente artificiellement le dataset
  - Am√©liore g√©n√©ralisation

- [ ] **Normalisation**
  ```rust
  pub fn normalize(&mut self, method: NormMethod)
  
  pub enum NormMethod {
      MinMax,           // [0, 1]
      StandardScore,    // mean=0, std=1
      MaxAbs,           // [-1, 1]
  }
  ```

---

### 2. **Cross-Validation** üîÑ

- [ ] **K-Fold Cross-Validation**
  ```rust
  pub fn cross_validate(
      dataset: &Dataset, 
      k: usize, 
      network_builder: impl Fn() -> Network
  ) -> Vec<f64>
  ```
  - Divise le dataset en k folds
  - Entra√Æne k fois (chaque fold sert une fois de validation)
  - Retourne les k scores
  - Moyenne pour score final

- [ ] **Stratified K-Fold**
  - Pr√©serve la distribution des classes dans chaque fold
  - Essentiel pour datasets d√©s√©quilibr√©s

---

### 3. **Hyperparameter Search** üîç

- [ ] **Grid Search**
  ```rust
  pub struct GridSearch {
      learning_rates: Vec<f64>,
      hidden_sizes: Vec<Vec<usize>>,
      dropout_rates: Vec<f64>,
  }
  ```
  - Teste toutes les combinaisons
  - Lent mais exhaustif

- [ ] **Random Search**
  - √âchantillonne al√©atoirement l'espace des hyperparam√®tres
  - Plus efficace que Grid Search en haute dimension

---

### 4. **Visualisation et Debug** üîç

- [ ] **Visualisation des Poids**
  ```rust
  pub fn visualize_weights(&self, layer: usize) -> Array2<f64>
  ```
  - Comprendre ce que le r√©seau a appris

- [ ] **Activation Maps**
  - Voir quels neurones s'activent pour une entr√©e donn√©e

- [ ] **Gradient Flow Analysis**
  - D√©tecter vanishing/exploding gradients
  - Norms des gradients par couche

- [ ] **Learning Curves**
  - Plot train_loss vs val_loss
  - D√©tecter overfitting/underfitting

---

### 5. **Performance et Optimisation** ‚ö°

- [ ] **Parallelisation**
  - Utiliser `rayon` pour parall√©liser batch processing
  - Multi-threading pour forward/backward pass

- [ ] **SIMD Optimizations**
  - Vectorisation avec instructions CPU modernes
  - ndarray supporte d√©j√† partiellement

- [ ] **GPU Support** (Long terme)
  - Int√©gration avec `wgpu` ou `cudarc`
  - 10-100x speedup sur gros r√©seaux

- [ ] **Quantization**
  - R√©duire pr√©cision (f32 ‚Üí f16, int8)
  - Inference plus rapide, moins de m√©moire

---

### 6. **Architecture Avanc√©es** üß†

#### Convolutional Neural Networks (CNN)
- [ ] **Conv2D Layer**
  ```rust
  pub struct Conv2D {
      filters: Array4<f64>,  // [num_filters, channels, height, width]
      stride: (usize, usize),
      padding: Padding,
  }
  ```
- [ ] **MaxPool2D / AvgPool2D**
- [ ] **Flatten Layer**
- [ ] Example : LeNet-5, ResNet basique

#### Recurrent Neural Networks (RNN)
- [ ] **LSTM Cell**
  ```rust
  pub struct LSTM {
      input_size: usize,
      hidden_size: usize,
      // Gates : forget, input, output
  }
  ```
- [ ] **GRU Cell** (version simplifi√©e de LSTM)
- [ ] **Bidirectional RNN**
- [ ] Example : classification de s√©quences

#### Attention Mechanisms
- [ ] **Multi-Head Attention**
- [ ] **Transformer Block** (tr√®s long terme)

---

## üìö R√©f√©rences

### Livres
- **"Deep Learning"** by Goodfellow, Bengio, Courville
- **"Neural Networks from Scratch"** by Harrison Kinsley
- **"Hands-On Machine Learning"** by Aur√©lien G√©ron

### Papers
- **Dropout:** Srivastava et al., 2014
- **Batch Normalization:** Ioffe & Szegedy, 2015
- **Adam:** Kingma & Ba, 2015
- **ResNet:** He et al., 2015
- **Transformers:** Vaswani et al., 2017

### Ressources Rust
- [burn](https://github.com/tracel-ai/burn) - Deep learning framework en Rust
- [candle](https://github.com/huggingface/candle) - ML framework Hugging Face
- [tch-rs](https://github.com/LaurentMazare/tch-rs) - Rust bindings pour PyTorch

---

## üí° Notes Techniques

### Design Patterns Rust
- **Builder Pattern** : Pour construction flexible des r√©seaux
- **Type Safety** : Utiliser types phantom pour valider architecture √† compile-time
- **Zero-Cost Abstractions** : Pas de runtime overhead pour les abstractions
- **Error Handling** : `Result<T, E>` partout, jamais de panic en production

### Best Practices
- Tests unitaires pour chaque feature
- Benchmarks avec `criterion`
- Documentation avec exemples ex√©cutables (`cargo test --doc`)
- CI/CD avec GitHub Actions

### Performance Tips
- `ndarray` avec BLAS (OpenBLAS, MKL) pour algebra lin√©aire
- Profile avec `perf`, `flamegraph`
- √âviter allocations inutiles dans boucles d'entra√Ænement
- `cargo build --release` donne 10-100x speedup vs debug

---

## üìö R√©f√©rences Techniques

### Papers Fondamentaux
- **Dropout:** Srivastava et al., 2014 - "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
- **Batch Normalization:** Ioffe & Szegedy, 2015 - "Batch Normalization: Accelerating Deep Network Training"
- **Adam:** Kingma & Ba, 2015 - "Adam: A Method for Stochastic Optimization"
- **Xavier Init:** Glorot & Bengio, 2010 - "Understanding the difficulty of training deep feedforward neural networks"
- **He Init:** He et al., 2015 - "Delving Deep into Rectifiers"

### Frameworks Rust ML/DL
- **burn** - Framework complet, tr√®s prometteur
- **candle** - Par Hugging Face, l√©ger et rapide
- **tch-rs** - Bindings PyTorch pour Rust
- **linfa** - Scikit-learn-like pour Rust

### Datasets
- **MNIST** : 60k images de chiffres manuscrits
- **CIFAR-10** : 60k images 32x32 (10 classes)
- **Iris** : 150 samples, classification florale
- **Boston Housing** : R√©gression de prix
