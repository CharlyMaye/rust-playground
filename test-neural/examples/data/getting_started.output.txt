â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Test Neural - Getting Started Guide                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ 1. Data preparation (XOR problem)

   Train: 320 samples | Validation: 80 samples

ğŸ”§ 2. Building a network with the Builder Pattern

   âœ“ Network created: 2 â†’ [8] â†’ 1
   âœ“ Activation: Tanh â†’ Sigmoid
   âœ“ Optimizer: Adam (lr=0.01)

ğŸ›¡ï¸  3. Network with regularization (Dropout + L2)

   âœ“ Architecture: 2 â†’ [16, 8] â†’ 1
   âœ“ Dropout: 0.2 (prevents overfitting)
   âœ“ L2: 0.001 (penalizes large weights)

âš¡ 4. Quick optimizer comparison

   SGD        â†’ Final loss: 0.693186
   Momentum   â†’ Final loss: 0.000458
   Adam       â†’ Final loss: 0.002354

ğŸ“Š 5. Training with callbacks

   Configuration:
   â€¢ EarlyStopping (patience=15, 0.1% relative improvement)
   â€¢ LR Scheduler (ReduceOnPlateau)

ğŸš€ DÃ©but de l'entraÃ®nement (1000 epochs)
Epoch 10/1000 [1.0%] - train_loss: 0.008165 - val_loss: 0.008165 - ETA: 22s   
Epoch 20/1000 [2.0%] - train_loss: 0.001830 - val_loss: 0.001830 - ETA: 21s   
Epoch 30/1000 [3.0%] - train_loss: 0.000852 - val_loss: 0.000852 - ETA: 21s   
Epoch 40/1000 [4.0%] - train_loss: 0.000509 - val_loss: 0.000509 - ETA: 21s   
Epoch 50/1000 [5.0%] - train_loss: 0.000345 - val_loss: 0.000345 - ETA: 20s   
Epoch 60/1000 [6.0%] - train_loss: 0.000251 - val_loss: 0.000251 - ETA: 20s   
Epoch 64/1000 [6.4%] - train_loss: 0.000225 - val_loss: 0.000225 - ETA: 20s   
ğŸ“‰ LR Scheduler: Epoch 64 - Plateau detected, reducing LR 0.050000 â†’ 0.025000
Epoch 70/1000 [7.0%] - train_loss: 0.000205 - val_loss: 0.000205 - ETA: 20s   
Epoch 74/1000 [7.4%] - train_loss: 0.000195 - val_loss: 0.000195 - ETA: 20s   
ğŸ“‰ LR Scheduler: Epoch 74 - Plateau detected, reducing LR 0.025000 â†’ 0.012500
Epoch 80/1000 [8.0%] - train_loss: 0.000186 - val_loss: 0.000186 - ETA: 20s   
Epoch 85/1000 [8.5%] - train_loss: 0.000180 - val_loss: 0.000180 - ETA: 20s   
ğŸ“‰ LR Scheduler: Epoch 85 - Plateau detected, reducing LR 0.012500 â†’ 0.006250
Epoch 90/1000 [9.0%] - train_loss: 0.000177 - val_loss: 0.000177 - ETA: 20s   
Epoch 95/1000 [9.5%] - train_loss: 0.000174 - val_loss: 0.000174 - ETA: 20s   
ğŸ“‰ LR Scheduler: Epoch 95 - Plateau detected, reducing LR 0.006250 â†’ 0.003125
Epoch 100/1000 [10.0%] - train_loss: 0.000172 - val_loss: 0.000172 - ETA: 19s   
Epoch 105/1000 [10.5%] - train_loss: 0.000170 - val_loss: 0.000170 - ETA: 19s   
ğŸ“‰ LR Scheduler: Epoch 105 - Plateau detected, reducing LR 0.003125 â†’ 0.001563
Epoch 110/1000 [11.0%] - train_loss: 0.000170 - val_loss: 0.000170 - ETA: 19s   
Epoch 115/1000 [11.5%] - train_loss: 0.000169 - val_loss: 0.000169 - ETA: 19s   
ğŸ“‰ LR Scheduler: Epoch 115 - Plateau detected, reducing LR 0.001563 â†’ 0.000781
Epoch 120/1000 [12.0%] - train_loss: 0.000168 - val_loss: 0.000168 - ETA: 19s   
Epoch 125/1000 [12.5%] - train_loss: 0.000168 - val_loss: 0.000168 - ETA: 19s   
ğŸ“‰ LR Scheduler: Epoch 125 - Plateau detected, reducing LR 0.000781 â†’ 0.000391
Epoch 130/1000 [13.0%] - train_loss: 0.000168 - val_loss: 0.000168 - ETA: 19s   
Epoch 135/1000 [13.5%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 19s   
ğŸ“‰ LR Scheduler: Epoch 135 - Plateau detected, reducing LR 0.000391 â†’ 0.000195
Epoch 140/1000 [14.0%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 19s   
Epoch 145/1000 [14.5%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 18s   
ğŸ“‰ LR Scheduler: Epoch 145 - Plateau detected, reducing LR 0.000195 â†’ 0.000098
Epoch 150/1000 [15.0%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 18s   
Epoch 155/1000 [15.5%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 18s   
ğŸ“‰ LR Scheduler: Epoch 155 - Plateau detected, reducing LR 0.000098 â†’ 0.000049
Epoch 160/1000 [16.0%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 18s   
Epoch 162/1000 [16.2%] - train_loss: 0.000167 - val_loss: 0.000167 - ETA: 18s   
âš ï¸ EarlyStopping: Stopped at epoch 162 (best epoch: 147, loss: 0.000167)
âœ… EntraÃ®nement terminÃ© en 3.52s

   âœ“ Training completed in 163 epochs
   âœ“ Final loss - Train: 0.000167 | Val: 0.000167

ğŸ“ˆ 6. Evaluation and metrics

   Predictions:
   [0, 0] â†’ 0.000 (expected 0) âœ“
   [0, 1] â†’ 1.000 (expected 1) âœ“
   [1, 0] â†’ 1.000 (expected 1) âœ“
   [1, 1] â†’ 0.000 (expected 0) âœ“

   Metrics:
   â€¢ Accuracy:  100.0%
   â€¢ Precision: 1.000
   â€¢ Recall:    1.000
   â€¢ F1-Score:  1.000

ğŸ”„ 7. Model comparison

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      Model          â”‚     Loss      â”‚   Accuracy    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Previous (saved)    â”‚   0.000158    â”‚    100.0%      â”‚
   â”‚ Current  (new)      â”‚   0.000167    â”‚    100.0%      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   âŒ Analysis: Previous model is still better.
      â€¢ Loss: 0.000158 (previous) vs 0.000167 (current)
      â€¢ Accuracy: 100.0% (previous) vs 100.0% (current)
   ğŸ’¾ Keeping previous model.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        SUMMARY                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ â€¢ NetworkBuilder::new(input, output)                         â•‘
â•‘     .hidden_layer(size, activation)                          â•‘
â•‘     .optimizer(OptimizerType::adam(lr))                      â•‘
â•‘     .dropout(rate).l2(lambda)                                â•‘
â•‘     .build()                                                 â•‘
â•‘                                                              â•‘
â•‘ â€¢ network.trainer()                                          â•‘
â•‘     .train_data(&dataset)                                    â•‘
â•‘     .epochs(100).batch_size(32)                              â•‘
â•‘     .callback(Box::new(...))                                 â•‘
â•‘     .fit()                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š Other examples:
   cargo run --example serialization   - Save/Load models
   cargo run --example minibatch_demo  - Mini-batch training
   cargo run --example metrics_demo    - Detailed metrics