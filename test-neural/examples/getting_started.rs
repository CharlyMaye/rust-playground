//! Getting Started - Exemple complet de la bibliothÃ¨que
//!
//! Cet exemple montre toutes les fonctionnalitÃ©s principales:
//! - Construction de rÃ©seaux avec le Builder Pattern
//! - DiffÃ©rents optimiseurs (SGD, Adam, etc.)
//! - RÃ©gularisation (Dropout, L2)
//! - Callbacks (EarlyStopping, ModelCheckpoint, LR Scheduler)
//! - Ã‰valuation avec mÃ©triques

use test_neural::builder::{NetworkBuilder, NetworkTrainer};
use test_neural::network::{Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::dataset::Dataset;
use test_neural::callbacks::{EarlyStopping, ModelCheckpoint, LearningRateScheduler, LRSchedule, ProgressBar};
use test_neural::metrics::{accuracy, binary_metrics};
use ndarray::array;

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘         Test Neural - Getting Started Guide                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 1. PRÃ‰PARATION DES DONNÃ‰ES
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ“¦ 1. PrÃ©paration des donnÃ©es (XOR problem)\n");
    
    // CrÃ©er un dataset XOR Ã©tendu pour l'entraÃ®nement
    let mut inputs = Vec::new();
    let mut targets = Vec::new();
    
    for _ in 0..100 {
        inputs.push(array![0.0, 0.0]); targets.push(array![0.0]);
        inputs.push(array![0.0, 1.0]); targets.push(array![1.0]);
        inputs.push(array![1.0, 0.0]); targets.push(array![1.0]);
        inputs.push(array![1.0, 1.0]); targets.push(array![0.0]);
    }
    
    let dataset = Dataset::new(inputs, targets);
    let (train, val) = dataset.split(0.8);
    
    println!("   Train: {} exemples | Validation: {} exemples\n", train.len(), val.len());

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 2. CONSTRUCTION D'UN RÃ‰SEAU SIMPLE
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ”§ 2. Construction d'un rÃ©seau avec le Builder Pattern\n");
    
    let network = NetworkBuilder::new(2, 1)          // 2 entrÃ©es, 1 sortie
        .hidden_layer(8, Activation::Tanh)           // Couche cachÃ©e
        .output_activation(Activation::Sigmoid)      // Sortie binaire
        .loss(LossFunction::BinaryCrossEntropy)      // Classification binaire
        .optimizer(OptimizerType::adam(0.01))        // Adam optimizer
        .build();
    
    println!("   âœ“ RÃ©seau crÃ©Ã©: 2 â†’ [8] â†’ 1");
    println!("   âœ“ Activation: Tanh â†’ Sigmoid");
    println!("   âœ“ Optimizer: Adam (lr=0.01)\n");
    drop(network);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 3. RÃ‰SEAU AVEC RÃ‰GULARISATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ›¡ï¸  3. RÃ©seau avec rÃ©gularisation (Dropout + L2)\n");
    
    let network_reg = NetworkBuilder::new(2, 1)
        .hidden_layer(16, Activation::ReLU)
        .hidden_layer(8, Activation::ReLU)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.001))
        .dropout(0.2)    // 20% des neurones dÃ©sactivÃ©s pendant training
        .l2(0.001)       // RÃ©gularisation L2 (weight decay)
        .build();
    
    println!("   âœ“ Architecture: 2 â†’ [16, 8] â†’ 1");
    println!("   âœ“ Dropout: 0.2 (prÃ©vient l'overfitting)");
    println!("   âœ“ L2: 0.001 (pÃ©nalise les grands poids)\n");
    drop(network_reg);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 4. COMPARAISON D'OPTIMISEURS
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("âš¡ 4. Comparaison rapide des optimiseurs\n");
    
    let optimizers = vec![
        ("SGD",      OptimizerType::sgd(0.5)),
        ("Momentum", OptimizerType::momentum(0.1)),
        ("Adam",     OptimizerType::adam(0.01)),
    ];
    
    let test_inputs = vec![
        array![0.0, 0.0], array![0.0, 1.0],
        array![1.0, 0.0], array![1.0, 1.0],
    ];
    let test_targets = vec![
        array![0.0], array![1.0], array![1.0], array![0.0],
    ];
    
    for (name, optimizer) in optimizers {
        let mut net = NetworkBuilder::new(2, 1)
            .hidden_layer(8, Activation::Tanh)
            .output_activation(Activation::Sigmoid)
            .loss(LossFunction::BinaryCrossEntropy)
            .optimizer(optimizer)
            .build();
        
        // EntraÃ®nement rapide
        for _ in 0..1000 {
            for (input, target) in test_inputs.iter().zip(test_targets.iter()) {
                net.train(input, target);
            }
        }
        
        let loss = net.evaluate(&test_inputs, &test_targets);
        println!("   {:<10} â†’ Loss finale: {:.6}", name, loss);
    }
    println!();

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 5. ENTRAÃNEMENT AVEC CALLBACKS
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ“Š 5. EntraÃ®nement avec callbacks\n");
    
    let mut network = NetworkBuilder::new(2, 1)
        .hidden_layer(10, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.05))
        .build();
    
    println!("   Configuration:");
    println!("   â€¢ EarlyStopping (patience=15)");
    println!("   â€¢ ModelCheckpoint (sauvegarde le meilleur)");
    println!("   â€¢ LR Scheduler (ReduceOnPlateau)\n");
    
    let history = network.trainer()
        .train_data(&train)
        .validation_data(&val)
        .epochs(100)
        .batch_size(32)
        .callback(Box::new(EarlyStopping::new(15, 0.00001)))
        .callback(Box::new(ModelCheckpoint::new("best_model.json", true)))
        .callback(Box::new(ProgressBar::new(100).set_verbose(false)))
        .scheduler(LearningRateScheduler::new(
            LRSchedule::ReduceOnPlateau { 
                patience: 10, 
                factor: 0.5, 
                min_delta: 0.0001 
            }
        ))
        .fit();
    
    println!("\n   âœ“ EntraÃ®nement terminÃ© en {} epochs", history.len());
    if let Some((train_loss, val_loss)) = history.last() {
        println!("   âœ“ Loss finale - Train: {:.6} | Val: {:.6}", 
            train_loss, val_loss.unwrap_or(0.0));
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 6. Ã‰VALUATION ET MÃ‰TRIQUES
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nğŸ“ˆ 6. Ã‰valuation et mÃ©triques\n");
    
    network.eval_mode();  // DÃ©sactive le dropout pour l'infÃ©rence
    
    let predictions: Vec<_> = test_inputs.iter()
        .map(|input| network.predict(input))
        .collect();
    
    println!("   PrÃ©dictions:");
    for (input, (pred, target)) in test_inputs.iter()
        .zip(predictions.iter().zip(test_targets.iter())) 
    {
        let correct = (pred[0].round() - target[0]).abs() < 0.1;
        println!("   [{:.0}, {:.0}] â†’ {:.3} (attendu {:.0}) {}", 
            input[0], input[1], pred[0], target[0],
            if correct { "âœ“" } else { "âœ—" });
    }
    
    let acc = accuracy(&predictions, &test_targets, 0.5);
    let metrics = binary_metrics(&predictions, &test_targets, 0.5);
    
    println!("\n   MÃ©triques:");
    println!("   â€¢ Accuracy:  {:.1}%", acc * 100.0);
    println!("   â€¢ Precision: {:.3}", metrics.precision);
    println!("   â€¢ Recall:    {:.3}", metrics.recall);
    println!("   â€¢ F1-Score:  {:.3}", metrics.f1_score);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // RÃ‰SUMÃ‰
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                        RÃ‰SUMÃ‰                                â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘ â€¢ NetworkBuilder::new(input, output)                         â•‘");
    println!("â•‘     .hidden_layer(size, activation)                          â•‘");
    println!("â•‘     .optimizer(OptimizerType::adam(lr))                      â•‘");
    println!("â•‘     .dropout(rate).l2(lambda)                                â•‘");
    println!("â•‘     .build()                                                 â•‘");
    println!("â•‘                                                              â•‘");
    println!("â•‘ â€¢ network.trainer()                                          â•‘");
    println!("â•‘     .train_data(&dataset)                                    â•‘");
    println!("â•‘     .epochs(100).batch_size(32)                              â•‘");
    println!("â•‘     .callback(Box::new(...))                                 â•‘");
    println!("â•‘     .fit()                                                   â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    println!("ğŸ“š Autres exemples:");
    println!("   cargo run --example serialization   - Save/Load modÃ¨les");
    println!("   cargo run --example minibatch_demo  - Mini-batch training");
    println!("   cargo run --example metrics_demo    - MÃ©triques dÃ©taillÃ©es\n");
    
    // Cleanup
    std::fs::remove_file("best_model.json").ok();
}
