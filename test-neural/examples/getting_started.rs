//! Getting Started - Complete library example
//!
//! This example demonstrates all main features:
//! - Building networks with the Builder Pattern
//! - Different optimizers (SGD, Adam, etc.)
//! - Regularization (Dropout, L2)
//! - Callbacks (EarlyStopping, ModelCheckpoint, LR Scheduler)
//! - Evaluation with metrics

use cma_neural_network::builder::{NetworkBuilder, NetworkTrainer};
use cma_neural_network::network::{Activation, LossFunction};
use cma_neural_network::optimizer::OptimizerType;
use cma_neural_network::dataset::Dataset;
use cma_neural_network::callbacks::{EarlyStopping, DeltaMode, LearningRateScheduler, LRSchedule, ProgressBar};
use cma_neural_network::metrics::{accuracy, binary_metrics};
use cma_neural_network::io;
use ndarray::array;
use std::fs;
use std::path::Path;

fn main() {
    // Create data directory for output files
    let data_dir = "examples/data";
    fs::create_dir_all(data_dir).expect("Failed to create data directory");
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘         Test Neural - Getting Started Guide                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 1. DATA PREPARATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ“¦ 1. Data preparation (XOR problem)\n");
    
    // Create an extended XOR dataset for training
    let mut inputs = Vec::new();
    let mut targets = Vec::new();
    
    for _ in 0..100 {
        inputs.push(array![0.0, 0.0]); targets.push(array![0.0]);
        inputs.push(array![0.0, 1.0]); targets.push(array![1.0]);
        inputs.push(array![1.0, 0.0]); targets.push(array![1.0]);
        inputs.push(array![1.0, 1.0]); targets.push(array![0.0]);
    }
    
    let dataset = Dataset::new(inputs, targets);
    let (train, val) = dataset.split(0.8);
    
    println!("   Train: {} samples | Validation: {} samples\n", train.len(), val.len());

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 2. BUILDING A SIMPLE NETWORK
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ”§ 2. Building a network with the Builder Pattern\n");
    
    let network = NetworkBuilder::new(2, 1)          // 2 inputs, 1 output
        .hidden_layer(8, Activation::Tanh)           // Hidden layer
        .output_activation(Activation::Sigmoid)      // Binary output
        .loss(LossFunction::BinaryCrossEntropy)      // Binary classification
        .optimizer(OptimizerType::adam(0.01))        // Adam optimizer
        .build();

    println!("   âœ“ Network created: 2 â†’ [8] â†’ 1");
    println!("   âœ“ Activation: Tanh â†’ Sigmoid");
    println!("   âœ“ Optimizer: Adam (lr=0.01)\n");
    drop(network);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 3. NETWORK WITH REGULARIZATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ›¡ï¸  3. Network with regularization (Dropout + L2)\n");

    let network_reg = NetworkBuilder::new(2, 1)
        .hidden_layer(16, Activation::ReLU)
        .hidden_layer(8, Activation::ReLU)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.001))
        .dropout(0.2)    // 20% of neurons disabled during training
        .l2(0.001)       // L2 regularization (weight decay)
        .build();

    println!("   âœ“ Architecture: 2 â†’ [16, 8] â†’ 1");
    println!("   âœ“ Dropout: 0.2 (prevents overfitting)");
    println!("   âœ“ L2: 0.001 (penalizes large weights)\n");
    drop(network_reg);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 4. OPTIMIZER COMPARISON
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("âš¡ 4. Quick optimizer comparison\n");
    
    let optimizers = vec![
        ("SGD",      OptimizerType::sgd(0.5)),
        ("Momentum", OptimizerType::momentum(0.1)),
        ("Adam",     OptimizerType::adam(0.01)),
    ];
    
    let test_inputs = vec![
        array![0.0, 0.0], array![0.0, 1.0],
        array![1.0, 0.0], array![1.0, 1.0],
    ];
    let test_targets = vec![
        array![0.0], array![1.0], array![1.0], array![0.0],
    ];
    
    for (name, optimizer) in optimizers {
        let mut net = NetworkBuilder::new(2, 1)
            .hidden_layer(8, Activation::Tanh)
            .output_activation(Activation::Sigmoid)
            .loss(LossFunction::BinaryCrossEntropy)
            .optimizer(optimizer)
            .build();
        
        net.set_seed(42); // Reproducible results
        
        // Quick training
        for _ in 0..1000 {
            for (input, target) in test_inputs.iter().zip(test_targets.iter()) {
                net.train(input, target);
            }
        }

        let loss = net.evaluate(&test_inputs, &test_targets);
        println!("   {:<10} â†’ Final loss: {:.6}", name, loss);
    }
    println!();

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 5. TRAINING WITH CALLBACKS
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ“Š 5. Training with callbacks\n");

    let mut network = NetworkBuilder::new(2, 1)
        .hidden_layer(10, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.05))
        .build();

    network.set_seed(42); // Reproducible training

    println!("   Configuration:");
    println!("   â€¢ EarlyStopping (patience=15, 0.1% relative improvement)");
    println!("   â€¢ LR Scheduler (ReduceOnPlateau)\n");

    let epoch = 1_000;
    let history = network.trainer()
        .train_data(&train)
        .validation_data(&val)
        .epochs(epoch)
        .batch_size(32)
        .callback(Box::new(EarlyStopping::new(15, 0.001).mode(DeltaMode::Relative)))  // 0.1% improvement
        .callback(Box::new(ProgressBar::new(epoch)))
        .scheduler(LearningRateScheduler::new(
            LRSchedule::ReduceOnPlateau {
                patience: 10,
                factor: 0.5,
                min_delta: 0.0001
            }
        ))
        .fit();
    
    println!("\n   âœ“ Training completed in {} epochs", history.len());
    if let Some((train_loss, val_loss)) = history.last() {
        println!("   âœ“ Final loss - Train: {:.6} | Val: {:.6}",
            train_loss, val_loss.unwrap_or(0.0));
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 6. EVALUATION AND METRICS
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nğŸ“ˆ 6. Evaluation and metrics\n");
    
    // Note: predict() automatically uses eval mode (no dropout)
    
    let predictions: Vec<_> = test_inputs.iter()
        .map(|input| network.predict(input))
        .collect();
    
    println!("   Predictions:");
    for (input, (pred, target)) in test_inputs.iter()
        .zip(predictions.iter().zip(test_targets.iter()))
    {
        let correct = (pred[0].round() - target[0]).abs() < 0.1;
        println!("   [{:.0}, {:.0}] â†’ {:.3} (expected {:.0}) {}",
            input[0], input[1], pred[0], target[0],
            if correct { "âœ“" } else { "âœ—" });
    }
    
    let acc = accuracy(&predictions, &test_targets, 0.5);
    let metrics = binary_metrics(&predictions, &test_targets, 0.5);
    
    println!("\n   Metrics:");
    println!("   â€¢ Accuracy:  {:.1}%", acc * 100.0);
    println!("   â€¢ Precision: {:.3}", metrics.precision);
    println!("   â€¢ Recall:    {:.3}", metrics.recall);
    println!("   â€¢ F1-Score:  {:.3}", metrics.f1_score);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 7. MODEL COMPARISON AND SAVE
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nğŸ”„ 7. Model comparison\n");

    let model_path = format!("{}/best_model.json", data_dir);
    let current_loss = network.evaluate(&test_inputs, &test_targets);
    let current_acc = acc;

    // Check if a previous model exists
    if Path::new(&model_path).exists() {
        match io::load_json(&model_path) {
            Ok(previous_model) => {
                let previous_loss = previous_model.evaluate(&test_inputs, &test_targets);
                let previous_preds: Vec<_> = test_inputs.iter()
                    .map(|input| previous_model.predict(input))
                    .collect();
                let previous_acc = accuracy(&previous_preds, &test_targets, 0.5);

                println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
                println!("   â”‚      Model          â”‚     Loss      â”‚   Accuracy    â”‚");
                println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
                println!("   â”‚ Previous (saved)    â”‚   {:.6}    â”‚    {:.1}%      â”‚", previous_loss, previous_acc * 100.0);
                println!("   â”‚ Current  (new)      â”‚   {:.6}    â”‚    {:.1}%      â”‚", current_loss, current_acc * 100.0);
                println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

                // Compare and decide
                let loss_improved = current_loss < previous_loss;
                let acc_improved = current_acc > previous_acc;

                println!();
                if loss_improved && acc_improved {
                    println!("   âœ… Analysis: New model is BETTER on all metrics!");
                    println!("      â€¢ Loss improved by {:.2}%", (1.0 - current_loss / previous_loss) * 100.0);
                    println!("      â€¢ Accuracy improved by {:.1} points", (current_acc - previous_acc) * 100.0);
                    println!("   ğŸ’¾ Saving new model...");
                    io::save_json(&network, &model_path).expect("Failed to save model");
                    println!("   âœ“ Model saved to {}", model_path);
                } else if loss_improved {
                    println!("   ğŸŸ¡ Analysis: New model has LOWER loss but same/lower accuracy.");
                    println!("      â€¢ Loss: {:.6} â†’ {:.6} (â†“ {:.2}%)", previous_loss, current_loss, (1.0 - current_loss / previous_loss) * 100.0);
                    println!("      â€¢ Accuracy: {:.1}% â†’ {:.1}%", previous_acc * 100.0, current_acc * 100.0);
                    println!("   ğŸ’¾ Saving new model (lower loss is preferred)...");
                    io::save_json(&network, &model_path).expect("Failed to save model");
                    println!("   âœ“ Model saved to {}", model_path);
                } else if acc_improved {
                    println!("   ğŸŸ¡ Analysis: New model has BETTER accuracy but higher loss.");
                    println!("      â€¢ Loss: {:.6} â†’ {:.6}", previous_loss, current_loss);
                    println!("      â€¢ Accuracy: {:.1}% â†’ {:.1}% (â†‘ {:.1} points)", previous_acc * 100.0, current_acc * 100.0, (current_acc - previous_acc) * 100.0);
                    println!("   ğŸ’¾ Saving new model (better accuracy)...");
                    io::save_json(&network, &model_path).expect("Failed to save model");
                    println!("   âœ“ Model saved to {}", model_path);
                } else {
                    println!("   âŒ Analysis: Previous model is still better.");
                    println!("      â€¢ Loss: {:.6} (previous) vs {:.6} (current)", previous_loss, current_loss);
                    println!("      â€¢ Accuracy: {:.1}% (previous) vs {:.1}% (current)", previous_acc * 100.0, current_acc * 100.0);
                    println!("   ğŸ’¾ Keeping previous model.");
                }
            }
            Err(e) => {
                println!("   âš ï¸  Could not load previous model: {}", e);
                println!("   ğŸ’¾ Saving current model as new baseline...");
                io::save_json(&network, &model_path).expect("Failed to save model");
                println!("   âœ“ Model saved to {}", model_path);
            }
        }
    } else {
        println!("   â„¹ï¸  No previous model found.");
        println!("   Current model: Loss = {:.6} | Accuracy = {:.1}%", current_loss, current_acc * 100.0);
        println!("   ğŸ’¾ Saving as first baseline...");
        io::save_json(&network, &model_path).expect("Failed to save model");
        println!("   âœ“ Model saved to {}", model_path);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // SUMMARY
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                        SUMMARY                               â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘ â€¢ NetworkBuilder::new(input, output)                         â•‘");
    println!("â•‘     .hidden_layer(size, activation)                          â•‘");
    println!("â•‘     .optimizer(OptimizerType::adam(lr))                      â•‘");
    println!("â•‘     .dropout(rate).l2(lambda)                                â•‘");
    println!("â•‘     .build()                                                 â•‘");
    println!("â•‘                                                              â•‘");
    println!("â•‘ â€¢ network.trainer()                                          â•‘");
    println!("â•‘     .train_data(&dataset)                                    â•‘");
    println!("â•‘     .epochs(100).batch_size(32)                              â•‘");
    println!("â•‘     .callback(Box::new(...))                                 â•‘");
    println!("â•‘     .fit()                                                   â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    println!("ğŸ“š Other examples:");
    println!("   cargo run --example serialization   - Save/Load models");
    println!("   cargo run --example minibatch_demo  - Mini-batch training");
    println!("   cargo run --example metrics_demo    - Detailed metrics\n");
}
