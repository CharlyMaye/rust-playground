/// D√©mo de la r√©gularisation : Dropout, L1 et L2
/// 
/// Montre comment la r√©gularisation aide √† pr√©venir l'overfitting
/// en p√©nalisant les mod√®les trop complexes.

use test_neural::builder::NetworkBuilder;
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use ndarray::array;

fn main() {
    println!("=== D√©monstration de la R√©gularisation ===\n");

    // Dataset XOR (simple)
    let inputs = vec![
        array![0.0, 0.0],
        array![0.0, 1.0],
        array![1.0, 0.0],
        array![1.0, 1.0],
    ];
    
    let targets = vec![
        array![0.0],
        array![1.0],
        array![1.0],
        array![0.0],
    ];

    let epochs = 5000;
    let hidden_size = 20;  // R√©seau surdimensionn√© pour montrer l'overfitting

    println!("Configuration:");
    println!("  ‚Ä¢ Architecture: 2 ‚Üí [{}] ‚Üí 1 (r√©seau surdimensionn√©)", hidden_size);
    println!("  ‚Ä¢ Dataset: XOR (4 exemples seulement)");
    println!("  ‚Ä¢ Epochs: {}\n", epochs);

    // Test 1: Sans r√©gularisation
    println!("--- 1. Sans R√©gularisation (Baseline) ---");
    let mut network_baseline = NetworkBuilder::new(2, 1)
        .hidden_layer(hidden_size, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .build();

    train_and_evaluate(&mut network_baseline, "Baseline", &inputs, &targets, epochs);

    // Test 2: Avec Dropout
    println!("\n--- 2. Avec Dropout (rate=0.3) ---");
    let mut network_dropout = NetworkBuilder::new(2, 1)
        .hidden_layer(hidden_size, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .dropout(0.3)  // 30% des neurones d√©sactiv√©s
        .build();

    train_and_evaluate(&mut network_dropout, "Dropout", &inputs, &targets, epochs);

    // Test 3: Avec L2 (Weight Decay)
    println!("\n--- 3. Avec L2 Regularization (lambda=0.01) ---");
    let mut network_l2 = NetworkBuilder::new(2, 1)
        .hidden_layer(hidden_size, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .l2(0.01)
        .build();

    train_and_evaluate(&mut network_l2, "L2", &inputs, &targets, epochs);

    // Test 4: Avec L1 (Sparsity)
    println!("\n--- 4. Avec L1 Regularization (lambda=0.01) ---");
    let mut network_l1 = NetworkBuilder::new(2, 1)
        .hidden_layer(hidden_size, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .l1(0.01)
        .build();

    train_and_evaluate(&mut network_l1, "L1", &inputs, &targets, epochs);

    // Test 5: Combin√© (Dropout + L2)
    println!("\n--- 5. Dropout + L2 (Combin√©) ---");
    let mut network_combined = NetworkBuilder::new(2, 1)
        .hidden_layer(hidden_size, Activation::Tanh)
        .output_activation(Activation::Sigmoid)
        .loss(LossFunction::BinaryCrossEntropy)
        .optimizer(OptimizerType::adam(0.01))
        .dropout(0.2)
        .l2(0.005)
        .build();

    train_and_evaluate(&mut network_combined, "Combined", &inputs, &targets, epochs);

    // R√©sum√©
    println!("\n=== R√©sum√© ===");
    println!("üéØ R√©gularisation : Techniques pour r√©duire l'overfitting\n");
    println!("üìä Observations :");
    println!("  ‚Ä¢ Sans r√©gularisation : Peut sur-apprendre (overfitting)");
    println!("  ‚Ä¢ Dropout : Force le r√©seau √† √™tre robuste");
    println!("  ‚Ä¢ L2 : P√©nalise les grands poids, mod√®le plus lisse");
    println!("  ‚Ä¢ L1 : Encourage la sparsit√© (poids √† z√©ro)");
    println!("  ‚Ä¢ Combin√© : Souvent la meilleure approche\n");
    println!("üí° Recommandations :");
    println!("  ‚Ä¢ Dataset petit ‚Üí Dropout (0.2-0.5) + L2 (0.001-0.01)");
    println!("  ‚Ä¢ Dataset grand ‚Üí L2 seul ou Dropout l√©ger");
    println!("  ‚Ä¢ Besoin de sparsit√© ‚Üí L1");
}

fn train_and_evaluate(
    network: &mut Network,
    name: &str,
    inputs: &Vec<ndarray::Array1<f64>>,
    targets: &Vec<ndarray::Array1<f64>>,
    epochs: usize,
) {
    // Training
    network.train_mode();
    for epoch in 0..epochs {
        for (input, target) in inputs.iter().zip(targets.iter()) {
            network.train(input, target);
        }

        if epoch % 1000 == 0 || epoch == epochs - 1 {
            let loss = network.evaluate(inputs, targets);
            println!("  Epoch {:4}: loss = {:.6}", epoch, loss);
        }
    }

    // Evaluation (switch to eval mode to disable dropout)
    network.eval_mode();
    let final_loss = network.evaluate(inputs, targets);
    
    println!("\n  Pr√©dictions finales ({}):", name);
    let mut all_correct = true;
    for (input, target) in inputs.iter().zip(targets.iter()) {
        let prediction = network.predict(input);
        let pred_value = prediction[0];
        let target_value = target[0];
        let correct = (pred_value.round() - target_value).abs() < 0.01;
        all_correct = all_correct && correct;
        println!("    [{:.1}, {:.1}] ‚Üí {:.4} (target: {:.1}) {}",
            input[0], input[1], pred_value, target_value,
            if correct { "‚úì" } else { "‚úó" }
        );
    }
    
    println!("  Loss finale (eval): {:.6}", final_loss);
    println!("  R√©sultat: {}", if all_correct { "‚úì PASSED" } else { "‚úó FAILED" });
}
