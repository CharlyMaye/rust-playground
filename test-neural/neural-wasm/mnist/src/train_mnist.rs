//! MNIST Model Training Script
//!
//! This binary trains a neural network on the MNIST classification problem
//! and saves it to neural-wasm/mnist/src/mnist_model.json

use cma_neural_network::builder::{NetworkBuilder, NetworkTrainer};
use cma_neural_network::callbacks::{DeltaMode, EarlyStopping, ProgressBar};
use cma_neural_network::dataset::Dataset;
use cma_neural_network::network::{Activation, LossFunction};
use cma_neural_network::optimizer::OptimizerType;
use csv::ReaderBuilder;
use ndarray::Array1;
use neural_wasm_shared::{calculate_multiclass_accuracy, save_model_binary, NormalizationStats};
use std::error::Error;

fn main() -> Result<(), Box<dyn Error>> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘         MNIST Classification Neural Network Training          â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let model_path = "src/mnist_model.bin";

    // Check if model already exists
    if std::path::Path::new(model_path).exists() {
        println!("âš ï¸  Model already exists at {}", model_path);
        println!("   Delete it manually if you want to retrain.\n");
        return Ok(());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 1. DATA PREPARATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ“¦ Preparing MNIST dataset...\n");

    let mnist_data = load_mnist_from_csv("data/mnist.csv")?;
    println!("   âœ… Loaded {} samples from CSV", mnist_data.len());

    let inputs: Vec<Array1<f64>> = mnist_data.iter().map(|(i, _)| i.clone()).collect();
    let targets: Vec<Array1<f64>> = mnist_data.iter().map(|(_, t)| t.clone()).collect();

    // Normalize inputs (z-score normalization per feature)
    let (inputs, norm_stats) = normalize_features_with_stats(&inputs);
    println!("   âœ… Features normalized (z-score)");
    println!(
        "   ğŸ“Š Stats: {} features normalized",
        norm_stats.means.len()
    );

    let mut dataset = Dataset::new(inputs, targets);

    // CRITICAL: Shuffle before split! The CSV is sorted by class (setosa, versicolor, virginica)
    // Without shuffling, the test set would contain only virginica samples!
    dataset.shuffle();
    println!("   âœ… Dataset shuffled");

    let (train, val) = dataset.split(0.7);

    println!("   Training samples: {} (70%)", train.len());
    println!("   Test samples: {} (30%)\n", val.len());

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 2. BUILD NETWORK
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ”§ Building network...\n");

    let mut network = NetworkBuilder::new(784, 10)
        .hidden_layer(128, Activation::ReLU)
        .hidden_layer(64, Activation::ReLU)
        .output_activation(Activation::Softmax)
        .loss(LossFunction::CategoricalCrossEntropy)
        .optimizer(OptimizerType::adam(0.001)) // RÃ©duit de 0.01 Ã  0.001
        .build();

    println!("   Architecture: 784 â†’ [128, 64] â†’ 10");
    println!("   Activation: ReLU â†’ ReLU â†’ Softmax");
    println!("   Optimizer: Adam (lr=0.01)\n");

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 3. TRAIN
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("ğŸ‹ï¸  Training...\n");

    let epochs = 2_000;
    let history = network
        .trainer()
        .train_data(&train)
        .validation_data(&val)
        .epochs(epochs)
        .batch_size(64)
        .callback(Box::new(
            EarlyStopping::new(100, 0.00001).mode(DeltaMode::Relative),
        ))
        .callback(Box::new(ProgressBar::new(epochs)))
        .fit();

    println!("\n   âœ… Training completed in {} epochs", history.len());

    if let Some((train_loss, val_loss)) = history.last() {
        println!(
            "   Final loss - Train: {:.6} | Val: {:.6}",
            train_loss,
            val_loss.unwrap_or(0.0)
        );
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 4. EVALUATE
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nğŸ“Š Evaluating...\n");

    network.eval_mode();

    let test_inputs = val.inputs();
    let test_targets = val.targets();

    let (correct, total) = calculate_multiclass_accuracy(&network, test_inputs, test_targets);
    let acc = correct as f64 / total as f64;

    println!("   MNIST Classification Results:");
    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!(
        "   â”‚  Correct: {}/{} ({:.2}%)        â”‚",
        correct,
        total,
        acc * 100.0
    );
    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

    println!("\n   Test Accuracy: {:.2}%", acc * 100.0);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 5. SAVE MODEL WITH METADATA
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\nğŸ’¾ Saving model...\n");

    match save_model_binary(network, acc, total, Some(norm_stats), model_path) {
        Ok(_) => {
            println!("   âœ… Model saved to {}", model_path);
            println!("   ğŸ“Š Accuracy: {:.2}%", acc * 100.0);
            println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            println!("â•‘              Training Complete! ğŸ‰                           â•‘");
            println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        }
        Err(e) => {
            eprintln!("   âŒ Failed to save model: {}", e);
            std::process::exit(1);
        }
    }

    Ok(())
}

/// Load the MNIST dataset from CSV file (OpenML format)
/// Format: 784 pixel values (0-255), then label (0-9) as last column
/// Dataset source: https://www.openml.org/d/554
fn load_mnist_from_csv(path: &str) -> Result<Vec<(Array1<f64>, Array1<f64>)>, Box<dyn Error>> {
    let mut data = Vec::new();
    let mut rdr = ReaderBuilder::new().has_headers(false).from_path(path)?;

    for result in rdr.records() {
        let record = result?;

        if record.len() != 785 {
            return Err(format!(
                "Expected 785 columns (784 pixels + 1 label), got {}",
                record.len()
            )
            .into());
        }

        // Parse the 784 pixel values (columns 0-783)
        let mut pixels = Vec::with_capacity(784);
        for i in 0..784 {
            let pixel: f64 = record[i].parse()?;
            pixels.push(pixel);
        }

        // Parse label (last column, index 784) and convert to one-hot encoding (10 classes)
        let label: usize = record[784].parse()?;
        if label > 9 {
            return Err(format!("Invalid label: {} (expected 0-9)", label).into());
        }

        let mut one_hot = vec![0.0; 10];
        one_hot[label] = 1.0;

        data.push((Array1::from_vec(pixels), Array1::from_vec(one_hot)));
    }

    Ok(data)
}

/// Normalize features using z-score normalization (mean=0, std=1)
/// Returns normalized data AND the normalization statistics for inference
fn normalize_features_with_stats(inputs: &[Array1<f64>]) -> (Vec<Array1<f64>>, NormalizationStats) {
    if inputs.is_empty() {
        return (vec![], NormalizationStats::new(vec![], vec![]));
    }

    let n_features = inputs[0].len();
    let n_samples = inputs.len() as f64;

    // Calculate mean for each feature
    let mut means = vec![0.0; n_features];
    for input in inputs {
        for (i, &val) in input.iter().enumerate() {
            means[i] += val;
        }
    }
    for mean in &mut means {
        *mean /= n_samples;
    }

    // Calculate standard deviation for each feature
    let mut stds = vec![0.0; n_features];
    for input in inputs {
        for (i, &val) in input.iter().enumerate() {
            stds[i] += (val - means[i]).powi(2);
        }
    }
    for std in &mut stds {
        *std = (*std / n_samples).sqrt();
        // Prevent division by zero
        if *std < 1e-8 {
            *std = 1.0;
        }
    }

    // Normalize each input
    let normalized = inputs
        .iter()
        .map(|input| {
            Array1::from_vec(
                input
                    .iter()
                    .enumerate()
                    .map(|(i, &val)| (val - means[i]) / stds[i])
                    .collect(),
            )
        })
        .collect();

    (normalized, NormalizationStats::new(means, stds))
}
