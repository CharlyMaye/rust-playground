# R√©seau de Neurones en Rust

## Concepts Cl√©s

### 1. Architecture (Couches/Neurones)

La **structure** de ton r√©seau : nombre de couches et nombre de neurones par couche.

- Dans ton code : `Network::new(2, 3, 1)` = 2 entr√©es ‚Üí 3 neurones cach√©s ‚Üí 1 sortie
- Plus de neurones/couches = plus de capacit√© d'apprentissage, mais risque de **surapprentissage**

### 2. Fonctions d'Activation (sigmoid, ReLU, tanh...)

Fonction qui **transforme** la sortie d'un neurone.

**Actuellement utilis√©e : Sigmoid**
- Formule : `1 / (1 + e^-x)`
- Sortie : entre `[0, 1]`

**Alternatives :**
- **ReLU** : `max(0, x)` ‚Üí plus rapide, standard moderne
- **tanh** : `tanh(x)` ‚Üí sortie entre `[-1, 1]`
- **Leaky ReLU**, **ELU**, etc.

---

## Fonctions d'Activation D√©taill√©es

### Sigmoid (Logistic)
**Formule :** $f(x) = \frac{1}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) \cdot (1 - f(x))$

**Propri√©t√©s :**
- Sortie : `[0, 1]`
- Lisse et diff√©rentiable partout
- Interpr√©table comme une probabilit√©

**Avantages :**
- ‚úÖ Sortie normalis√©e (bonne pour la couche de sortie en classification binaire)
- ‚úÖ Gradient bien d√©fini

**Inconv√©nients :**
- ‚ùå **Probl√®me du gradient qui dispara√Æt** (vanishing gradient) pour grandes/petites valeurs
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Co√ªteux en calcul (`exp()`)

**Impl√©mentation Rust :**
```rust
fn sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp()))
}

fn sigmoid_derivative(x: &Array1<f64>) -> Array1<f64> {
    x * &(1.0 - x)
}
```

---

### ReLU (Rectified Linear Unit)
**Formule :** $f(x) = \max(0, x)$

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `[0, +‚àû)`
- Lin√©aire pour valeurs positives, z√©ro sinon
- **Standard moderne pour les couches cach√©es**

**Avantages :**
- ‚úÖ **Tr√®s rapide** (simple comparaison et multiplication)
- ‚úÖ Pas de gradient qui dispara√Æt pour valeurs positives
- ‚úÖ Favorise la sparsit√© (certains neurones s'√©teignent)
- ‚úÖ Convergence plus rapide que sigmoid/tanh

**Inconv√©nients :**
- ‚ùå **Probl√®me des neurones morts** : si gradient = 0, le neurone ne s'active plus jamais
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Non diff√©rentiable en x = 0 (en pratique, on prend 0 ou 1)

**Impl√©mentation Rust :**
```rust
fn relu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(0.0))
}

fn relu_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 })
}
```

---

### Leaky ReLU
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 0.01$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-‚àû, +‚àû)`
- Petite pente pour valeurs n√©gatives

**Avantages :**
- ‚úÖ R√©sout le probl√®me des neurones morts de ReLU
- ‚úÖ Rapide comme ReLU
- ‚úÖ Garde un gradient pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå R√©sultats incoh√©rents selon les t√¢ches
- ‚ùå N√©cessite un hyperparam√®tre (alpha)

**Impl√©mentation Rust :**
```rust
fn leaky_relu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * x })
}

fn leaky_relu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha })
}
```

---

### Tanh (Tangente Hyperbolique)
**Formule :** $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**D√©riv√©e :** $f'(x) = 1 - f(x)^2$

**Propri√©t√©s :**
- Sortie : `[-1, 1]`
- **Centr√©e sur z√©ro** (contrairement √† sigmoid)
- Version "√©tendue" de sigmoid

**Avantages :**
- ‚úÖ Sortie centr√©e ‚Üí convergence plus rapide que sigmoid
- ‚úÖ Gradient plus fort que sigmoid
- ‚úÖ Bon pour les couches cach√©es

**Inconv√©nients :**
- ‚ùå Probl√®me du gradient qui dispara√Æt (moins que sigmoid)
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.tanh())
}

fn tanh_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 - x.powi(2))
}
```

---

### ELU (Exponential Linear Unit)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 1.0$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ f(x) + \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-Œ±, +‚àû)`
- Lisse partout

**Avantages :**
- ‚úÖ Moyenne des activations proche de z√©ro
- ‚úÖ Pas de neurones morts
- ‚úÖ Gradient non-nul partout

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp()`)
- ‚ùå L√©g√®rement plus lent que ReLU

**Impl√©mentation Rust :**
```rust
fn elu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * (x.exp() - 1.0) })
}

fn elu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha * x.exp() })
}
```

---

### Softmax (pour classification multi-classes)
**Formule :** $f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$

**Propri√©t√©s :**
- Sortie : `[0, 1]` pour chaque neurone, somme = 1
- Convertit logits en probabilit√©s
- **Uniquement pour la couche de sortie**

**Avantages :**
- ‚úÖ Interpr√©tation probabiliste claire
- ‚úÖ Standard pour classification multi-classes

**Impl√©mentation Rust :**
```rust
fn softmax(x: &Array1<f64>) -> Array1<f64> {
    let max = x.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    let exp_x = x.mapv(|x| (x - max).exp());
    let sum = exp_x.sum();
    exp_x / sum
}
```

---

## Fonctions d'Activation Avanc√©es

### PReLU (Parametric ReLU)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ o√π $\alpha$ est **appris** pendant l'entra√Ænement

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Avantages :**
- ‚úÖ Alpha adaptatif par neurone
- ‚úÖ Plus flexible que Leaky ReLU

**Inconv√©nients :**
- ‚ùå Plus de param√®tres √† entra√Æner
- ‚ùå Risque de surapprentissage

**Impl√©mentation Rust :**
```rust
fn prelu(x: &Array1<f64>, alpha: &Array1<f64>) -> Array1<f64> {
    x.iter().zip(alpha.iter())
        .map(|(&x, &a)| if x > 0.0 { x } else { a * x })
        .collect()
}
```

---

### GELU (Gaussian Error Linear Unit)
**Formule :** $f(x) = x \cdot \Phi(x)$ o√π $\Phi$ est la fonction de distribution cumulative gaussienne

**Approximation :** $f(x) \approx 0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$

**Propri√©t√©s :**
- Lisse et non-monotone
- **Utilis√© dans BERT, GPT**

**Avantages :**
- ‚úÖ Performance SOTA sur transformers
- ‚úÖ Lisse partout
- ‚úÖ Probabilistiquement motiv√©

**Inconv√©nients :**
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn gelu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| {
        0.5 * x * (1.0 + ((2.0 / std::f64::consts::PI).sqrt() 
            * (x + 0.044715 * x.powi(3))).tanh())
    })
}
```

---

### Swish / SiLU (Sigmoid Linear Unit)
**Formule :** $f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) + \sigma(x)(1 - f(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **D√©couvert par Google via recherche automatique**

**Avantages :**
- ‚úÖ Meilleure performance que ReLU sur certaines t√¢ches
- ‚úÖ Lisse partout

**Inconv√©nients :**
- ‚ùå Plus co√ªteux que ReLU

**Impl√©mentation Rust :**
```rust
fn swish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + (-x).exp()))
}

fn swish_derivative(x: &Array1<f64>) -> Array1<f64> {
    let sigmoid = x.mapv(|x| 1.0 / (1.0 + (-x).exp()));
    let swish = x * &sigmoid;
    &swish + &sigmoid * &(1.0 - &swish)
}
```

---

### Mish
**Formule :** $f(x) = x \cdot \tanh(\ln(1 + e^x)) = x \cdot \tanh(\text{softplus}(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **Alternatives r√©cente √† Swish**

**Avantages :**
- ‚úÖ Meilleure r√©gularisation que ReLU/Swish
- ‚úÖ Gradient non-nul pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå Tr√®s co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn mish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x * ((1.0 + x.exp()).ln()).tanh())
}
```

---

### SELU (Scaled ELU)
**Formule :** $f(x) = \lambda \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$

**Constantes :** $\lambda \approx 1.0507$, $\alpha \approx 1.6733$

**Propri√©t√©s :**
- Auto-normalisant (pr√©serve moyenne=0, variance=1)
- **Con√ßu pour FeedForward Networks**

**Avantages :**
- ‚úÖ Pas besoin de Batch Normalization
- ‚úÖ Convergence plus rapide

**Inconv√©nients :**
- ‚ùå Sensible √† l'initialisation (utiliser LeCun)
- ‚ùå Fonctionne mal avec Dropout

**Impl√©mentation Rust :**
```rust
fn selu(x: &Array1<f64>) -> Array1<f64> {
    let lambda = 1.0507;
    let alpha = 1.6733;
    x.mapv(|x| {
        lambda * if x > 0.0 { x } else { alpha * (x.exp() - 1.0) }
    })
}
```

---

### Softplus
**Formule :** $f(x) = \ln(1 + e^x)$

**D√©riv√©e :** $f'(x) = \frac{1}{1 + e^{-x}} = \sigma(x)$ (sigmoid!)

**Propri√©t√©s :**
- Version lisse de ReLU
- Toujours positif

**Avantages :**
- ‚úÖ Diff√©rentiable partout
- ‚úÖ Pas de neurones morts

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp`, `log`)
- ‚ùå Gradient qui dispara√Æt pour grandes valeurs n√©gatives

**Impl√©mentation Rust :**
```rust
fn softplus(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (1.0 + x.exp()).ln())
}

fn softplus_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp())) // sigmoid
}
```

---

### Softsign
**Formule :** $f(x) = \frac{x}{1 + |x|}$

**D√©riv√©e :** $f'(x) = \frac{1}{(1 + |x|)^2}$

**Propri√©t√©s :**
- Sortie : `(-1, 1)`
- Alternative √† tanh

**Avantages :**
- ‚úÖ Plus rapide que tanh (pas d'exponentielle)
- ‚úÖ Gradient d√©cro√Æt plus lentement

**Inconv√©nients :**
- ‚ùå Rarement utilis√© en pratique

**Impl√©mentation Rust :**
```rust
fn softsign(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + x.abs()))
}

fn softsign_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + x.abs()).powi(2))
}
```

---

### Hard Sigmoid
**Formule :** $f(x) = \max(0, \min(1, 0.2x + 0.5))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de sigmoid
- Tr√®s rapide

**Avantages :**
- ‚úÖ Calcul extr√™mement rapide (pas d'exponentielle)
- ‚úÖ Utile pour les appareils embarqu√©s

**Impl√©mentation Rust :**
```rust
fn hard_sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (0.2 * x + 0.5).max(0.0).min(1.0))
}
```

---

### Hard Tanh
**Formule :** $f(x) = \max(-1, \min(1, x))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de tanh
- Sortie : `[-1, 1]`

**Avantages :**
- ‚úÖ Tr√®s rapide

**Impl√©mentation Rust :**
```rust
fn hard_tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(-1.0).min(1.0))
}
```

---

## Tableau Comparatif Complet

| **Fonction** | **Plage** | **Vitesse** | **Usage Principal** | **Depuis** |
|--------------|-----------|-------------|---------------------|------------|
| Sigmoid | [0, 1] | Lent | Sortie binaire | Classique |
| Tanh | [-1, 1] | Lent | Couches cach√©es | Classique |
| ReLU | [0, ‚àû) | **Tr√®s rapide** | Couches cach√©es (d√©faut) | 2010 |
| Leaky ReLU | (-‚àû, ‚àû) | **Tr√®s rapide** | Fix neurones morts | 2013 |
| PReLU | (-‚àû, ‚àû) | Rapide | Am√©lioration LeakyReLU | 2015 |
| ELU | (-Œ±, ‚àû) | Moyen | R√©seaux profonds | 2015 |
| SELU | (-ŒªŒ±, ‚àû) | Moyen | FeedForward (sans BN) | 2017 |
| Swish/SiLU | (-‚àû, ‚àû) | Moyen | Alternative ReLU | 2017 |
| GELU | (-‚àû, ‚àû) | Lent | **Transformers (GPT, BERT)** | 2016 |
| Mish | (-‚àû, ‚àû) | Lent | Vision profonde | 2019 |
| Softmax | [0, 1] (somme=1) | Moyen | Sortie multi-classe | Classique |
| Softplus | (0, ‚àû) | Lent | ReLU lisse | Classique |
| Hard Sigmoid | [0, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |
| Hard Tanh | [-1, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |

---

## Guide de S√©lection

### Par Cas d'Usage

| **Cas d'Usage** | **Fonction Recommand√©e** | **Raison** |
|-----------------|--------------------------|------------|
| **Couches cach√©es (d√©faut 2024)** | **ReLU** | Rapide, efficace, standard industriel |
| Couches cach√©es (si neurones morts) | **Leaky ReLU** ou **ELU** | Gradient toujours actif |
| Couches cach√©es (r√©seaux profonds) | **SELU** ou **ELU** | Auto-normalisation, √©vite gradient qui dispara√Æt |
| Couches cach√©es (recherche de performance) | **Swish** ou **Mish** | Performance SOTA sur certaines t√¢ches |
| **Transformers / NLP (GPT, BERT)** | **GELU** | Standard pour attention mechanisms |
| **Vision par ordinateur (CNN)** | **ReLU** ou **Mish** | Rapide pour CNN, Mish pour profonds |
| R√©seaux r√©currents (RNN, LSTM) | **Tanh** | Standard historique pour gates |
| **Sortie classification binaire** | **Sigmoid** | Sortie [0,1] = probabilit√© |
| **Sortie classification multi-classes** | **Softmax** | Distribution de probabilit√©s (somme=1) |
| **Sortie r√©gression** | **Lin√©aire** (aucune) | Valeurs continues illimit√©es |
| Sortie r√©gression (valeurs positives) | **Softplus** ou **ReLU** | Force sortie ‚â• 0 |
| **Appareils embarqu√©s / Mobile** | **Hard Sigmoid** / **Hard Tanh** | Pas d'exponentielle, ultra-rapide |
| Recherche / Exp√©rimentation | **PReLU** | Alpha adaptatif par neurone |

### Par Priorit√©

#### üèÜ **Si tu veux la meilleure performance (sans contrainte)** :
1. **Couches cach√©es** : GELU, Swish, Mish
2. **Sortie** : Softmax (multi-classe), Sigmoid (binaire)

#### ‚ö° **Si tu veux la rapidit√© (contrainte temps r√©el)** :
1. **Couches cach√©es** : ReLU, Leaky ReLU
2. **Embarqu√©** : Hard Sigmoid, Hard Tanh

#### üéØ **Si tu veux la stabilit√© (r√©seaux tr√®s profonds)** :
1. **Couches cach√©es** : SELU (avec initialisation LeCun), ELU
2. **√âviter** : Sigmoid, Tanh (gradient qui dispara√Æt)

#### üîß **Si tu d√©butes / prototype rapide** :
1. **D√©faut recommand√©** : ReLU partout sauf sortie
2. **Sortie** : Sigmoid (binaire), Softmax (multi-classe)

### Par Type de R√©seau

| **Architecture** | **Couches Cach√©es** | **Sortie** |
|------------------|---------------------|------------|
| **Feedforward simple** | ReLU | Sigmoid / Softmax |
| **Feedforward profond** | SELU, ELU | Sigmoid / Softmax |
| **CNN (Computer Vision)** | ReLU, Mish | Softmax |
| **RNN / LSTM** | Tanh | Sigmoid / Softmax |
| **Transformer** | GELU | Softmax |
| **GAN (G√©n√©rateur)** | ReLU, Leaky ReLU | Tanh |
| **GAN (Discriminateur)** | Leaky ReLU | Sigmoid |
| **Autoencoder** | ReLU | Sigmoid (binaire), Lin√©aire (continu) |
| **Reinforcement Learning** | ReLU, ELU | Lin√©aire, Softmax |

### Arbre de D√©cision

```
Quelle est ta couche ?
‚îú‚îÄ Couche de SORTIE
‚îÇ  ‚îú‚îÄ Classification binaire ? ‚Üí Sigmoid
‚îÇ  ‚îú‚îÄ Classification multi-classes ? ‚Üí Softmax
‚îÇ  ‚îú‚îÄ R√©gression (valeurs continues) ? ‚Üí Lin√©aire (aucune activation)
‚îÇ  ‚îî‚îÄ R√©gression (valeurs positives) ? ‚Üí Softplus / ReLU
‚îÇ
‚îî‚îÄ Couche CACH√âE
   ‚îú‚îÄ Contrainte de VITESSE ?
   ‚îÇ  ‚îú‚îÄ Ultra-rapide (embarqu√©) ? ‚Üí Hard Sigmoid / Hard Tanh
   ‚îÇ  ‚îî‚îÄ Rapide ‚Üí ReLU, Leaky ReLU
   ‚îÇ
   ‚îú‚îÄ Type de R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Transformer / NLP ? ‚Üí GELU
   ‚îÇ  ‚îú‚îÄ CNN profond ? ‚Üí Mish
   ‚îÇ  ‚îú‚îÄ RNN / LSTM ? ‚Üí Tanh
   ‚îÇ  ‚îî‚îÄ Feedforward ? ‚Üí Voir ci-dessous
   ‚îÇ
   ‚îú‚îÄ Profondeur du R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Peu de couches (< 5) ? ‚Üí ReLU
   ‚îÇ  ‚îú‚îÄ Profond (> 10 couches) ? ‚Üí SELU, ELU
   ‚îÇ  ‚îî‚îÄ Tr√®s profond (> 50) ? ‚Üí SELU avec LeCun init
   ‚îÇ
   ‚îú‚îÄ Probl√®me de NEURONES MORTS (gradient = 0) ?
   ‚îÇ  ‚îú‚îÄ Oui ‚Üí Leaky ReLU, PReLU, ELU
   ‚îÇ  ‚îî‚îÄ Non ‚Üí ReLU
   ‚îÇ
   ‚îî‚îÄ Recherche de PERFORMANCE maximale ?
      ‚îú‚îÄ Oui (GPU puissant) ‚Üí Swish, Mish, GELU
      ‚îî‚îÄ Non ‚Üí ReLU (d√©faut)
```

### Recommandations par Ann√©e

| **√âpoque** | **Standard** | **Contexte** |
|------------|--------------|--------------|
| 1990-2010 | Sigmoid, Tanh | R√©seaux peu profonds |
| 2010-2015 | ReLU | R√©volution deep learning |
| 2015-2017 | Leaky ReLU, ELU, PReLU | Am√©lioration ReLU |
| 2017-2019 | Swish, SELU | Auto-recherche Google |
| 2019-2024 | **GELU** (transformers), **Mish** (vision) | SOTA actuel |
| 2024+ | **GELU** (d√©faut NLP), **ReLU** (d√©faut vision) | Standard industriel |

### Combinaisons √âprouv√©es

**Classification d'images (CNN) :**
```rust
// Couches conv : ReLU ou Mish
// Couches fully-connected : ReLU
// Sortie : Softmax
```

**Mod√®le de langage (Transformer) :**
```rust
// Attention + FFN : GELU
// Sortie : Softmax
```

**R√©seau profond (> 20 couches) :**
```rust
// Toutes couches cach√©es : SELU
// Initialisation : LeCun normal
// PAS de Batch Normalization
// Sortie : Sigmoid / Softmax
```

**Prototype rapide :**
```rust
// Couches cach√©es : ReLU
// Sortie : Sigmoid (binaire) ou Softmax (multi-classe)
```

### 3. Learning Rate (Taux d'apprentissage)

**Vitesse d'apprentissage** : √† quel point modifier les poids √† chaque √©tape.

- Actuellement : `0.1`
- **Trop petit** ‚Üí apprentissage lent
- **Trop grand** ‚Üí instabilit√©, ne converge pas
- **Typique** : `0.001` √† `0.1`

---

## Fonctions de Perte (Loss Functions)

### Concept de Base

La **loss function** (fonction de perte/co√ªt) mesure **√† quel point le r√©seau se trompe** dans ses pr√©dictions.

**Objectif :** Minimiser l'erreur entre la pr√©diction et la valeur r√©elle.

```
Loss = Diff√©rence(Pr√©diction, Valeur_R√©elle)
```

Plus la loss est **petite** ‚Üí meilleure pr√©diction  
Plus la loss est **grande** ‚Üí pire pr√©diction

### Cycle d'apprentissage

```
1. Forward pass ‚Üí Pr√©diction
2. Calcul de la Loss ‚Üí Mesurer l'erreur
3. Backpropagation ‚Üí Calculer les gradients
4. Update des poids ‚Üí R√©duire la Loss
```

---

### 1. MSE (Mean Squared Error)

**Formule :** $\text{MSE} = \frac{1}{n}\sum(y - \hat{y})^2$

**Usage :** R√©gression (pr√©dire des valeurs continues)

**Avantages :**
- ‚úÖ P√©nalise fortement les grandes erreurs
- ‚úÖ Diff√©rentiable partout
- ‚úÖ Interpr√©tation intuitive

**Inconv√©nients :**
- ‚ùå Pas optimal pour classification
- ‚ùå Gradient qui dispara√Æt avec Sigmoid

**Exemple :**
```
Pr√©diction: 2.5, R√©el: 3.0
Loss = (3.0 - 2.5)¬≤ = 0.25
```

**Impl√©mentation Rust :**
```rust
fn mse(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let diff = predictions - targets;
    (&diff * &diff).sum() / predictions.len() as f64
}
```

---

### 2. MAE (Mean Absolute Error)

**Formule :** $\text{MAE} = \frac{1}{n}\sum|y - \hat{y}|$

**Usage :** R√©gression (moins sensible aux outliers)

**Avantages :**
- ‚úÖ Robuste aux outliers
- ‚úÖ Interpr√©tation intuitive
- ‚úÖ Toutes les erreurs trait√©es lin√©airement

**Inconv√©nients :**
- ‚ùå Gradients constants (convergence plus lente)
- ‚ùå Non diff√©rentiable en z√©ro

**Exemple :**
```
Pr√©diction: 2.5, R√©el: 3.0
Loss = |3.0 - 2.5| = 0.5
```

**Impl√©mentation Rust :**
```rust
fn mae(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    (predictions - targets).mapv(|x| x.abs()).sum() / predictions.len() as f64
}
```

---

### 3. Binary Cross-Entropy (Log Loss)

**Formule :** $\text{BCE} = -\frac{1}{n}\sum[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$

**Usage :** Classification binaire (avec Sigmoid)

**Avantages :**
- ‚úÖ Interpr√©tation probabiliste
- ‚úÖ Gradient plus stable que MSE pour classification
- ‚úÖ Convergence plus rapide
- ‚úÖ Standard pour classification binaire

**Inconv√©nients :**
- ‚ùå N√©cessite pr√©dictions dans [0, 1]
- ‚ùå Instable si pr√©diction = 0 ou 1 (log(0))

**Exemple :**
```
Pr√©diction: 0.9, R√©el: 1 (classe positive)
Loss = -[1√ólog(0.9) + 0√ólog(0.1)] = 0.105  // Bonne pr√©diction

Pr√©diction: 0.1, R√©el: 1 (classe positive)
Loss = -[1√ólog(0.1) + 0√ólog(0.9)] = 2.303  // Grosse erreur!
```

**Impl√©mentation Rust :**
```rust
fn binary_cross_entropy(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let epsilon = 1e-15; // √âviter log(0)
    let mut sum = 0.0;
    for (p, t) in predictions.iter().zip(targets.iter()) {
        let p_clamped = p.max(epsilon).min(1.0 - epsilon);
        sum += -(t * p_clamped.ln() + (1.0 - t) * (1.0 - p_clamped).ln());
    }
    sum / predictions.len() as f64
}
```

---

### 4. Categorical Cross-Entropy

**Formule :** $\text{CCE} = -\sum y_i \log(\hat{y}_i)$

**Usage :** Classification multi-classes (avec Softmax)

**Avantages :**
- ‚úÖ Standard pour multi-classes
- ‚úÖ Interpr√©tation probabiliste claire
- ‚úÖ Gradient bien adapt√© avec Softmax

**Exemple :**
```
Classes: [Chat, Chien, Oiseau]
R√©el:    [1,    0,     0]      // C'est un chat
Pr√©dit:  [0.7,  0.2,   0.1]
Loss = -(1√ólog(0.7) + 0√ólog(0.2) + 0√ólog(0.1)) = 0.357
```

**Impl√©mentation Rust :**
```rust
fn categorical_cross_entropy(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let epsilon = 1e-15;
    -targets.iter()
        .zip(predictions.iter())
        .map(|(t, p)| t * (p.max(epsilon)).ln())
        .sum::<f64>()
}
```

---

### 5. Huber Loss

**Formule :** 
$$\text{Huber} = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \leq \delta \\ \delta(|y - \hat{y}| - \frac{1}{2}\delta) & \text{sinon} \end{cases}$$

**Usage :** R√©gression robuste aux outliers

**Avantages :**
- ‚úÖ Combine MSE (petites erreurs) et MAE (grandes erreurs)
- ‚úÖ Moins sensible aux outliers que MSE
- ‚úÖ Diff√©rentiable partout

**Param√®tre :** $\delta$ (typiquement 1.0) = seuil entre comportement MSE et MAE

**Impl√©mentation Rust :**
```rust
fn huber_loss(predictions: &Array1<f64>, targets: &Array1<f64>, delta: f64) -> f64 {
    let diff = predictions - targets;
    let mut sum = 0.0;
    for &d in diff.iter() {
        let abs_d = d.abs();
        if abs_d <= delta {
            sum += 0.5 * d * d;  // MSE pour petites erreurs
        } else {
            sum += delta * (abs_d - 0.5 * delta);  // MAE pour grandes erreurs
        }
    }
    sum / predictions.len() as f64
}
```

---

### Guide de S√©lection des Loss Functions

| **T√¢che** | **Activation Sortie** | **Loss Function Recommand√©e** | **Pourquoi** |
|-----------|----------------------|-------------------------------|--------------|
| R√©gression | Linear | **MSE** | Standard, p√©nalise grandes erreurs |
| R√©gression robuste | Linear | **MAE** ou **Huber** | R√©siste aux outliers |
| Classification binaire | Sigmoid | **Binary Cross-Entropy** | Interpr√©tation probabiliste |
| Classification multi-classes | Softmax | **Categorical Cross-Entropy** | Standard multi-classes |
| D√©tection d'objets | Variable | IoU Loss, Focal Loss | Adapt√© aux bo√Ætes |
| Segmentation | Softmax | Dice Loss, Focal Loss | Adapt√© aux pixels |

---

### Comparaison MSE vs Binary Cross-Entropy (XOR)

**Probl√®me :** Classification binaire avec Sigmoid

#### MSE pour classification
- ‚ùå Gradient qui dispara√Æt quand proche de 0 ou 1
- ‚ùå Pas d'interpr√©tation probabiliste
- ‚ùå Convergence plus lente

#### Binary Cross-Entropy pour classification
- ‚úÖ Gradient plus stable
- ‚úÖ Converge plus vite
- ‚úÖ Interpr√©tation comme probabilit√©
- ‚úÖ Meilleur choix pour XOR

**R√©sultats typiques (50k epochs, lr=0.5) :**
```
MSE:  Final loss: 0.0000 ‚úì
BCE:  Final loss: 0.0000 ‚úì
MAE:  Final loss: 0.2500 (n√©cessite lr=0.2, epochs=150k)
```

---

### Visualisation de la Convergence

```
Haute Loss ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
                    ‚îÉ    D√©but
                    ‚îÉ      ‚Üì
                    ‚îÉ      ‚Ä¢
                    ‚îÉ     ‚ï±
                    ‚îÉ    ‚ï±
                    ‚îÉ   ‚ï±     Training
                    ‚îÉ  ‚ï±      ‚Üì
                    ‚îÉ ‚ï±
Basse Loss ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÉ‚ï±________‚Ä¢ Convergence
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
                         Epochs
```

**Objectif du training :** Descendre cette courbe le plus vite possible en ajustant les poids.

---

## Documentation Recommand√©e

1. **[3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)** (YouTube) : visualisations excellentes
2. **[The Rust ML Book](https://rust-ml.github.io/book/)** : apprentissage automatique en Rust
3. **[ndarray docs](https://docs.rs/ndarray/latest/ndarray/)** : documentation de la biblioth√®que
4. **Neural Networks from Scratch** (livre) : explications math√©matiques d√©taill√©es
5. **[ML Cheatsheet - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)** : r√©f√©rence compl√®te

## Exp√©rimentation

### Architecture
```rust
Network::new(2, 5, 1)   // 5 neurones cach√©s
Network::new(2, 10, 1)  // 10 neurones cach√©s
```

### Learning Rate
```rust
let learning_rate = 0.01;  // Plus lent
let learning_rate = 0.5;   // Plus rapide
let learning_rate = 1.0;   // Tr√®s rapide (attention √† la stabilit√©)
```

### Fonction d'Activation et Loss
```rust
// Classification binaire (XOR)
Network::new(2, 5, 1, 
    Activation::Tanh,           // Couche cach√©e
    Activation::Sigmoid,        // Sortie
    LossFunction::BinaryCrossEntropy)

// R√©gression
Network::new(4, 10, 1,
    Activation::ReLU,           // Couche cach√©e
    Activation::Linear,         // Sortie
    LossFunction::MSE)

// Multi-classes
Network::new(784, 128, 10,
    Activation::GELU,           // Couche cach√©e
    Activation::Softmax,        // Sortie
    LossFunction::CategoricalCrossEntropy)
```
