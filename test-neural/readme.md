# R√©seau de Neurones en Rust

## Quick Start

### Compilation et Ex√©cution

```bash
# Compiler le projet
cargo build --release

# Ex√©cuter le programme principal
cargo run --release

# Ex√©cuter les exemples
cargo run --release --example xor_tests       # Tests de fonctions de perte et r√©seaux profonds
cargo run --release --example serialization   # D√©monstration save/load de mod√®les
cargo run --release --example metrics_demo    # D√©monstration des m√©triques d'√©valuation
```

### Exemples Disponibles

1. **`xor_tests`** - Tests complets du r√©seau
   - Teste toutes les fonctions de perte (MSE, MAE, BCE, Huber)
   - Teste diff√©rentes combinaisons d'activations
   - Teste les r√©seaux profonds multi-couches
   - Validation compl√®te sur le probl√®me XOR

2. **`serialization`** - Persistance des mod√®les
   - Entra√Æne un r√©seau sur XOR
   - Sauvegarde en JSON (human-readable) et binaire (compact)
   - Charge et v√©rifie les pr√©dictions
   - Compare les tailles de fichiers

3. **`metrics_demo`** - √âvaluation de performance
   - Entra√Æne un r√©seau sur XOR
   - Calcule accuracy, precision, recall, F1-score
   - Affiche la matrice de confusion
   - Compare diff√©rents seuils de classification
   - Calcule ROC-AUC

### Utilisation Basique

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::io;
use test_neural::metrics::accuracy;
use ndarray::array;

// Cr√©er un r√©seau simple avec optimiseur Adam
let mut network = Network::new(
    2,                              // 2 entr√©es
    5,                              // 5 neurones cach√©s
    1,                              // 1 sortie
    Activation::Tanh,               // Activation couche cach√©e
    Activation::Sigmoid,            // Activation sortie
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.01)       // Optimiseur Adam, lr=0.01
);

// Entra√Æner (learning rate est dans l'optimiseur)
let input = array![0.0, 1.0];
let target = array![1.0];
network.train(&input, &target);

// Pr√©dire
let prediction = network.predict(&input);

// √âvaluer
let predictions = vec![network.predict(&array![0.0, 1.0])];
let targets = vec![array![1.0]];
let acc = accuracy(&predictions, &targets, 0.5);
println!("Accuracy: {:.2}%", acc * 100.0);

// Sauvegarder
io::save_json(&network, "model.json").unwrap();

// Charger
let loaded = io::load_json("model.json").unwrap();
```

---

## Optimiseurs

Le module `optimizer` fournit 5 algorithmes d'optimisation modernes pour l'entra√Ænement des r√©seaux.

### Optimiseurs Disponibles

#### 1. **SGD** - Stochastic Gradient Descent (Simple)
```rust
use test_neural::optimizer::OptimizerType;

let optimizer = OptimizerType::sgd(0.1);
```
- **Utilisation** : Basique, pour d√©buter ou tester
- **Learning rate** : Typiquement 0.01 - 0.5
- **Avantages** : Simple, rapide, reproductible
- **Inconv√©nients** : Convergence lente, n√©cessite tuning du LR

#### 2. **Momentum** - SGD avec momentum
```rust
let optimizer = OptimizerType::momentum(0.1);  // beta=0.9 par d√©faut
```
- **Utilisation** : Acc√©l√®re la convergence
- **Learning rate** : Typiquement 0.01 - 0.1
- **Avantages** : Plus rapide que SGD, navigue mieux les vall√©es
- **Beta** : 0.9 (d√©faut) accumule 90% du gradient pr√©c√©dent

#### 3. **RMSprop** - Root Mean Square Propagation
```rust
let optimizer = OptimizerType::rmsprop(0.01);  // beta=0.9, epsilon=1e-8
```
- **Utilisation** : Adapte le learning rate par param√®tre
- **Learning rate** : Typiquement 0.001 - 0.01
- **Avantages** : G√®re bien les gradients instables
- **Id√©al pour** : RNN, probl√®mes avec gradients variables

#### 4. **Adam** - Adaptive Moment Estimation (Recommand√© ‚≠ê)
```rust
let optimizer = OptimizerType::adam(0.001);  // beta1=0.9, beta2=0.999, epsilon=1e-8
```
- **Utilisation** : **Standard moderne pour la plupart des cas**
- **Learning rate** : Typiquement 0.001 (3e-4 √† 1e-3)
- **Avantages** : 
  - Combine momentum + RMSprop
  - Convergence 2-10x plus rapide que SGD
  - Adapte le LR par param√®tre
  - Correction de biais au d√©but
- **Id√©al pour** : Deep learning en g√©n√©ral, par d√©faut

#### 5. **AdamW** - Adam avec Weight Decay d√©coupl√©
```rust
let optimizer = OptimizerType::adamw(0.001, 0.01);  // lr=0.001, weight_decay=0.01
```
- **Utilisation** : Am√©liore la g√©n√©ralisation
- **Learning rate** : Typiquement 0.001
- **Weight decay** : Typiquement 0.01 - 0.1
- **Avantages** : Meilleure r√©gularisation que L2 classique
- **Id√©al pour** : Grands mod√®les, pr√©venir l'overfitting

### Comparaison de Performance

```bash
cargo run --example optimizer_comparison --release
```

R√©sultats sur XOR (2000 epochs) :
| Optimiseur | Loss finale | Vitesse | Remarques |
|-----------|-------------|---------|-----------|
| SGD (lr=0.5) | 0.000471 | üê¢ Lent | N√©cessite LR √©lev√© |
| Momentum (lr=0.1) | 0.000138 | üèÉ Rapide | 3x plus rapide que SGD |
| RMSprop (lr=0.01) | ~0.000000 | üöÄ Tr√®s rapide | Excellente convergence |
| Adam (lr=0.01) | 0.000207 | üöÄ Tr√®s rapide | **Meilleur compromis** |
| AdamW (lr=0.01) | 0.001215 | üöÄ Rapide | Meilleure g√©n√©ralisation |

### Personnalisation des Param√®tres

```rust
use test_neural::optimizer::OptimizerType;

// Momentum personnalis√©
let momentum = OptimizerType::Momentum { 
    learning_rate: 0.05, 
    beta: 0.95  // Plus de momentum
};

// Adam personnalis√©
let adam = OptimizerType::Adam {
    learning_rate: 0.0005,
    beta1: 0.9,      // Momentum
    beta2: 0.999,    // Variance
    epsilon: 1e-8    // Stabilit√© num√©rique
};

// AdamW personnalis√©
let adamw = OptimizerType::AdamW {
    learning_rate: 0.001,
    beta1: 0.9,
    beta2: 0.999,
    epsilon: 1e-8,
    weight_decay: 0.05  // Plus de r√©gularisation
};

let network = Network::new(2, 5, 1, 
    Activation::ReLU, 
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    adam
);
```

### Guide de S√©lection

| Cas d'Usage | Optimiseur Recommand√© | Raison |
|-------------|----------------------|--------|
| **Premier essai / Prototype** | Adam (lr=0.001) | Fonctionne dans 90% des cas |
| **Petit dataset** | AdamW (wd=0.01) | √âvite l'overfitting |
| **Grand dataset** | Adam ou SGD + Momentum | SGD scale mieux |
| **Recherche / Benchmark** | SGD avec schedule | Reproductibilit√© |
| **Gradients instables** | RMSprop | Adapte le LR |
| **Besoin de vitesse** | Adam | Convergence la plus rapide |

### Conseils Pratiques

**Learning Rates de D√©part :**
- SGD : 0.01 - 0.1
- Momentum : 0.01 - 0.1  
- RMSprop : 0.001 - 0.01
- Adam : **0.001** (le plus universel)
- AdamW : 0.001

**Si l'entra√Ænement ne converge pas :**
1. R√©duire le learning rate (√∑10)
2. Essayer Adam si vous utilisiez SGD
3. V√©rifier l'initialisation des poids (Xavier pour Sigmoid/Tanh, He pour ReLU)

**Pour de meilleurs r√©sultats :**
- Adam est le meilleur choix par d√©faut
- AdamW si vous observez de l'overfitting
- Momentum + SGD pour la recherche acad√©mique
- RMSprop pour les RNN/LSTM

---

## R√©gularisation

La r√©gularisation permet de **pr√©venir l'overfitting** en p√©nalisant les mod√®les trop complexes qui "m√©morisent" les donn√©es d'entra√Ænement au lieu de g√©n√©raliser.

### Qu'est-ce que l'Overfitting ?

**Overfitting** = Le mod√®le performe tr√®s bien sur les donn√©es d'entra√Ænement mais mal sur de nouvelles donn√©es.

**Signes d'overfitting :**
- Loss d'entra√Ænement tr√®s faible mais loss de validation √©lev√©e
- Pr√©dictions parfaites sur le training set, mauvaises sur le test set
- Poids tr√®s grands dans le r√©seau

**Solution : R√©gularisation** üõ°Ô∏è

### Types de R√©gularisation

#### 1. **Dropout** - D√©sactivation Al√©atoire de Neurones

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;

let network = Network::new(
    2, 20, 1,
    Activation::ReLU,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
).with_dropout(0.3);  // 30% des neurones d√©sactiv√©s pendant training
```

**Comment √ßa marche :**
- **Training** : D√©sactive al√©atoirement 30% des neurones (rate=0.3)
- **Inference** : Tous les neurones actifs (mise √† l'√©chelle automatique)
- Force le r√©seau √† ne pas d√©pendre d'un seul neurone

**Quand l'utiliser :**
- Dataset petit (risque d'overfitting √©lev√©)
- R√©seaux profonds ou larges
- Typiquement : **0.2 - 0.5** pour couches cach√©es

**Avantages :**
- Tr√®s efficace contre l'overfitting
- √âquivalent √† entra√Æner un ensemble de mod√®les
- Pas de co√ªt computationnel en inference

#### 2. **L2 Regularization (Weight Decay)** - P√©nalise les Grands Poids

```rust
let network = Network::new(
    2, 20, 1,
    Activation::ReLU,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
).with_l2(0.01);  // Lambda = 0.01
```

**Comment √ßa marche :**
- Ajoute une p√©nalit√© proportionnelle au carr√© des poids : `loss += 0.5 * lambda * Œ£(w¬≤)`
- Pousse les poids vers z√©ro (mais jamais exactement z√©ro)
- Favorise des solutions plus "lisses" et simples

**Quand l'utiliser :**
- **Par d√©faut** pour la plupart des mod√®les
- Lambda typique : **0.0001 - 0.01**
- Plus lambda est grand, plus la r√©gularisation est forte

**Avantages :**
- Simple et efficace
- Stabilise l'entra√Ænement
- Am√©liore la g√©n√©ralisation

#### 3. **L1 Regularization (Lasso)** - Encourage la Sparsit√©

```rust
let network = Network::new(
    2, 50, 1,
    Activation::ReLU,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
).with_l1(0.01);  // Lambda = 0.01
```

**Comment √ßa marche :**
- Ajoute une p√©nalit√© proportionnelle √† la valeur absolue des poids : `loss += lambda * Œ£|w|`
- Pousse de nombreux poids **exactement √† z√©ro**
- S√©lection automatique de features

**Quand l'utiliser :**
- Besoin de **sparsit√©** (poids √† z√©ro)
- S√©lection de features automatique
- Interpr√©tabilit√© du mod√®le

**Avantages :**
- Mod√®les plus compacts (beaucoup de poids √† 0)
- Feature selection int√©gr√©e
- Meilleure interpr√©tabilit√©

#### 4. **Elastic Net** - Combine L1 et L2

```rust
let network = Network::new(
    2, 50, 1,
    Activation::ReLU,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
).with_elastic_net(0.5, 0.01);  // 50% L1, 50% L2
```

**Comment √ßa marche :**
- Combine les avantages de L1 et L2
- `l1_ratio` contr√¥le la balance (0.0 = pur L2, 1.0 = pur L1)

**Quand l'utiliser :**
- Quand vous voulez sparsit√© ET stabilit√©
- Features corr√©l√©es

### Modes Training vs Eval

**Important** : Le dropout doit √™tre d√©sactiv√© lors de l'inf√©rence !

```rust
// Training
network.train_mode();  // Active le dropout
for epoch in 0..1000 {
    for (input, target) in train_data {
        network.train(&input, &target);
    }
}

// Evaluation/Inference
network.eval_mode();  // D√©sactive le dropout
let predictions = test_data.iter()
    .map(|input| network.predict(input))
    .collect();
```

### Combiner Plusieurs R√©gularisations

```rust
// Dropout + L2 (approche recommand√©e)
let network = Network::new(
    2, 100, 1,
    Activation::ReLU,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.001)
)
.with_dropout(0.2)   // Dropout l√©ger
.with_l2(0.005);     // L2 mod√©r√©

// Entra√Ænement
network.train_mode();
// ... training loop ...

// Inference
network.eval_mode();
let prediction = network.predict(&input);
```

### Guide de S√©lection

| Situation | R√©gularisation Recommand√©e | Param√®tres |
|-----------|---------------------------|------------|
| **Dataset petit (<1000 exemples)** | Dropout + L2 | dropout=0.3-0.5, Œª=0.01 |
| **Dataset moyen (1k-100k)** | L2 ou Dropout l√©ger | dropout=0.2, Œª=0.001-0.01 |
| **Dataset grand (>100k)** | L2 faible | Œª=0.0001-0.001 |
| **R√©seau tr√®s large** | Dropout fort | dropout=0.4-0.5 |
| **Besoin de sparsit√©** | L1 | Œª=0.01-0.1 |
| **Features corr√©l√©es** | Elastic Net | l1_ratio=0.5, Œª=0.01 |

### Conseils Pratiques

**Diagnostic de l'overfitting :**
1. Split vos donn√©es : train (70%), validation (15%), test (15%)
2. Surveillez train_loss vs val_loss
3. Si val_loss monte pendant que train_loss baisse ‚Üí **Overfitting !**

**Solutions par ordre de priorit√© :**
1. **Plus de donn√©es** (si possible)
2. **Dropout** (0.3-0.5) - Le plus efficace
3. **L2 regularization** (0.001-0.01)
4. **R√©duire la taille du r√©seau**
5. **Early stopping**

**Tuning des hyperparam√®tres :**
- Commencer sans r√©gularisation
- Si overfitting : ajouter Dropout (0.3)
- Si encore overfitting : augmenter dropout (0.4-0.5) ou ajouter L2
- Si underfitting : r√©duire la r√©gularisation

### Exemple Complet

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;

// Cr√©er un r√©seau avec r√©gularisation
let mut network = Network::new(
    784,    // MNIST input size
    128,    // Hidden layer (large)
    10,     // 10 classes
    Activation::ReLU,
    Activation::Softmax,
    LossFunction::CategoricalCrossEntropy,
    OptimizerType::adam(0.001)
)
.with_dropout(0.3)   // Prevent overfitting
.with_l2(0.001);     // Weight decay

// Training mode
network.train_mode();
for epoch in 0..epochs {
    for (input, target) in train_data.iter() {
        network.train(input, target);
    }
    
    // Validation (en mode eval)
    network.eval_mode();
    let val_loss = network.evaluate(&val_inputs, &val_targets);
    println!("Epoch {}: val_loss = {:.4}", epoch, val_loss);
    network.train_mode();  // Retour en mode training
}

// Final evaluation
network.eval_mode();
let test_accuracy = accuracy(&test_predictions, &test_targets, 0.5);
println!("Test Accuracy: {:.2}%", test_accuracy * 100.0);
```

### D√©mo

```bash
cargo run --example regularization_demo --release
```

R√©sultats sur XOR avec r√©seau surdimensionn√© (2 ‚Üí [20] ‚Üí 1) :
| M√©thode | Loss Finale | Convergence | G√©n√©ralisation |
|---------|-------------|-------------|----------------|
| Sans r√©gularisation | 0.000000 | Tr√®s rapide | Risque d'overfitting |
| Dropout (0.3) | 0.000001 | Stable | Excellente |
| L2 (0.01) | 0.135389 | Lente | Tr√®s bonne |
| L1 (0.01) | Variable | Instable | Bonne avec sparsit√© |
| Combin√© | 0.00001 | **Optimale** | **Meilleure** |

**Conclusion** : Sur les petits datasets, **Dropout + L2** offre le meilleur compromis.

---

## Mini-Batch Training

Le **mini-batch training** consiste √† entra√Æner le r√©seau sur des groupes d'exemples (batches) au lieu d'un seul exemple √† la fois. C'est une technique essentielle pour l'entra√Ænement efficace sur de grands datasets.

### Pourquoi Mini-Batch ?

**‚ùå Probl√®mes du Single-Sample Training (SGD pur):**
- Tr√®s lent sur grands datasets
- Gradients bruit√©s ‚Üí convergence instable
- Impossible d'utiliser la vectorisation
- Mise √† jour trop fr√©quente des poids

**‚úÖ Avantages du Mini-Batch:**
- **2-3x plus rapide** en pratique
- Gradients plus stables (moyenne sur le batch)
- Meilleure utilisation du cache CPU
- Convergence plus smooth
- Permet la parall√©lisation

### Utilisation du Module `dataset`

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::dataset::Dataset;
use ndarray::array;

// 1. Cr√©er le dataset
let inputs = vec![
    array![0.0, 0.0], array![0.0, 1.0],
    array![1.0, 0.0], array![1.0, 1.0],
];
let targets = vec![
    array![0.0], array![1.0],
    array![1.0], array![0.0],
];

let dataset = Dataset::new(inputs, targets);

// 2. Split train/validation/test
let (train, val, test) = dataset.split_three(0.7, 0.15, 0.15);
// R√©sultat: 70% train, 15% validation, 15% test

// 3. Cr√©er le r√©seau (learning rate plus √©lev√© pour batch training)
let mut network = Network::new(
    2, 8, 1,
    Activation::Tanh,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.01)  // 10x plus que single-sample (0.001)
);

// 4. Entra√Æner avec mini-batches
let batch_size = 32;
let epochs = 100;

for epoch in 0..epochs {
    // IMPORTANT: Shuffle avant chaque epoch !
    train.shuffle();
    
    // It√©rer sur les batches
    for (batch_inputs, batch_targets) in train.batches(batch_size) {
        network.train_batch(&batch_inputs, &batch_targets);
    }
    
    // √âvaluation p√©riodique
    if epoch % 10 == 0 {
        let train_loss = network.evaluate(train.inputs(), train.targets());
        let val_loss = network.evaluate(val.inputs(), val.targets());
        println!("Epoch {}: train={:.4}, val={:.4}", epoch, train_loss, val_loss);
    }
}

// 5. Test final
let test_loss = network.evaluate(test.inputs(), test.targets());
println!("Test loss: {:.6}", test_loss);
```

### API du Module Dataset

#### **`Dataset::new(inputs, targets)`**
Cr√©e un dataset √† partir de vecteurs d'inputs et targets.

```rust
let dataset = Dataset::new(inputs, targets);
println!("Dataset size: {}", dataset.len());
```

#### **`dataset.shuffle()`**
M√©lange al√©atoirement l'ordre des exemples.

```rust
dataset.shuffle();  // √Ä appeler avant chaque epoch !
```

‚ö†Ô∏è **IMPORTANT** : Toujours shuffle entre les epochs pour √©viter que le r√©seau apprenne l'ordre des exemples.

#### **`dataset.split(ratio)`**
Split en train/test.

```rust
let (train, test) = dataset.split(0.8);  // 80% train, 20% test
```

#### **`dataset.split_three(train_ratio, val_ratio)`**
Split en train/validation/test.

```rust
let (train, val, test) = dataset.split_three(0.7, 0.15);
// 70% train, 15% val, 15% test (le reste)
```

#### **`dataset.batches(batch_size)`**
Retourne un iterator sur les batches.

```rust
for (batch_inputs, batch_targets) in dataset.batches(32) {
    network.train_batch(&batch_inputs, &batch_targets);
}
```

### Comparaison: Single-Sample vs Mini-Batch

Sur un dataset de 1000 exemples (50 epochs):

| M√©thode | Batch Size | Temps | Loss Finale | Speedup |
|---------|------------|-------|-------------|---------|
| Single-sample | 1 | 0.10s | 0.000000 | 1.0x (baseline) |
| Mini-batch | 32 | **0.05s** | 0.001794 | **2.1x** ‚ö° |
| Mini-batch | 64 | 0.05s | 0.006283 | 2.1x |
| Mini-batch | 128 | 0.05s | 0.015565 | 2.2x |

**R√©sultats** (exemple minibatch_demo.rs):
- **batch_size=32** offre le meilleur compromis vitesse/qualit√©
- Plus le batch est grand, plus c'est rapide mais convergence l√©g√®rement moins bonne
- Ajuster le learning rate : √ó10 pour batch training vs single-sample

### Guide de S√©lection du Batch Size

| Taille Dataset | Batch Size Recommand√© | Raison |
|----------------|----------------------|--------|
| < 1000 exemples | 16-32 | Dataset petit, petits batches suffisent |
| 1000-10k exemples | 32-64 | Compromis optimal |
| 10k-100k exemples | 64-128 | Meilleure vectorisation |
| > 100k exemples | 128-256 | Maximiser la vitesse |

**R√®gles g√©n√©rales:**
- Batch size **trop petit** (< 16) : trop lent, gradients bruit√©s
- Batch size **trop grand** (> 256) : convergence plus difficile, demande plus de m√©moire
- **Puissance de 2** (16, 32, 64, 128) : optimis√© pour le CPU
- Toujours **augmenter le learning rate** proportionnellement au batch size

### Ajuster le Learning Rate

```rust
// Single-sample training
OptimizerType::adam(0.001)

// Mini-batch training (batch_size=32)
OptimizerType::adam(0.01)   // 10x plus √©lev√©

// Mini-batch training (batch_size=128)
OptimizerType::adam(0.03)   // 30x plus √©lev√©
```

**R√®gle empirique** : Learning rate ‚âà 0.001 √ó sqrt(batch_size)

### Conseils Pratiques

‚úÖ **√Ä faire:**
- Toujours `shuffle()` le dataset avant chaque epoch
- Split en train/val/test pour d√©tecter l'overfitting
- Commencer avec batch_size=32 puis exp√©rimenter
- Augmenter le learning rate pour le batch training
- Surveiller la loss de validation (early stopping)

‚ùå **√Ä √©viter:**
- Oublier de shuffle ‚Üí le r√©seau apprend l'ordre !
- Batch size de 1 sur grand dataset (trop lent)
- Utiliser le m√™me learning rate que single-sample
- Batch size > 10% du dataset (perd le b√©n√©fice SGD)

### D√©mo

```bash
cargo run --example minibatch_demo --release
```

R√©sultats sur dataset XOR √©largi (1000 exemples, 50 epochs):
```
üìà Temps d'entra√Ænement:
  ‚Ä¢ Single-sample:  0.10s
  ‚Ä¢ Mini-batch (32): 0.05s (2.1x speedup) ‚ö°

üéØ Loss finale (test):
  ‚Ä¢ Single-sample:  0.000000
  ‚Ä¢ Mini-batch (32): 0.001794  (excellent compromis)
```

**Conclusion** : Le mini-batch training est **essentiel** pour datasets > 1000 exemples. Batch size 32 offre le meilleur compromis.

---

## Callbacks - Automatisation de l'Entra√Ænement

Les **callbacks** sont des fonctions qui s'ex√©cutent automatiquement √† diff√©rents moments de l'entra√Ænement (d√©but/fin epoch, d√©but/fin training). Ils permettent d'**automatiser** et d'**optimiser** l'entra√Ænement sans modifier la boucle principale.

### Pourquoi les Callbacks ?

**‚ùå Probl√®mes sans callbacks:**
- Code d'entra√Ænement r√©p√©titif et verbeux
- Difficile de surveiller la progression
- Pas de sauvegarde automatique du meilleur mod√®le
- Surentra√Ænement (overfitting) si on ne surveille pas
- Learning rate fixe = convergence sous-optimale

**‚úÖ Avec callbacks:**
- **EarlyStopping** : Arr√™te automatiquement si overfitting
- **ModelCheckpoint** : Sauvegarde le meilleur mod√®le
- **LearningRateScheduler** : Adapte le LR dynamiquement
- **ProgressBar** : Affiche la progression en temps r√©el
- Code propre, maintenable, r√©utilisable

### Callbacks Disponibles

#### 1. **EarlyStopping** - Arr√™t Pr√©coce

Surveille la validation loss et **arr√™te l'entra√Ænement** apr√®s `patience` epochs sans am√©lioration.

```rust
use test_neural::callbacks::EarlyStopping;

let mut early_stop = EarlyStopping::new(
    10,      // patience: attendre 10 epochs sans am√©lioration
    0.0001   // min_delta: am√©lioration minimale requise
);

// Dans la boucle d'entra√Ænement
let mut callbacks: Vec<Box<dyn Callback>> = vec![
    Box::new(early_stop),
];

network.fit(&train, Some(&val), 100, 32, &mut callbacks);
```

**Fonctionnement:**
- Compare val_loss √† chaque epoch
- Si am√©lioration < min_delta pendant `patience` epochs ‚Üí **arr√™te**
- √âvite l'overfitting automatiquement
- √âconomise du temps de calcul

**Quand utiliser:**
- Toujours ! Surtout sur petits datasets
- patience=10-20 pour datasets moyens
- patience=5-10 pour petits datasets
- min_delta=0.0001 typique

#### 2. **ModelCheckpoint** - Sauvegarde Automatique

Sauvegarde automatiquement le mod√®le quand la validation loss **s'am√©liore**.

```rust
use test_neural::callbacks::ModelCheckpoint;

let mut checkpoint = ModelCheckpoint::new(
    "best_model.json",  // Chemin du fichier
    true                // save_best_only: sauvegarder uniquement si am√©lioration
);

let mut callbacks: Vec<Box<dyn Callback>> = vec![
    Box::new(checkpoint),
];

network.fit(&train, Some(&val), 100, 32, &mut callbacks);

// Apr√®s l'entra√Ænement, charger le meilleur mod√®le
let best_network = test_neural::io::load_json("best_model.json").unwrap();
```

**Fonctionnement:**
- Compare val_loss √† chaque epoch
- Si am√©lioration ‚Üí sauvegarde automatique (JSON ou binary)
- Vous r√©cup√©rez le meilleur mod√®le m√™me si l'entra√Ænement overfitte ensuite

**Formats support√©s:**
- `.json` ‚Üí JSON (human-readable)
- `.bin` ‚Üí Binary (compact, 2-3x plus petit)

**Quand utiliser:**
- Entra√Ænements longs (> 50 epochs)
- Quand la loss peut fluctuer
- Pour garder le meilleur mod√®le automatiquement

#### 3. **LearningRateScheduler** - Ajustement Dynamique du LR

Ajuste automatiquement le learning rate pendant l'entra√Ænement. Trois strat√©gies disponibles.

##### **StepLR** - R√©duction √† Intervalles R√©guliers

```rust
use test_neural::callbacks::{LearningRateScheduler, LRSchedule};

let mut scheduler = LearningRateScheduler::new(
    LRSchedule::StepLR {
        step_size: 10,  // R√©duire tous les 10 epochs
        gamma: 0.5      // Diviser LR par 2
    }
);

network.fit_with_scheduler(&train, Some(&val), 50, 32, &mut scheduler, &mut callbacks);
```

**Fonctionnement:**
- Tous les `step_size` epochs: `LR = LR √ó gamma`
- Exemple: LR=0.1 ‚Üí 0.05 ‚Üí 0.025 ‚Üí 0.0125...
- Simple, pr√©visible

**Quand utiliser:**
- Convergence instable avec LR fixe
- Vous connaissez approximativement la dur√©e de l'entra√Ænement
- step_size=10-20 typique

##### **ReduceOnPlateau** - R√©duction Intelligente

```rust
let mut scheduler = LearningRateScheduler::new(
    LRSchedule::ReduceOnPlateau {
        patience: 5,      // Attendre 5 epochs sans am√©lioration
        factor: 0.5,      // Diviser LR par 2
        min_delta: 0.0001 // Am√©lioration minimale
    }
);

network.fit_with_scheduler(&train, Some(&val), 50, 32, &mut scheduler, &mut callbacks);
```

**Fonctionnement:**
- Surveille la validation loss
- Si stagnation pendant `patience` epochs ‚Üí `LR = LR √ó factor`
- S'adapte automatiquement √† la convergence

**Quand utiliser:**
- **Recommand√© dans la plupart des cas**
- Convergence adaptative, intelligente
- Ne n√©cessite pas de conna√Ætre la dur√©e d'entra√Ænement
- patience=5-10 typique

##### **ExponentialLR** - D√©croissance Exponentielle

```rust
let mut scheduler = LearningRateScheduler::new(
    LRSchedule::ExponentialLR {
        gamma: 0.95  // Multiplier LR par 0.95 chaque epoch
    }
);

network.fit_with_scheduler(&train, Some(&val), 50, 32, &mut scheduler, &mut callbacks);
```

**Fonctionnement:**
- Chaque epoch: `LR = LR √ó gamma`
- D√©croissance smooth et continue
- LR diminue exponentiellement

**Quand utiliser:**
- Fine-tuning avec d√©croissance lente
- gamma=0.95-0.99 typique
- Convergence tr√®s smooth

#### 4. **ProgressBar** - Affichage de Progression

Affiche la progression en temps r√©el avec ETA (temps restant estim√©).

```rust
use test_neural::callbacks::ProgressBar;

let mut progress = ProgressBar::new(100);  // 100 epochs total

let mut callbacks: Vec<Box<dyn Callback>> = vec![
    Box::new(progress),
];

network.fit(&train, Some(&val), 100, 32, &mut callbacks);
```

**Affichage:**
```
üöÄ D√©but de l'entra√Ænement (100 epochs)
Epoch 10/100 [10.0%] - train_loss: 0.123456 - val_loss: 0.234567 - ETA: 45s
Epoch 20/100 [20.0%] - train_loss: 0.056789 - val_loss: 0.123456 - ETA: 36s
...
‚úÖ Entra√Ænement termin√© en 50.23s
```

**Quand utiliser:**
- Entra√Ænements longs (> 20 epochs)
- Pour suivre la progression visuellement
- Estim√©e du temps restant utile

### Combiner Plusieurs Callbacks

La vraie puissance vient de la **combinaison** de callbacks :

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::optimizer::OptimizerType;
use test_neural::dataset::Dataset;
use test_neural::callbacks::{
    EarlyStopping, ModelCheckpoint, LearningRateScheduler,
    ProgressBar, LRSchedule, Callback
};

// 1. Cr√©er le r√©seau
let mut network = Network::new(
    2, 8, 1,
    Activation::Tanh,
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy,
    OptimizerType::adam(0.01)
);

// 2. Pr√©parer les donn√©es
let dataset = Dataset::new(inputs, targets);
let (train, val) = dataset.split(0.8);

// 3. Configurer les callbacks
let mut scheduler = LearningRateScheduler::new(
    LRSchedule::ReduceOnPlateau {
        patience: 5,
        factor: 0.5,
        min_delta: 0.0001
    }
);

let mut callbacks: Vec<Box<dyn Callback>> = vec![
    Box::new(EarlyStopping::new(15, 0.00001)),
    Box::new(ModelCheckpoint::new("best_model.json", true)),
    Box::new(ProgressBar::new(100)),
];

// 4. Entra√Æner avec tout automatis√© !
let history = network.fit_with_scheduler(
    &train,
    Some(&val),
    100,        // max epochs
    32,         // batch size
    &mut scheduler,
    &mut callbacks
);

// 5. R√©sultat
println!("Entra√Ænement termin√© en {} epochs", history.len());
println!("Meilleur mod√®le sauvegard√© automatiquement dans best_model.json");
```

**R√©sultat:**
- ‚úÖ Progression affich√©e en temps r√©el
- ‚úÖ Learning rate adapt√© automatiquement quand stagnation
- ‚úÖ Arr√™t automatique si overfitting
- ‚úÖ Meilleur mod√®le sauvegard√© automatiquement
- ‚úÖ Code propre, maintenable, professionnel

### API Compl√®te

#### **M√©thodes d'Entra√Ænement avec Callbacks**

```rust
// Avec callbacks standard (pas de LR scheduler)
pub fn fit(
    &mut self,
    train_dataset: &Dataset,
    val_dataset: Option<&Dataset>,
    epochs: usize,
    batch_size: usize,
    callbacks: &mut Vec<Box<dyn Callback>>,
) -> Vec<(f64, Option<f64>)>  // Retourne history (train_loss, val_loss)

// Avec LR scheduler
pub fn fit_with_scheduler(
    &mut self,
    train_dataset: &Dataset,
    val_dataset: Option<&Dataset>,
    epochs: usize,
    batch_size: usize,
    scheduler: &mut LearningRateScheduler,
    callbacks: &mut Vec<Box<dyn Callback>>,
) -> Vec<(f64, Option<f64>)>
```

#### **Trait Callback** - Cr√©er Vos Propres Callbacks

```rust
pub trait Callback {
    fn on_train_begin(&mut self, network: &Network) {}
    fn on_train_end(&mut self, network: &Network) {}
    fn on_epoch_begin(&mut self, epoch: usize, network: &Network) {}
    fn on_epoch_end(&mut self, epoch: usize, network: &Network, 
                     train_loss: f64, val_loss: Option<f64>) -> bool {
        true  // Return false to stop training
    }
}
```

**Exemple - Callback Personnalis√©:**

```rust
use test_neural::callbacks::Callback;
use test_neural::network::Network;

struct LossLogger {
    losses: Vec<f64>,
}

impl Callback for LossLogger {
    fn on_epoch_end(&mut self, epoch: usize, _network: &Network, 
                     _train_loss: f64, val_loss: Option<f64>) -> bool {
        if let Some(loss) = val_loss {
            self.losses.push(loss);
            println!("Epoch {}: val_loss = {:.6}", epoch, loss);
        }
        true  // Continue training
    }
}
```

### Comparaison: Avec vs Sans Callbacks

| Aspect | Sans Callbacks | Avec Callbacks |
|--------|---------------|----------------|
| **Code** | Verbeux, r√©p√©titif | Concis, r√©utilisable |
| **Monitoring** | Manuel (print dans boucle) | Automatique (ProgressBar) |
| **Sauvegarde** | Manuelle (if best_loss...) | Automatique (ModelCheckpoint) |
| **Overfitting** | Risque √©lev√© | Pr√©venu (EarlyStopping) |
| **Learning Rate** | Fixe, sous-optimal | Adapt√© (LR Scheduler) |
| **Temps dev** | Plus long | Plus court |
| **Maintenabilit√©** | Difficile | Facile |
| **Professionnalisme** | Amateur | Production-ready |

### Guide de S√©lection

| Situation | Callbacks Recommand√©s |
|-----------|----------------------|
| **Prototypage rapide** | ProgressBar |
| **Entra√Ænement long** | EarlyStopping + ProgressBar |
| **Production** | EarlyStopping + ModelCheckpoint + ReduceOnPlateau |
| **Fine-tuning** | ExponentialLR + ModelCheckpoint |
| **Petit dataset** | EarlyStopping (patience=5) + Dropout |
| **Grand dataset** | ReduceOnPlateau + ModelCheckpoint |
| **Optimal (recommand√©)** | **Tous combin√©s !** |

### Conseils Pratiques

‚úÖ **√Ä faire:**
- Toujours utiliser **EarlyStopping** (√©vite overfitting)
- **ModelCheckpoint** pour entra√Ænements > 20 epochs
- **ReduceOnPlateau** = meilleur scheduler dans la plupart des cas
- Combiner plusieurs callbacks pour r√©sultat optimal
- Ajuster `patience` selon la taille du dataset

‚ùå **√Ä √©viter:**
- Entra√Ænement sans validation dataset (impossible d'utiliser callbacks intelligemment)
- patience trop faible (< 5) ‚Üí arr√™t pr√©matur√©
- Oublier save_best_only=true dans ModelCheckpoint
- Ne pas v√©rifier que val_dataset est fourni

### D√©mo

```bash
cargo run --example callbacks_demo --release
```

**R√©sultats** (dataset XOR 1000 exemples, 100 epochs max):

| Configuration | Epochs | Loss Finale | Notes |
|--------------|--------|-------------|-------|
| Baseline (sans callbacks) | 100 | 0.000291 | Overfitting possible |
| EarlyStopping | 90 | 0.000349 | Arr√™t automatique ‚úì |
| ModelCheckpoint | 50 | 0.001442 | Meilleur mod√®le sauvegard√© ‚úì |
| StepLR | 50 | 0.000166 | LR r√©duit 3√ó |
| ReduceOnPlateau | 50 | 0.001441 | LR adapt√© intelligemment ‚úì |
| ExponentialLR | 50 | 0.000685 | D√©croissance smooth |
| **Combinaison optimale** | **90** | **0.000079** | **Meilleur r√©sultat** ‚ö° |

**Observation**: La combinaison **EarlyStopping + ModelCheckpoint + ReduceOnPlateau + ProgressBar** donne les meilleurs r√©sultats avec automatisation compl√®te.

**Conclusion** : Les callbacks transforment l'entra√Ænement de r√©seaux neuronaux. Ils sont **essentiels** pour un code production-ready, √©vitent l'overfitting, et optimisent automatiquement la convergence.

---

## M√©triques d'√âvaluation

Le module `metrics` fournit des outils complets pour √©valuer la performance de vos mod√®les.

### M√©triques Disponibles

#### 1. **`accuracy()`** - Exactitude
```rust
use test_neural::metrics::accuracy;

let acc = accuracy(&predictions, &targets, 0.5);
println!("Accuracy: {:.2}%", acc * 100.0);
```
- **Binaire** : seuil personnalisable (d√©faut 0.5)
- **Multi-classes** : argmax automatique
- Simple, rapide, intuitif
- Retourne le pourcentage de pr√©dictions correctes

#### 2. **`binary_metrics()`** - M√©triques Compl√®tes pour Classification Binaire
```rust
use test_neural::metrics::binary_metrics;

let metrics = binary_metrics(&predictions, &targets, 0.5);
println!("{}", metrics.summary());
// Accuracy: 0.9500 | Precision: 0.9231 | Recall: 0.9600 | F1: 0.9412
// TP: 24 | FP: 2 | TN: 19 | FN: 1
```

**M√©triques retourn√©es :**
- **Precision** : `TP / (TP + FP)` - "Quand je pr√©dis positif, √† quelle fr√©quence ai-je raison?"
- **Recall** : `TP / (TP + FN)` - "Je capture quel % de tous les positifs r√©els?"
- **F1-Score** : Moyenne harmonique de Precision et Recall
- **TP, FP, TN, FN** : True/False Positives/Negatives

#### 3. **`confusion_matrix_binary()` & `confusion_matrix_multiclass()`** - Matrice de Confusion
```rust
use test_neural::metrics::{confusion_matrix_binary, format_confusion_matrix};

let matrix = confusion_matrix_binary(&predictions, &targets, 0.5);
println!("{}", format_confusion_matrix(&matrix, Some(&["Neg", "Pos"])));
```

```
Confusion Matrix:
                Predicted
              Neg      Pos 
Actual   Neg   19        2
         Pos    1       24
```

- **Binaire** : Matrice 2x2
- **Multi-classes** : Matrice NxN
- Helper `format_confusion_matrix()` pour affichage lisible
- Visualise pr√©cis√©ment les types d'erreurs

#### 4. **`roc_curve()` & `auc_roc()`** - Analyse ROC
```rust
use test_neural::metrics::{roc_curve, auc_roc};

// Courbe ROC compl√®te
let (fpr, tpr, thresholds) = roc_curve(&predictions, &targets, 100);

// AUC (Area Under Curve)
let auc = auc_roc(&predictions, &targets);
println!("AUC: {:.4}", auc);
// AUC: 0.9850 (1.0 = parfait, 0.5 = al√©atoire)
```

- **Courbe ROC** : FPR vs TPR √† diff√©rents seuils
- **AUC** : 1.0 = pr√©dictions parfaites, 0.5 = performance al√©atoire
- **Ind√©pendant du seuil** : √âvalue la performance globale
- Id√©al pour comparer diff√©rents mod√®les

### Quand Utiliser Quelle M√©trique ?

| Situation | M√©trique Recommand√©e | Raison |
|-----------|---------------------|--------|
| **Dataset √©quilibr√©** | Accuracy | Simple et intuitif |
| **Dataset d√©s√©quilibr√©** | F1-Score, Recall | √âvite les fausses bonnes performances |
| **Co√ªt FP √©lev√©** (ex: spam) | Precision | Ne pas bloquer vrais emails |
| **Co√ªt FN √©lev√©** (ex: m√©dical) | Recall | Ne pas manquer de malades |
| **Comparaison de mod√®les** | AUC | Ind√©pendant du seuil |
| **Analyse d√©taill√©e** | Confusion Matrix | Voir pr√©cis√©ment les erreurs |

### Exemple Complet

```rust
use test_neural::network::{Network, Activation, LossFunction};
use test_neural::metrics::{accuracy, binary_metrics, confusion_matrix_binary};
use ndarray::array;

// Entra√Æner le r√©seau
let mut network = Network::new(2, 5, 1, 
    Activation::Tanh, 
    Activation::Sigmoid,
    LossFunction::BinaryCrossEntropy
);

// ... entra√Ænement ...

// Obtenir les pr√©dictions
let predictions: Vec<_> = test_inputs.iter()
    .map(|input| network.predict(input))
    .collect();

// √âvaluer avec diff√©rentes m√©triques
let acc = accuracy(&predictions, &test_targets, 0.5);
let metrics = binary_metrics(&predictions, &test_targets, 0.5);
let matrix = confusion_matrix_binary(&predictions, &test_targets, 0.5);

println!("Accuracy: {:.2}%", acc * 100.0);
println!("{}", metrics.summary());
println!("{}", format_confusion_matrix(&matrix, Some(&["Neg", "Pos"])));
```

Pour plus de d√©tails, consultez [METRICS_GUIDE.md](METRICS_GUIDE.md) qui contient :
- Guide complet de toutes les m√©triques
- Cas d'usage par domaine (m√©dical, finance, vision, NLP)
- M√©triques avanc√©es √† impl√©menter
- Bonnes pratiques et pi√®ges √† √©viter

---

## Concepts Cl√©s

### 1. Architecture (Couches/Neurones)

La **structure** de ton r√©seau : nombre de couches et nombre de neurones par couche.

- Dans ton code : `Network::new(2, 3, 1)` = 2 entr√©es ‚Üí 3 neurones cach√©s ‚Üí 1 sortie
- Plus de neurones/couches = plus de capacit√© d'apprentissage, mais risque de **surapprentissage**

### 2. Fonctions d'Activation (sigmoid, ReLU, tanh...)

Fonction qui **transforme** la sortie d'un neurone.

**Actuellement utilis√©e : Sigmoid**
- Formule : `1 / (1 + e^-x)`
- Sortie : entre `[0, 1]`

**Alternatives :**
- **ReLU** : `max(0, x)` ‚Üí plus rapide, standard moderne
- **tanh** : `tanh(x)` ‚Üí sortie entre `[-1, 1]`
- **Leaky ReLU**, **ELU**, etc.

---

## Fonctions d'Activation D√©taill√©es

### Sigmoid (Logistic)
**Formule :** $f(x) = \frac{1}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) \cdot (1 - f(x))$

**Propri√©t√©s :**
- Sortie : `[0, 1]`
- Lisse et diff√©rentiable partout
- Interpr√©table comme une probabilit√©

**Avantages :**
- ‚úÖ Sortie normalis√©e (bonne pour la couche de sortie en classification binaire)
- ‚úÖ Gradient bien d√©fini

**Inconv√©nients :**
- ‚ùå **Probl√®me du gradient qui dispara√Æt** (vanishing gradient) pour grandes/petites valeurs
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Co√ªteux en calcul (`exp()`)

**Impl√©mentation Rust :**
```rust
fn sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp()))
}

fn sigmoid_derivative(x: &Array1<f64>) -> Array1<f64> {
    x * &(1.0 - x)
}
```

---

### ReLU (Rectified Linear Unit)
**Formule :** $f(x) = \max(0, x)$

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `[0, +‚àû)`
- Lin√©aire pour valeurs positives, z√©ro sinon
- **Standard moderne pour les couches cach√©es**

**Avantages :**
- ‚úÖ **Tr√®s rapide** (simple comparaison et multiplication)
- ‚úÖ Pas de gradient qui dispara√Æt pour valeurs positives
- ‚úÖ Favorise la sparsit√© (certains neurones s'√©teignent)
- ‚úÖ Convergence plus rapide que sigmoid/tanh

**Inconv√©nients :**
- ‚ùå **Probl√®me des neurones morts** : si gradient = 0, le neurone ne s'active plus jamais
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Non diff√©rentiable en x = 0 (en pratique, on prend 0 ou 1)

**Impl√©mentation Rust :**
```rust
fn relu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(0.0))
}

fn relu_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 })
}
```

---

### Leaky ReLU
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 0.01$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-‚àû, +‚àû)`
- Petite pente pour valeurs n√©gatives

**Avantages :**
- ‚úÖ R√©sout le probl√®me des neurones morts de ReLU
- ‚úÖ Rapide comme ReLU
- ‚úÖ Garde un gradient pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå R√©sultats incoh√©rents selon les t√¢ches
- ‚ùå N√©cessite un hyperparam√®tre (alpha)

**Impl√©mentation Rust :**
```rust
fn leaky_relu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * x })
}

fn leaky_relu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha })
}
```

---

### Tanh (Tangente Hyperbolique)
**Formule :** $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**D√©riv√©e :** $f'(x) = 1 - f(x)^2$

**Propri√©t√©s :**
- Sortie : `[-1, 1]`
- **Centr√©e sur z√©ro** (contrairement √† sigmoid)
- Version "√©tendue" de sigmoid

**Avantages :**
- ‚úÖ Sortie centr√©e ‚Üí convergence plus rapide que sigmoid
- ‚úÖ Gradient plus fort que sigmoid
- ‚úÖ Bon pour les couches cach√©es

**Inconv√©nients :**
- ‚ùå Probl√®me du gradient qui dispara√Æt (moins que sigmoid)
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.tanh())
}

fn tanh_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 - x.powi(2))
}
```

---

### ELU (Exponential Linear Unit)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 1.0$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ f(x) + \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-Œ±, +‚àû)`
- Lisse partout

**Avantages :**
- ‚úÖ Moyenne des activations proche de z√©ro
- ‚úÖ Pas de neurones morts
- ‚úÖ Gradient non-nul partout

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp()`)
- ‚ùå L√©g√®rement plus lent que ReLU

**Impl√©mentation Rust :**
```rust
fn elu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * (x.exp() - 1.0) })
}

fn elu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha * x.exp() })
}
```

---

### Softmax (pour classification multi-classes)
**Formule :** $f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$

**Propri√©t√©s :**
- Sortie : `[0, 1]` pour chaque neurone, somme = 1
- Convertit logits en probabilit√©s
- **Uniquement pour la couche de sortie**

**Avantages :**
- ‚úÖ Interpr√©tation probabiliste claire
- ‚úÖ Standard pour classification multi-classes

**Impl√©mentation Rust :**
```rust
fn softmax(x: &Array1<f64>) -> Array1<f64> {
    let max = x.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    let exp_x = x.mapv(|x| (x - max).exp());
    let sum = exp_x.sum();
    exp_x / sum
}
```

---

## Fonctions d'Activation Avanc√©es

### PReLU (Parametric ReLU)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ o√π $\alpha$ est **appris** pendant l'entra√Ænement

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Avantages :**
- ‚úÖ Alpha adaptatif par neurone
- ‚úÖ Plus flexible que Leaky ReLU

**Inconv√©nients :**
- ‚ùå Plus de param√®tres √† entra√Æner
- ‚ùå Risque de surapprentissage

**Impl√©mentation Rust :**
```rust
fn prelu(x: &Array1<f64>, alpha: &Array1<f64>) -> Array1<f64> {
    x.iter().zip(alpha.iter())
        .map(|(&x, &a)| if x > 0.0 { x } else { a * x })
        .collect()
}
```

---

### GELU (Gaussian Error Linear Unit)
**Formule :** $f(x) = x \cdot \Phi(x)$ o√π $\Phi$ est la fonction de distribution cumulative gaussienne

**Approximation :** $f(x) \approx 0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$

**Propri√©t√©s :**
- Lisse et non-monotone
- **Utilis√© dans BERT, GPT**

**Avantages :**
- ‚úÖ Performance SOTA sur transformers
- ‚úÖ Lisse partout
- ‚úÖ Probabilistiquement motiv√©

**Inconv√©nients :**
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn gelu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| {
        0.5 * x * (1.0 + ((2.0 / std::f64::consts::PI).sqrt() 
            * (x + 0.044715 * x.powi(3))).tanh())
    })
}
```

---

### Swish / SiLU (Sigmoid Linear Unit)
**Formule :** $f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) + \sigma(x)(1 - f(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **D√©couvert par Google via recherche automatique**

**Avantages :**
- ‚úÖ Meilleure performance que ReLU sur certaines t√¢ches
- ‚úÖ Lisse partout

**Inconv√©nients :**
- ‚ùå Plus co√ªteux que ReLU

**Impl√©mentation Rust :**
```rust
fn swish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + (-x).exp()))
}

fn swish_derivative(x: &Array1<f64>) -> Array1<f64> {
    let sigmoid = x.mapv(|x| 1.0 / (1.0 + (-x).exp()));
    let swish = x * &sigmoid;
    &swish + &sigmoid * &(1.0 - &swish)
}
```

---

### Mish
**Formule :** $f(x) = x \cdot \tanh(\ln(1 + e^x)) = x \cdot \tanh(\text{softplus}(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **Alternatives r√©cente √† Swish**

**Avantages :**
- ‚úÖ Meilleure r√©gularisation que ReLU/Swish
- ‚úÖ Gradient non-nul pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå Tr√®s co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn mish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x * ((1.0 + x.exp()).ln()).tanh())
}
```

---

### SELU (Scaled ELU)
**Formule :** $f(x) = \lambda \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$

**Constantes :** $\lambda \approx 1.0507$, $\alpha \approx 1.6733$

**Propri√©t√©s :**
- Auto-normalisant (pr√©serve moyenne=0, variance=1)
- **Con√ßu pour FeedForward Networks**

**Avantages :**
- ‚úÖ Pas besoin de Batch Normalization
- ‚úÖ Convergence plus rapide

**Inconv√©nients :**
- ‚ùå Sensible √† l'initialisation (utiliser LeCun)
- ‚ùå Fonctionne mal avec Dropout

**Impl√©mentation Rust :**
```rust
fn selu(x: &Array1<f64>) -> Array1<f64> {
    let lambda = 1.0507;
    let alpha = 1.6733;
    x.mapv(|x| {
        lambda * if x > 0.0 { x } else { alpha * (x.exp() - 1.0) }
    })
}
```

---

### Softplus
**Formule :** $f(x) = \ln(1 + e^x)$

**D√©riv√©e :** $f'(x) = \frac{1}{1 + e^{-x}} = \sigma(x)$ (sigmoid!)

**Propri√©t√©s :**
- Version lisse de ReLU
- Toujours positif

**Avantages :**
- ‚úÖ Diff√©rentiable partout
- ‚úÖ Pas de neurones morts

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp`, `log`)
- ‚ùå Gradient qui dispara√Æt pour grandes valeurs n√©gatives

**Impl√©mentation Rust :**
```rust
fn softplus(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (1.0 + x.exp()).ln())
}

fn softplus_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp())) // sigmoid
}
```

---

### Softsign
**Formule :** $f(x) = \frac{x}{1 + |x|}$

**D√©riv√©e :** $f'(x) = \frac{1}{(1 + |x|)^2}$

**Propri√©t√©s :**
- Sortie : `(-1, 1)`
- Alternative √† tanh

**Avantages :**
- ‚úÖ Plus rapide que tanh (pas d'exponentielle)
- ‚úÖ Gradient d√©cro√Æt plus lentement

**Inconv√©nients :**
- ‚ùå Rarement utilis√© en pratique

**Impl√©mentation Rust :**
```rust
fn softsign(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + x.abs()))
}

fn softsign_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + x.abs()).powi(2))
}
```

---

### Hard Sigmoid
**Formule :** $f(x) = \max(0, \min(1, 0.2x + 0.5))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de sigmoid
- Tr√®s rapide

**Avantages :**
- ‚úÖ Calcul extr√™mement rapide (pas d'exponentielle)
- ‚úÖ Utile pour les appareils embarqu√©s

**Impl√©mentation Rust :**
```rust
fn hard_sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (0.2 * x + 0.5).max(0.0).min(1.0))
}
```

---

### Hard Tanh
**Formule :** $f(x) = \max(-1, \min(1, x))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de tanh
- Sortie : `[-1, 1]`

**Avantages :**
- ‚úÖ Tr√®s rapide

**Impl√©mentation Rust :**
```rust
fn hard_tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(-1.0).min(1.0))
}
```

---

## Tableau Comparatif Complet

| **Fonction** | **Plage** | **Vitesse** | **Usage Principal** | **Depuis** |
|--------------|-----------|-------------|---------------------|------------|
| Sigmoid | [0, 1] | Lent | Sortie binaire | Classique |
| Tanh | [-1, 1] | Lent | Couches cach√©es | Classique |
| ReLU | [0, ‚àû) | **Tr√®s rapide** | Couches cach√©es (d√©faut) | 2010 |
| Leaky ReLU | (-‚àû, ‚àû) | **Tr√®s rapide** | Fix neurones morts | 2013 |
| PReLU | (-‚àû, ‚àû) | Rapide | Am√©lioration LeakyReLU | 2015 |
| ELU | (-Œ±, ‚àû) | Moyen | R√©seaux profonds | 2015 |
| SELU | (-ŒªŒ±, ‚àû) | Moyen | FeedForward (sans BN) | 2017 |
| Swish/SiLU | (-‚àû, ‚àû) | Moyen | Alternative ReLU | 2017 |
| GELU | (-‚àû, ‚àû) | Lent | **Transformers (GPT, BERT)** | 2016 |
| Mish | (-‚àû, ‚àû) | Lent | Vision profonde | 2019 |
| Softmax | [0, 1] (somme=1) | Moyen | Sortie multi-classe | Classique |
| Softplus | (0, ‚àû) | Lent | ReLU lisse | Classique |
| Hard Sigmoid | [0, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |
| Hard Tanh | [-1, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |

---

## Guide de S√©lection

### Par Cas d'Usage

| **Cas d'Usage** | **Fonction Recommand√©e** | **Raison** |
|-----------------|--------------------------|------------|
| **Couches cach√©es (d√©faut 2024)** | **ReLU** | Rapide, efficace, standard industriel |
| Couches cach√©es (si neurones morts) | **Leaky ReLU** ou **ELU** | Gradient toujours actif |
| Couches cach√©es (r√©seaux profonds) | **SELU** ou **ELU** | Auto-normalisation, √©vite gradient qui dispara√Æt |
| Couches cach√©es (recherche de performance) | **Swish** ou **Mish** | Performance SOTA sur certaines t√¢ches |
| **Transformers / NLP (GPT, BERT)** | **GELU** | Standard pour attention mechanisms |
| **Vision par ordinateur (CNN)** | **ReLU** ou **Mish** | Rapide pour CNN, Mish pour profonds |
| R√©seaux r√©currents (RNN, LSTM) | **Tanh** | Standard historique pour gates |
| **Sortie classification binaire** | **Sigmoid** | Sortie [0,1] = probabilit√© |
| **Sortie classification multi-classes** | **Softmax** | Distribution de probabilit√©s (somme=1) |
| **Sortie r√©gression** | **Lin√©aire** (aucune) | Valeurs continues illimit√©es |
| Sortie r√©gression (valeurs positives) | **Softplus** ou **ReLU** | Force sortie ‚â• 0 |
| **Appareils embarqu√©s / Mobile** | **Hard Sigmoid** / **Hard Tanh** | Pas d'exponentielle, ultra-rapide |
| Recherche / Exp√©rimentation | **PReLU** | Alpha adaptatif par neurone |

### Par Priorit√©

#### üèÜ **Si tu veux la meilleure performance (sans contrainte)** :
1. **Couches cach√©es** : GELU, Swish, Mish
2. **Sortie** : Softmax (multi-classe), Sigmoid (binaire)

#### ‚ö° **Si tu veux la rapidit√© (contrainte temps r√©el)** :
1. **Couches cach√©es** : ReLU, Leaky ReLU
2. **Embarqu√©** : Hard Sigmoid, Hard Tanh

#### üéØ **Si tu veux la stabilit√© (r√©seaux tr√®s profonds)** :
1. **Couches cach√©es** : SELU (avec initialisation LeCun), ELU
2. **√âviter** : Sigmoid, Tanh (gradient qui dispara√Æt)

#### üîß **Si tu d√©butes / prototype rapide** :
1. **D√©faut recommand√©** : ReLU partout sauf sortie
2. **Sortie** : Sigmoid (binaire), Softmax (multi-classe)

### Par Type de R√©seau

| **Architecture** | **Couches Cach√©es** | **Sortie** |
|------------------|---------------------|------------|
| **Feedforward simple** | ReLU | Sigmoid / Softmax |
| **Feedforward profond** | SELU, ELU | Sigmoid / Softmax |
| **CNN (Computer Vision)** | ReLU, Mish | Softmax |
| **RNN / LSTM** | Tanh | Sigmoid / Softmax |
| **Transformer** | GELU | Softmax |
| **GAN (G√©n√©rateur)** | ReLU, Leaky ReLU | Tanh |
| **GAN (Discriminateur)** | Leaky ReLU | Sigmoid |
| **Autoencoder** | ReLU | Sigmoid (binaire), Lin√©aire (continu) |
| **Reinforcement Learning** | ReLU, ELU | Lin√©aire, Softmax |

### Arbre de D√©cision

```
Quelle est ta couche ?
‚îú‚îÄ Couche de SORTIE
‚îÇ  ‚îú‚îÄ Classification binaire ? ‚Üí Sigmoid
‚îÇ  ‚îú‚îÄ Classification multi-classes ? ‚Üí Softmax
‚îÇ  ‚îú‚îÄ R√©gression (valeurs continues) ? ‚Üí Lin√©aire (aucune activation)
‚îÇ  ‚îî‚îÄ R√©gression (valeurs positives) ? ‚Üí Softplus / ReLU
‚îÇ
‚îî‚îÄ Couche CACH√âE
   ‚îú‚îÄ Contrainte de VITESSE ?
   ‚îÇ  ‚îú‚îÄ Ultra-rapide (embarqu√©) ? ‚Üí Hard Sigmoid / Hard Tanh
   ‚îÇ  ‚îî‚îÄ Rapide ‚Üí ReLU, Leaky ReLU
   ‚îÇ
   ‚îú‚îÄ Type de R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Transformer / NLP ? ‚Üí GELU
   ‚îÇ  ‚îú‚îÄ CNN profond ? ‚Üí Mish
   ‚îÇ  ‚îú‚îÄ RNN / LSTM ? ‚Üí Tanh
   ‚îÇ  ‚îî‚îÄ Feedforward ? ‚Üí Voir ci-dessous
   ‚îÇ
   ‚îú‚îÄ Profondeur du R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Peu de couches (< 5) ? ‚Üí ReLU
   ‚îÇ  ‚îú‚îÄ Profond (> 10 couches) ? ‚Üí SELU, ELU
   ‚îÇ  ‚îî‚îÄ Tr√®s profond (> 50) ? ‚Üí SELU avec LeCun init
   ‚îÇ
   ‚îú‚îÄ Probl√®me de NEURONES MORTS (gradient = 0) ?
   ‚îÇ  ‚îú‚îÄ Oui ‚Üí Leaky ReLU, PReLU, ELU
   ‚îÇ  ‚îî‚îÄ Non ‚Üí ReLU
   ‚îÇ
   ‚îî‚îÄ Recherche de PERFORMANCE maximale ?
      ‚îú‚îÄ Oui (GPU puissant) ‚Üí Swish, Mish, GELU
      ‚îî‚îÄ Non ‚Üí ReLU (d√©faut)
```

### Recommandations par Ann√©e

| **√âpoque** | **Standard** | **Contexte** |
|------------|--------------|--------------|
| 1990-2010 | Sigmoid, Tanh | R√©seaux peu profonds |
| 2010-2015 | ReLU | R√©volution deep learning |
| 2015-2017 | Leaky ReLU, ELU, PReLU | Am√©lioration ReLU |
| 2017-2019 | Swish, SELU | Auto-recherche Google |
| 2019-2024 | **GELU** (transformers), **Mish** (vision) | SOTA actuel |
| 2024+ | **GELU** (d√©faut NLP), **ReLU** (d√©faut vision) | Standard industriel |

### Combinaisons √âprouv√©es

**Classification d'images (CNN) :**
```rust
// Couches conv : ReLU ou Mish
// Couches fully-connected : ReLU
// Sortie : Softmax
```

**Mod√®le de langage (Transformer) :**
```rust
// Attention + FFN : GELU
// Sortie : Softmax
```

**R√©seau profond (> 20 couches) :**
```rust
// Toutes couches cach√©es : SELU
// Initialisation : LeCun normal
// PAS de Batch Normalization
// Sortie : Sigmoid / Softmax
```

**Prototype rapide :**
```rust
// Couches cach√©es : ReLU
// Sortie : Sigmoid (binaire) ou Softmax (multi-classe)
```

### 3. Learning Rate (Taux d'apprentissage)

**Vitesse d'apprentissage** : √† quel point modifier les poids √† chaque √©tape.

- Actuellement : `0.1`
- **Trop petit** ‚Üí apprentissage lent
- **Trop grand** ‚Üí instabilit√©, ne converge pas
- **Typique** : `0.001` √† `0.1`

---

## Fonctions de Perte (Loss Functions)

### Concept de Base

La **loss function** (fonction de perte/co√ªt) mesure **√† quel point le r√©seau se trompe** dans ses pr√©dictions.

**Objectif :** Minimiser l'erreur entre la pr√©diction et la valeur r√©elle.

```
Loss = Diff√©rence(Pr√©diction, Valeur_R√©elle)
```

Plus la loss est **petite** ‚Üí meilleure pr√©diction  
Plus la loss est **grande** ‚Üí pire pr√©diction

### Cycle d'apprentissage

```
1. Forward pass ‚Üí Pr√©diction
2. Calcul de la Loss ‚Üí Mesurer l'erreur
3. Backpropagation ‚Üí Calculer les gradients
4. Update des poids ‚Üí R√©duire la Loss
```

---

### 1. MSE (Mean Squared Error)

**Formule :** $\text{MSE} = \frac{1}{n}\sum(y - \hat{y})^2$

**Usage :** R√©gression (pr√©dire des valeurs continues)

**Avantages :**
- ‚úÖ P√©nalise fortement les grandes erreurs
- ‚úÖ Diff√©rentiable partout
- ‚úÖ Interpr√©tation intuitive

**Inconv√©nients :**
- ‚ùå Pas optimal pour classification
- ‚ùå Gradient qui dispara√Æt avec Sigmoid

**Exemple :**
```
Pr√©diction: 2.5, R√©el: 3.0
Loss = (3.0 - 2.5)¬≤ = 0.25
```

**Impl√©mentation Rust :**
```rust
fn mse(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let diff = predictions - targets;
    (&diff * &diff).sum() / predictions.len() as f64
}
```

---

### 2. MAE (Mean Absolute Error)

**Formule :** $\text{MAE} = \frac{1}{n}\sum|y - \hat{y}|$

**Usage :** R√©gression (moins sensible aux outliers)

**Avantages :**
- ‚úÖ Robuste aux outliers
- ‚úÖ Interpr√©tation intuitive
- ‚úÖ Toutes les erreurs trait√©es lin√©airement

**Inconv√©nients :**
- ‚ùå Gradients constants (convergence plus lente)
- ‚ùå Non diff√©rentiable en z√©ro

**Exemple :**
```
Pr√©diction: 2.5, R√©el: 3.0
Loss = |3.0 - 2.5| = 0.5
```

**Impl√©mentation Rust :**
```rust
fn mae(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    (predictions - targets).mapv(|x| x.abs()).sum() / predictions.len() as f64
}
```

---

### 3. Binary Cross-Entropy (Log Loss)

**Formule :** $\text{BCE} = -\frac{1}{n}\sum[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$

**Usage :** Classification binaire (avec Sigmoid)

**Avantages :**
- ‚úÖ Interpr√©tation probabiliste
- ‚úÖ Gradient plus stable que MSE pour classification
- ‚úÖ Convergence plus rapide
- ‚úÖ Standard pour classification binaire

**Inconv√©nients :**
- ‚ùå N√©cessite pr√©dictions dans [0, 1]
- ‚ùå Instable si pr√©diction = 0 ou 1 (log(0))

**Exemple :**
```
Pr√©diction: 0.9, R√©el: 1 (classe positive)
Loss = -[1√ólog(0.9) + 0√ólog(0.1)] = 0.105  // Bonne pr√©diction

Pr√©diction: 0.1, R√©el: 1 (classe positive)
Loss = -[1√ólog(0.1) + 0√ólog(0.9)] = 2.303  // Grosse erreur!
```

**Impl√©mentation Rust :**
```rust
fn binary_cross_entropy(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let epsilon = 1e-15; // √âviter log(0)
    let mut sum = 0.0;
    for (p, t) in predictions.iter().zip(targets.iter()) {
        let p_clamped = p.max(epsilon).min(1.0 - epsilon);
        sum += -(t * p_clamped.ln() + (1.0 - t) * (1.0 - p_clamped).ln());
    }
    sum / predictions.len() as f64
}
```

---

### 4. Categorical Cross-Entropy

**Formule :** $\text{CCE} = -\sum y_i \log(\hat{y}_i)$

**Usage :** Classification multi-classes (avec Softmax)

**Avantages :**
- ‚úÖ Standard pour multi-classes
- ‚úÖ Interpr√©tation probabiliste claire
- ‚úÖ Gradient bien adapt√© avec Softmax

**Exemple :**
```
Classes: [Chat, Chien, Oiseau]
R√©el:    [1,    0,     0]      // C'est un chat
Pr√©dit:  [0.7,  0.2,   0.1]
Loss = -(1√ólog(0.7) + 0√ólog(0.2) + 0√ólog(0.1)) = 0.357
```

**Impl√©mentation Rust :**
```rust
fn categorical_cross_entropy(predictions: &Array1<f64>, targets: &Array1<f64>) -> f64 {
    let epsilon = 1e-15;
    -targets.iter()
        .zip(predictions.iter())
        .map(|(t, p)| t * (p.max(epsilon)).ln())
        .sum::<f64>()
}
```

---

### 5. Huber Loss

**Formule :** 
$$\text{Huber} = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \leq \delta \\ \delta(|y - \hat{y}| - \frac{1}{2}\delta) & \text{sinon} \end{cases}$$

**Usage :** R√©gression robuste aux outliers

**Avantages :**
- ‚úÖ Combine MSE (petites erreurs) et MAE (grandes erreurs)
- ‚úÖ Moins sensible aux outliers que MSE
- ‚úÖ Diff√©rentiable partout

**Param√®tre :** $\delta$ (typiquement 1.0) = seuil entre comportement MSE et MAE

**Impl√©mentation Rust :**
```rust
fn huber_loss(predictions: &Array1<f64>, targets: &Array1<f64>, delta: f64) -> f64 {
    let diff = predictions - targets;
    let mut sum = 0.0;
    for &d in diff.iter() {
        let abs_d = d.abs();
        if abs_d <= delta {
            sum += 0.5 * d * d;  // MSE pour petites erreurs
        } else {
            sum += delta * (abs_d - 0.5 * delta);  // MAE pour grandes erreurs
        }
    }
    sum / predictions.len() as f64
}
```

---

### Guide de S√©lection des Loss Functions

| **T√¢che** | **Activation Sortie** | **Loss Function Recommand√©e** | **Pourquoi** |
|-----------|----------------------|-------------------------------|--------------|
| R√©gression | Linear | **MSE** | Standard, p√©nalise grandes erreurs |
| R√©gression robuste | Linear | **MAE** ou **Huber** | R√©siste aux outliers |
| Classification binaire | Sigmoid | **Binary Cross-Entropy** | Interpr√©tation probabiliste |
| Classification multi-classes | Softmax | **Categorical Cross-Entropy** | Standard multi-classes |
| D√©tection d'objets | Variable | IoU Loss, Focal Loss | Adapt√© aux bo√Ætes |
| Segmentation | Softmax | Dice Loss, Focal Loss | Adapt√© aux pixels |

---

### Comparaison MSE vs Binary Cross-Entropy (XOR)

**Probl√®me :** Classification binaire avec Sigmoid

#### MSE pour classification
- ‚ùå Gradient qui dispara√Æt quand proche de 0 ou 1
- ‚ùå Pas d'interpr√©tation probabiliste
- ‚ùå Convergence plus lente

#### Binary Cross-Entropy pour classification
- ‚úÖ Gradient plus stable
- ‚úÖ Converge plus vite
- ‚úÖ Interpr√©tation comme probabilit√©
- ‚úÖ Meilleur choix pour XOR

**R√©sultats typiques (50k epochs, lr=0.5) :**
```
MSE:  Final loss: 0.0000 ‚úì
BCE:  Final loss: 0.0000 ‚úì
MAE:  Final loss: 0.2500 (n√©cessite lr=0.2, epochs=150k)
```

---

### Visualisation de la Convergence

```
Haute Loss ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
                    ‚îÉ    D√©but
                    ‚îÉ      ‚Üì
                    ‚îÉ      ‚Ä¢
                    ‚îÉ     ‚ï±
                    ‚îÉ    ‚ï±
                    ‚îÉ   ‚ï±     Training
                    ‚îÉ  ‚ï±      ‚Üì
                    ‚îÉ ‚ï±
Basse Loss ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÉ‚ï±________‚Ä¢ Convergence
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
                         Epochs
```

**Objectif du training :** Descendre cette courbe le plus vite possible en ajustant les poids.

---

## Documentation Recommand√©e

1. **[3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)** (YouTube) : visualisations excellentes
2. **[The Rust ML Book](https://rust-ml.github.io/book/)** : apprentissage automatique en Rust
3. **[ndarray docs](https://docs.rs/ndarray/latest/ndarray/)** : documentation de la biblioth√®que
4. **Neural Networks from Scratch** (livre) : explications math√©matiques d√©taill√©es
5. **[ML Cheatsheet - Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)** : r√©f√©rence compl√®te

## Exp√©rimentation

### Architecture
```rust
Network::new(2, 5, 1)   // 5 neurones cach√©s
Network::new(2, 10, 1)  // 10 neurones cach√©s
```

### Learning Rate
```rust
let learning_rate = 0.01;  // Plus lent
let learning_rate = 0.5;   // Plus rapide
let learning_rate = 1.0;   // Tr√®s rapide (attention √† la stabilit√©)
```

### Fonction d'Activation et Loss
```rust
// Classification binaire (XOR)
Network::new(2, 5, 1, 
    Activation::Tanh,           // Couche cach√©e
    Activation::Sigmoid,        // Sortie
    LossFunction::BinaryCrossEntropy)

// R√©gression
Network::new(4, 10, 1,
    Activation::ReLU,           // Couche cach√©e
    Activation::Linear,         // Sortie
    LossFunction::MSE)

// Multi-classes
Network::new(784, 128, 10,
    Activation::GELU,           // Couche cach√©e
    Activation::Softmax,        // Sortie
    LossFunction::CategoricalCrossEntropy)
```
