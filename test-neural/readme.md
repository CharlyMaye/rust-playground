# R√©seau de Neurones en Rust

## Concepts Cl√©s

### 1. Architecture (Couches/Neurones)

La **structure** de ton r√©seau : nombre de couches et nombre de neurones par couche.

- Dans ton code : `Network::new(2, 3, 1)` = 2 entr√©es ‚Üí 3 neurones cach√©s ‚Üí 1 sortie
- Plus de neurones/couches = plus de capacit√© d'apprentissage, mais risque de **surapprentissage**

### 2. Fonctions d'Activation (sigmoid, ReLU, tanh...)

Fonction qui **transforme** la sortie d'un neurone.

**Actuellement utilis√©e : Sigmoid**
- Formule : `1 / (1 + e^-x)`
- Sortie : entre `[0, 1]`

**Alternatives :**
- **ReLU** : `max(0, x)` ‚Üí plus rapide, standard moderne
- **tanh** : `tanh(x)` ‚Üí sortie entre `[-1, 1]`
- **Leaky ReLU**, **ELU**, etc.

---

## Fonctions d'Activation D√©taill√©es

### Sigmoid (Logistic)
**Formule :** $f(x) = \frac{1}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) \cdot (1 - f(x))$

**Propri√©t√©s :**
- Sortie : `[0, 1]`
- Lisse et diff√©rentiable partout
- Interpr√©table comme une probabilit√©

**Avantages :**
- ‚úÖ Sortie normalis√©e (bonne pour la couche de sortie en classification binaire)
- ‚úÖ Gradient bien d√©fini

**Inconv√©nients :**
- ‚ùå **Probl√®me du gradient qui dispara√Æt** (vanishing gradient) pour grandes/petites valeurs
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Co√ªteux en calcul (`exp()`)

**Impl√©mentation Rust :**
```rust
fn sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp()))
}

fn sigmoid_derivative(x: &Array1<f64>) -> Array1<f64> {
    x * &(1.0 - x)
}
```

---

### ReLU (Rectified Linear Unit)
**Formule :** $f(x) = \max(0, x)$

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `[0, +‚àû)`
- Lin√©aire pour valeurs positives, z√©ro sinon
- **Standard moderne pour les couches cach√©es**

**Avantages :**
- ‚úÖ **Tr√®s rapide** (simple comparaison et multiplication)
- ‚úÖ Pas de gradient qui dispara√Æt pour valeurs positives
- ‚úÖ Favorise la sparsit√© (certains neurones s'√©teignent)
- ‚úÖ Convergence plus rapide que sigmoid/tanh

**Inconv√©nients :**
- ‚ùå **Probl√®me des neurones morts** : si gradient = 0, le neurone ne s'active plus jamais
- ‚ùå Sortie non centr√©e sur z√©ro
- ‚ùå Non diff√©rentiable en x = 0 (en pratique, on prend 0 ou 1)

**Impl√©mentation Rust :**
```rust
fn relu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(0.0))
}

fn relu_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 })
}
```

---

### Leaky ReLU
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 0.01$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-‚àû, +‚àû)`
- Petite pente pour valeurs n√©gatives

**Avantages :**
- ‚úÖ R√©sout le probl√®me des neurones morts de ReLU
- ‚úÖ Rapide comme ReLU
- ‚úÖ Garde un gradient pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå R√©sultats incoh√©rents selon les t√¢ches
- ‚ùå N√©cessite un hyperparam√®tre (alpha)

**Impl√©mentation Rust :**
```rust
fn leaky_relu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * x })
}

fn leaky_relu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha })
}
```

---

### Tanh (Tangente Hyperbolique)
**Formule :** $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**D√©riv√©e :** $f'(x) = 1 - f(x)^2$

**Propri√©t√©s :**
- Sortie : `[-1, 1]`
- **Centr√©e sur z√©ro** (contrairement √† sigmoid)
- Version "√©tendue" de sigmoid

**Avantages :**
- ‚úÖ Sortie centr√©e ‚Üí convergence plus rapide que sigmoid
- ‚úÖ Gradient plus fort que sigmoid
- ‚úÖ Bon pour les couches cach√©es

**Inconv√©nients :**
- ‚ùå Probl√®me du gradient qui dispara√Æt (moins que sigmoid)
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.tanh())
}

fn tanh_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 - x.powi(2))
}
```

---

### ELU (Exponential Linear Unit)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$ (typiquement $\alpha = 1.0$)

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ f(x) + \alpha & \text{si } x \leq 0 \end{cases}$

**Propri√©t√©s :**
- Sortie : `(-Œ±, +‚àû)`
- Lisse partout

**Avantages :**
- ‚úÖ Moyenne des activations proche de z√©ro
- ‚úÖ Pas de neurones morts
- ‚úÖ Gradient non-nul partout

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp()`)
- ‚ùå L√©g√®rement plus lent que ReLU

**Impl√©mentation Rust :**
```rust
fn elu(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { x } else { alpha * (x.exp() - 1.0) })
}

fn elu_derivative(x: &Array1<f64>, alpha: f64) -> Array1<f64> {
    x.mapv(|x| if x > 0.0 { 1.0 } else { alpha * x.exp() })
}
```

---

### Softmax (pour classification multi-classes)
**Formule :** $f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$

**Propri√©t√©s :**
- Sortie : `[0, 1]` pour chaque neurone, somme = 1
- Convertit logits en probabilit√©s
- **Uniquement pour la couche de sortie**

**Avantages :**
- ‚úÖ Interpr√©tation probabiliste claire
- ‚úÖ Standard pour classification multi-classes

**Impl√©mentation Rust :**
```rust
fn softmax(x: &Array1<f64>) -> Array1<f64> {
    let max = x.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    let exp_x = x.mapv(|x| (x - max).exp());
    let sum = exp_x.sum();
    exp_x / sum
}
```

---

## Fonctions d'Activation Avanc√©es

### PReLU (Parametric ReLU)
**Formule :** $f(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$ o√π $\alpha$ est **appris** pendant l'entra√Ænement

**D√©riv√©e :** $f'(x) = \begin{cases} 1 & \text{si } x > 0 \\ \alpha & \text{si } x \leq 0 \end{cases}$

**Avantages :**
- ‚úÖ Alpha adaptatif par neurone
- ‚úÖ Plus flexible que Leaky ReLU

**Inconv√©nients :**
- ‚ùå Plus de param√®tres √† entra√Æner
- ‚ùå Risque de surapprentissage

**Impl√©mentation Rust :**
```rust
fn prelu(x: &Array1<f64>, alpha: &Array1<f64>) -> Array1<f64> {
    x.iter().zip(alpha.iter())
        .map(|(&x, &a)| if x > 0.0 { x } else { a * x })
        .collect()
}
```

---

### GELU (Gaussian Error Linear Unit)
**Formule :** $f(x) = x \cdot \Phi(x)$ o√π $\Phi$ est la fonction de distribution cumulative gaussienne

**Approximation :** $f(x) \approx 0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$

**Propri√©t√©s :**
- Lisse et non-monotone
- **Utilis√© dans BERT, GPT**

**Avantages :**
- ‚úÖ Performance SOTA sur transformers
- ‚úÖ Lisse partout
- ‚úÖ Probabilistiquement motiv√©

**Inconv√©nients :**
- ‚ùå Co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn gelu(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| {
        0.5 * x * (1.0 + ((2.0 / std::f64::consts::PI).sqrt() 
            * (x + 0.044715 * x.powi(3))).tanh())
    })
}
```

---

### Swish / SiLU (Sigmoid Linear Unit)
**Formule :** $f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$

**D√©riv√©e :** $f'(x) = f(x) + \sigma(x)(1 - f(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **D√©couvert par Google via recherche automatique**

**Avantages :**
- ‚úÖ Meilleure performance que ReLU sur certaines t√¢ches
- ‚úÖ Lisse partout

**Inconv√©nients :**
- ‚ùå Plus co√ªteux que ReLU

**Impl√©mentation Rust :**
```rust
fn swish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + (-x).exp()))
}

fn swish_derivative(x: &Array1<f64>) -> Array1<f64> {
    let sigmoid = x.mapv(|x| 1.0 / (1.0 + (-x).exp()));
    let swish = x * &sigmoid;
    &swish + &sigmoid * &(1.0 - &swish)
}
```

---

### Mish
**Formule :** $f(x) = x \cdot \tanh(\ln(1 + e^x)) = x \cdot \tanh(\text{softplus}(x))$

**Propri√©t√©s :**
- Lisse, non-monotone
- **Alternatives r√©cente √† Swish**

**Avantages :**
- ‚úÖ Meilleure r√©gularisation que ReLU/Swish
- ‚úÖ Gradient non-nul pour valeurs n√©gatives

**Inconv√©nients :**
- ‚ùå Tr√®s co√ªteux en calcul

**Impl√©mentation Rust :**
```rust
fn mish(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x * ((1.0 + x.exp()).ln()).tanh())
}
```

---

### SELU (Scaled ELU)
**Formule :** $f(x) = \lambda \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$

**Constantes :** $\lambda \approx 1.0507$, $\alpha \approx 1.6733$

**Propri√©t√©s :**
- Auto-normalisant (pr√©serve moyenne=0, variance=1)
- **Con√ßu pour FeedForward Networks**

**Avantages :**
- ‚úÖ Pas besoin de Batch Normalization
- ‚úÖ Convergence plus rapide

**Inconv√©nients :**
- ‚ùå Sensible √† l'initialisation (utiliser LeCun)
- ‚ùå Fonctionne mal avec Dropout

**Impl√©mentation Rust :**
```rust
fn selu(x: &Array1<f64>) -> Array1<f64> {
    let lambda = 1.0507;
    let alpha = 1.6733;
    x.mapv(|x| {
        lambda * if x > 0.0 { x } else { alpha * (x.exp() - 1.0) }
    })
}
```

---

### Softplus
**Formule :** $f(x) = \ln(1 + e^x)$

**D√©riv√©e :** $f'(x) = \frac{1}{1 + e^{-x}} = \sigma(x)$ (sigmoid!)

**Propri√©t√©s :**
- Version lisse de ReLU
- Toujours positif

**Avantages :**
- ‚úÖ Diff√©rentiable partout
- ‚úÖ Pas de neurones morts

**Inconv√©nients :**
- ‚ùå Co√ªteux (`exp`, `log`)
- ‚ùå Gradient qui dispara√Æt pour grandes valeurs n√©gatives

**Impl√©mentation Rust :**
```rust
fn softplus(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (1.0 + x.exp()).ln())
}

fn softplus_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + (-x).exp())) // sigmoid
}
```

---

### Softsign
**Formule :** $f(x) = \frac{x}{1 + |x|}$

**D√©riv√©e :** $f'(x) = \frac{1}{(1 + |x|)^2}$

**Propri√©t√©s :**
- Sortie : `(-1, 1)`
- Alternative √† tanh

**Avantages :**
- ‚úÖ Plus rapide que tanh (pas d'exponentielle)
- ‚úÖ Gradient d√©cro√Æt plus lentement

**Inconv√©nients :**
- ‚ùå Rarement utilis√© en pratique

**Impl√©mentation Rust :**
```rust
fn softsign(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x / (1.0 + x.abs()))
}

fn softsign_derivative(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| 1.0 / (1.0 + x.abs()).powi(2))
}
```

---

### Hard Sigmoid
**Formule :** $f(x) = \max(0, \min(1, 0.2x + 0.5))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de sigmoid
- Tr√®s rapide

**Avantages :**
- ‚úÖ Calcul extr√™mement rapide (pas d'exponentielle)
- ‚úÖ Utile pour les appareils embarqu√©s

**Impl√©mentation Rust :**
```rust
fn hard_sigmoid(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| (0.2 * x + 0.5).max(0.0).min(1.0))
}
```

---

### Hard Tanh
**Formule :** $f(x) = \max(-1, \min(1, x))$

**Propri√©t√©s :**
- Approximation lin√©aire par morceaux de tanh
- Sortie : `[-1, 1]`

**Avantages :**
- ‚úÖ Tr√®s rapide

**Impl√©mentation Rust :**
```rust
fn hard_tanh(x: &Array1<f64>) -> Array1<f64> {
    x.mapv(|x| x.max(-1.0).min(1.0))
}
```

---

## Tableau Comparatif Complet

| **Fonction** | **Plage** | **Vitesse** | **Usage Principal** | **Depuis** |
|--------------|-----------|-------------|---------------------|------------|
| Sigmoid | [0, 1] | Lent | Sortie binaire | Classique |
| Tanh | [-1, 1] | Lent | Couches cach√©es | Classique |
| ReLU | [0, ‚àû) | **Tr√®s rapide** | Couches cach√©es (d√©faut) | 2010 |
| Leaky ReLU | (-‚àû, ‚àû) | **Tr√®s rapide** | Fix neurones morts | 2013 |
| PReLU | (-‚àû, ‚àû) | Rapide | Am√©lioration LeakyReLU | 2015 |
| ELU | (-Œ±, ‚àû) | Moyen | R√©seaux profonds | 2015 |
| SELU | (-ŒªŒ±, ‚àû) | Moyen | FeedForward (sans BN) | 2017 |
| Swish/SiLU | (-‚àû, ‚àû) | Moyen | Alternative ReLU | 2017 |
| GELU | (-‚àû, ‚àû) | Lent | **Transformers (GPT, BERT)** | 2016 |
| Mish | (-‚àû, ‚àû) | Lent | Vision profonde | 2019 |
| Softmax | [0, 1] (somme=1) | Moyen | Sortie multi-classe | Classique |
| Softplus | (0, ‚àû) | Lent | ReLU lisse | Classique |
| Hard Sigmoid | [0, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |
| Hard Tanh | [-1, 1] | **Tr√®s rapide** | Embarqu√© | Mobile |

---

## Guide de S√©lection

### Par Cas d'Usage

| **Cas d'Usage** | **Fonction Recommand√©e** | **Raison** |
|-----------------|--------------------------|------------|
| **Couches cach√©es (d√©faut 2024)** | **ReLU** | Rapide, efficace, standard industriel |
| Couches cach√©es (si neurones morts) | **Leaky ReLU** ou **ELU** | Gradient toujours actif |
| Couches cach√©es (r√©seaux profonds) | **SELU** ou **ELU** | Auto-normalisation, √©vite gradient qui dispara√Æt |
| Couches cach√©es (recherche de performance) | **Swish** ou **Mish** | Performance SOTA sur certaines t√¢ches |
| **Transformers / NLP (GPT, BERT)** | **GELU** | Standard pour attention mechanisms |
| **Vision par ordinateur (CNN)** | **ReLU** ou **Mish** | Rapide pour CNN, Mish pour profonds |
| R√©seaux r√©currents (RNN, LSTM) | **Tanh** | Standard historique pour gates |
| **Sortie classification binaire** | **Sigmoid** | Sortie [0,1] = probabilit√© |
| **Sortie classification multi-classes** | **Softmax** | Distribution de probabilit√©s (somme=1) |
| **Sortie r√©gression** | **Lin√©aire** (aucune) | Valeurs continues illimit√©es |
| Sortie r√©gression (valeurs positives) | **Softplus** ou **ReLU** | Force sortie ‚â• 0 |
| **Appareils embarqu√©s / Mobile** | **Hard Sigmoid** / **Hard Tanh** | Pas d'exponentielle, ultra-rapide |
| Recherche / Exp√©rimentation | **PReLU** | Alpha adaptatif par neurone |

### Par Priorit√©

#### üèÜ **Si tu veux la meilleure performance (sans contrainte)** :
1. **Couches cach√©es** : GELU, Swish, Mish
2. **Sortie** : Softmax (multi-classe), Sigmoid (binaire)

#### ‚ö° **Si tu veux la rapidit√© (contrainte temps r√©el)** :
1. **Couches cach√©es** : ReLU, Leaky ReLU
2. **Embarqu√©** : Hard Sigmoid, Hard Tanh

#### üéØ **Si tu veux la stabilit√© (r√©seaux tr√®s profonds)** :
1. **Couches cach√©es** : SELU (avec initialisation LeCun), ELU
2. **√âviter** : Sigmoid, Tanh (gradient qui dispara√Æt)

#### üîß **Si tu d√©butes / prototype rapide** :
1. **D√©faut recommand√©** : ReLU partout sauf sortie
2. **Sortie** : Sigmoid (binaire), Softmax (multi-classe)

### Par Type de R√©seau

| **Architecture** | **Couches Cach√©es** | **Sortie** |
|------------------|---------------------|------------|
| **Feedforward simple** | ReLU | Sigmoid / Softmax |
| **Feedforward profond** | SELU, ELU | Sigmoid / Softmax |
| **CNN (Computer Vision)** | ReLU, Mish | Softmax |
| **RNN / LSTM** | Tanh | Sigmoid / Softmax |
| **Transformer** | GELU | Softmax |
| **GAN (G√©n√©rateur)** | ReLU, Leaky ReLU | Tanh |
| **GAN (Discriminateur)** | Leaky ReLU | Sigmoid |
| **Autoencoder** | ReLU | Sigmoid (binaire), Lin√©aire (continu) |
| **Reinforcement Learning** | ReLU, ELU | Lin√©aire, Softmax |

### Arbre de D√©cision

```
Quelle est ta couche ?
‚îú‚îÄ Couche de SORTIE
‚îÇ  ‚îú‚îÄ Classification binaire ? ‚Üí Sigmoid
‚îÇ  ‚îú‚îÄ Classification multi-classes ? ‚Üí Softmax
‚îÇ  ‚îú‚îÄ R√©gression (valeurs continues) ? ‚Üí Lin√©aire (aucune activation)
‚îÇ  ‚îî‚îÄ R√©gression (valeurs positives) ? ‚Üí Softplus / ReLU
‚îÇ
‚îî‚îÄ Couche CACH√âE
   ‚îú‚îÄ Contrainte de VITESSE ?
   ‚îÇ  ‚îú‚îÄ Ultra-rapide (embarqu√©) ? ‚Üí Hard Sigmoid / Hard Tanh
   ‚îÇ  ‚îî‚îÄ Rapide ‚Üí ReLU, Leaky ReLU
   ‚îÇ
   ‚îú‚îÄ Type de R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Transformer / NLP ? ‚Üí GELU
   ‚îÇ  ‚îú‚îÄ CNN profond ? ‚Üí Mish
   ‚îÇ  ‚îú‚îÄ RNN / LSTM ? ‚Üí Tanh
   ‚îÇ  ‚îî‚îÄ Feedforward ? ‚Üí Voir ci-dessous
   ‚îÇ
   ‚îú‚îÄ Profondeur du R√âSEAU ?
   ‚îÇ  ‚îú‚îÄ Peu de couches (< 5) ? ‚Üí ReLU
   ‚îÇ  ‚îú‚îÄ Profond (> 10 couches) ? ‚Üí SELU, ELU
   ‚îÇ  ‚îî‚îÄ Tr√®s profond (> 50) ? ‚Üí SELU avec LeCun init
   ‚îÇ
   ‚îú‚îÄ Probl√®me de NEURONES MORTS (gradient = 0) ?
   ‚îÇ  ‚îú‚îÄ Oui ‚Üí Leaky ReLU, PReLU, ELU
   ‚îÇ  ‚îî‚îÄ Non ‚Üí ReLU
   ‚îÇ
   ‚îî‚îÄ Recherche de PERFORMANCE maximale ?
      ‚îú‚îÄ Oui (GPU puissant) ‚Üí Swish, Mish, GELU
      ‚îî‚îÄ Non ‚Üí ReLU (d√©faut)
```

### Recommandations par Ann√©e

| **√âpoque** | **Standard** | **Contexte** |
|------------|--------------|--------------|
| 1990-2010 | Sigmoid, Tanh | R√©seaux peu profonds |
| 2010-2015 | ReLU | R√©volution deep learning |
| 2015-2017 | Leaky ReLU, ELU, PReLU | Am√©lioration ReLU |
| 2017-2019 | Swish, SELU | Auto-recherche Google |
| 2019-2024 | **GELU** (transformers), **Mish** (vision) | SOTA actuel |
| 2024+ | **GELU** (d√©faut NLP), **ReLU** (d√©faut vision) | Standard industriel |

### Combinaisons √âprouv√©es

**Classification d'images (CNN) :**
```rust
// Couches conv : ReLU ou Mish
// Couches fully-connected : ReLU
// Sortie : Softmax
```

**Mod√®le de langage (Transformer) :**
```rust
// Attention + FFN : GELU
// Sortie : Softmax
```

**R√©seau profond (> 20 couches) :**
```rust
// Toutes couches cach√©es : SELU
// Initialisation : LeCun normal
// PAS de Batch Normalization
// Sortie : Sigmoid / Softmax
```

**Prototype rapide :**
```rust
// Couches cach√©es : ReLU
// Sortie : Sigmoid (binaire) ou Softmax (multi-classe)
```

### 3. Learning Rate (Taux d'apprentissage)

**Vitesse d'apprentissage** : √† quel point modifier les poids √† chaque √©tape.

- Actuellement : `0.1`
- **Trop petit** ‚Üí apprentissage lent
- **Trop grand** ‚Üí instabilit√©, ne converge pas
- **Typique** : `0.001` √† `0.1`

## Documentation Recommand√©e

1. **[3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)** (YouTube) : visualisations excellentes
2. **[The Rust ML Book](https://rust-ml.github.io/book/)** : apprentissage automatique en Rust
3. **[ndarray docs](https://docs.rs/ndarray/latest/ndarray/)** : documentation de la biblioth√®que
4. **Neural Networks from Scratch** (livre) : explications math√©matiques d√©taill√©es

## Exp√©rimentation

### Architecture
```rust
Network::new(2, 5, 1)   // 5 neurones cach√©s
Network::new(2, 10, 1)  // 10 neurones cach√©s
```

### Learning Rate
```rust
let learning_rate = 0.01;  // Plus lent
let learning_rate = 0.5;   // Plus rapide
let learning_rate = 1.0;   // Tr√®s rapide (attention √† la stabilit√©)
```

### Fonction d'Activation
Remplace `sigmoid` par `ReLU` ou `tanh` dans [network.rs](src/network.rs).
